{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samochkin/our-teem/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22RecSys_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "qt7-Ri34JRg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IiEvdCNRXe1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder = nn.ModuleList()\n",
        "        for feature in features:\n",
        "            self.encoder.append(self._conv_block(in_channels, feature))\n",
        "            in_channels = feature\n",
        "        self.bottleneck = self._conv_block(features[-1], features[-1] * 2)\n",
        "\n",
        "        self.decoder = nn.ModuleList()\n",
        "        # torch.Size([32, 64, 2]), torch.Size([32, 32, 4]), torch.Size([32, 16, 8]), torch.Size([32, 8, 16])\n",
        "        # [8, 16, 32, 64]\n",
        "        for feature in reversed(features):\n",
        "            self.decoder.append(nn.ConvTranspose1d(feature * 2, feature, kernel_size=2, stride=2))\n",
        "            self.decoder.append(self._conv_block(feature * 2, feature, div_by=1))\n",
        "\n",
        "        self.final_layer = nn.Conv1d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for layer in self.encoder:\n",
        "            x = layer(x)\n",
        "            skip_connections.append(x)\n",
        "            x = nn.MaxPool1d(kernel_size=2, stride=2)(x)\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        skip_connections = skip_connections[::-1]\n",
        "        # print(f\"Shapes: {[s.shape for s in skip_connections]}\")\n",
        "        # print(x.shape)\n",
        "        # print(self.decoder)\n",
        "        for idx in range(0, len(self.decoder), 2):\n",
        "            # print(f\"{self.decoder[idx]=}\")\n",
        "            # print(f\"{x.shape=}\")\n",
        "            x = self.decoder[idx](x)\n",
        "            skip_connection = skip_connections[idx // 2] # 1: 32, 8, 8\n",
        "            # print(skip_connection.shape, x.shape)\n",
        "\n",
        "            x = torch.cat((skip_connection, x), dim=1)\n",
        "            # print(f\"2: {x.shape=}\")\n",
        "            # print(f\"{self.decoder[idx + 1]=}\")\n",
        "            x = self.decoder[idx + 1](x)\n",
        "            # print(f\"{x.shape=}\")\n",
        "\n",
        "        return self.final_layer(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def _conv_block(in_channels, out_channels, div_by = 1):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_channels, out_channels // div_by, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "model = UNet(in_channels=1, out_channels=1, features=[1, 8, 16, 64])\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-5V907kLLJD",
        "outputId": "63ded35c-f06d-4bc7-d1ad-e6a31b43d556"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet(\n",
            "  (encoder): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (bottleneck): Sequential(\n",
            "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (1): ReLU()\n",
            "    (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (decoder): ModuleList(\n",
            "    (0): ConvTranspose1d(128, 64, kernel_size=(2,), stride=(2,))\n",
            "    (1): Sequential(\n",
            "      (0): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (2): ConvTranspose1d(32, 16, kernel_size=(2,), stride=(2,))\n",
            "    (3): Sequential(\n",
            "      (0): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (4): ConvTranspose1d(16, 8, kernel_size=(2,), stride=(2,))\n",
            "    (5): Sequential(\n",
            "      (0): Conv1d(16, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(8, 8, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "    (6): ConvTranspose1d(2, 1, kernel_size=(2,), stride=(2,))\n",
            "    (7): Sequential(\n",
            "      (0): Conv1d(2, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): ReLU()\n",
            "      (2): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (3): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (final_layer): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'sessions.pickle'"
      ],
      "metadata": {
        "id": "UCKH2Enq9TuI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "N0loiK3pMPpb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Файл {file_path} не найден\")\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "file_path = '/content/sessions.pickle'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Файл {file_path} не найден. Проверьте путь.\")\n",
        "else:\n",
        "    clean_logs = load_dataset(file_path)\n",
        "    clean_logs = torch.Tensor(clean_logs['screen_seq']).view(100000, 16)\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Используется устройство: {device}\")\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 1e-4\n",
        "    beta_end = 2e-2\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "timesteps = 1000\n",
        "beta_schedule = linear_beta_schedule(timesteps)\n",
        "\n",
        "def forward_diffusion_process(x, t, noise_schedule):\n",
        "    beta_t = noise_schedule[t]\n",
        "    noise = torch.randn_like(x)\n",
        "    return torch.sqrt(1 - beta_t) * x + torch.sqrt(beta_t) * noise\n",
        "\n",
        "def reverse_diffusion_step(x, t, noise_schedule):     ## Прменить логи и посмотерть что получится\n",
        "    beta_t = noise_schedule[t]\n",
        "    noise = torch.randn_like(x)\n",
        "    sqrt_term = torch.sqrt(torch.clamp(1 - beta_t, min=1e-5))\n",
        "    return (x - torch.sqrt(beta_t) * noise) / sqrt_term\n",
        "\n",
        "# def reverse_diffusion_result(noisy_image, noise):     ## Прменить логи и посмотерть что получится\n",
        "#     beta_t = noise_schedule[t]\n",
        "#     noise = torch.randn_like(x)\n",
        "#     sqrt_term = torch.sqrt(torch.clamp(1 - beta_t, min=1e-5))\n",
        "#     return (x - torch.sqrt(beta_t) * noise) / sqrt_term\n",
        "\n",
        "\n",
        "image = clean_logs #log.view(1,1,16)\n",
        "noisy_image = forward_diffusion_process(image, 10, beta_schedule)\n",
        "denoised_image = reverse_diffusion_step(noisy_image, 10, beta_schedule)\n",
        "\n",
        "\n",
        "titles = [\"Изначальное\", \"Зашумленное\", \"После диффузии\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "predicted_noise = image\n",
        "actual_noise = denoised_image\n",
        "loss = loss_function(predicted_noise, actual_noise)\n",
        "\n",
        "print(f\"Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j4msLGWq-ga",
        "outputId": "762dc2d9-8a99-4db7-f177-1a834269a1e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используется устройство: cuda\n",
            "Loss: 0.0005987735348753631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = UNet(in_channels= 1 , out_channels= 1, features=[8, 16, 32, 64])\n",
        "\n",
        "# model(torch.randn(32, 1, 16)).shape"
      ],
      "metadata": {
        "id": "LLajUTna7rZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g3EAaXDCB2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []"
      ],
      "metadata": {
        "id": "5dUdQN_gB2Xo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_channels= 1 , out_channels= 1, features=[8, 16, 32, 64]).to(device)\n",
        "# print(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-7)\n",
        "\n",
        "epochs = 50\n",
        "timesteps = 1000\n",
        "beta_schedule = linear_beta_schedule(timesteps).to(device)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(clean_logs, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in  range(epochs):\n",
        "    print ( f\"Epoch {epoch+ 1 } / {epochs} \" )\n",
        "    for batch_idx, images in  enumerate (dataloader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        t = torch.randint( 0 , timesteps, (images.size( 1 ),)).to(device)\n",
        "        noise_images = forward_diffusion_process(images, t, beta_schedule)\n",
        "        noise = torch.randn_like(images)\n",
        "        # print(t.shape, noise_images.shape)\n",
        "        predicted_noise = model(noise_images.unsqueeze(1))\n",
        "\n",
        "        loss = loss_function(predicted_noise, noise)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 10 == 0 :\n",
        "            print ( f\"Batch {batch_idx} / { len (dataloader)} - Loss: {loss.item(): .4f} \" )\n",
        "    losses.append(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odx_II698w1G",
        "outputId": "49b77f7a-6d80-4088-ef92-02962286bd09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 50 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([32, 16])) that is different to the input size (torch.Size([32, 1, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "Batch 230 / 3125 - Loss:  1.0799 \n",
            "Batch 240 / 3125 - Loss:  1.0380 \n",
            "Batch 250 / 3125 - Loss:  1.0261 \n",
            "Batch 260 / 3125 - Loss:  1.1608 \n",
            "Batch 270 / 3125 - Loss:  0.9822 \n",
            "Batch 280 / 3125 - Loss:  0.9982 \n",
            "Batch 290 / 3125 - Loss:  0.9677 \n",
            "Batch 300 / 3125 - Loss:  0.9306 \n",
            "Batch 310 / 3125 - Loss:  1.0997 \n",
            "Batch 320 / 3125 - Loss:  0.9862 \n",
            "Batch 330 / 3125 - Loss:  1.0791 \n",
            "Batch 340 / 3125 - Loss:  1.0650 \n",
            "Batch 350 / 3125 - Loss:  0.9915 \n",
            "Batch 360 / 3125 - Loss:  1.0581 \n",
            "Batch 370 / 3125 - Loss:  1.0080 \n",
            "Batch 380 / 3125 - Loss:  1.0773 \n",
            "Batch 390 / 3125 - Loss:  1.0105 \n",
            "Batch 400 / 3125 - Loss:  1.0574 \n",
            "Batch 410 / 3125 - Loss:  1.1233 \n",
            "Batch 420 / 3125 - Loss:  0.9360 \n",
            "Batch 430 / 3125 - Loss:  1.1085 \n",
            "Batch 440 / 3125 - Loss:  1.0006 \n",
            "Batch 450 / 3125 - Loss:  1.1307 \n",
            "Batch 460 / 3125 - Loss:  0.9790 \n",
            "Batch 470 / 3125 - Loss:  0.9493 \n",
            "Batch 480 / 3125 - Loss:  1.0778 \n",
            "Batch 490 / 3125 - Loss:  1.1043 \n",
            "Batch 500 / 3125 - Loss:  1.1597 \n",
            "Batch 510 / 3125 - Loss:  1.1652 \n",
            "Batch 520 / 3125 - Loss:  1.0939 \n",
            "Batch 530 / 3125 - Loss:  0.9759 \n",
            "Batch 540 / 3125 - Loss:  1.1074 \n",
            "Batch 550 / 3125 - Loss:  0.9761 \n",
            "Batch 560 / 3125 - Loss:  1.0276 \n",
            "Batch 570 / 3125 - Loss:  0.9723 \n",
            "Batch 580 / 3125 - Loss:  1.0678 \n",
            "Batch 590 / 3125 - Loss:  1.1932 \n",
            "Batch 600 / 3125 - Loss:  0.9695 \n",
            "Batch 610 / 3125 - Loss:  1.1199 \n",
            "Batch 620 / 3125 - Loss:  0.9832 \n",
            "Batch 630 / 3125 - Loss:  0.9157 \n",
            "Batch 640 / 3125 - Loss:  0.9957 \n",
            "Batch 650 / 3125 - Loss:  1.0258 \n",
            "Batch 660 / 3125 - Loss:  1.0260 \n",
            "Batch 670 / 3125 - Loss:  1.0222 \n",
            "Batch 680 / 3125 - Loss:  1.0470 \n",
            "Batch 690 / 3125 - Loss:  0.9840 \n",
            "Batch 700 / 3125 - Loss:  1.0468 \n",
            "Batch 710 / 3125 - Loss:  1.1185 \n",
            "Batch 720 / 3125 - Loss:  1.0497 \n",
            "Batch 730 / 3125 - Loss:  1.1472 \n",
            "Batch 740 / 3125 - Loss:  0.9871 \n",
            "Batch 750 / 3125 - Loss:  1.0567 \n",
            "Batch 760 / 3125 - Loss:  0.9037 \n",
            "Batch 770 / 3125 - Loss:  1.0583 \n",
            "Batch 780 / 3125 - Loss:  1.0121 \n",
            "Batch 790 / 3125 - Loss:  1.0763 \n",
            "Batch 800 / 3125 - Loss:  1.0927 \n",
            "Batch 810 / 3125 - Loss:  1.0711 \n",
            "Batch 820 / 3125 - Loss:  1.1246 \n",
            "Batch 830 / 3125 - Loss:  1.0277 \n",
            "Batch 840 / 3125 - Loss:  1.0381 \n",
            "Batch 850 / 3125 - Loss:  1.0970 \n",
            "Batch 860 / 3125 - Loss:  0.9961 \n",
            "Batch 870 / 3125 - Loss:  0.9881 \n",
            "Batch 880 / 3125 - Loss:  0.8876 \n",
            "Batch 890 / 3125 - Loss:  0.9377 \n",
            "Batch 900 / 3125 - Loss:  1.1529 \n",
            "Batch 910 / 3125 - Loss:  1.1057 \n",
            "Batch 920 / 3125 - Loss:  1.0184 \n",
            "Batch 930 / 3125 - Loss:  1.1313 \n",
            "Batch 940 / 3125 - Loss:  0.9614 \n",
            "Batch 950 / 3125 - Loss:  1.0906 \n",
            "Batch 960 / 3125 - Loss:  1.0551 \n",
            "Batch 970 / 3125 - Loss:  1.0572 \n",
            "Batch 980 / 3125 - Loss:  1.0882 \n",
            "Batch 990 / 3125 - Loss:  1.0967 \n",
            "Batch 1000 / 3125 - Loss:  1.1232 \n",
            "Batch 1010 / 3125 - Loss:  1.0257 \n",
            "Batch 1020 / 3125 - Loss:  1.0495 \n",
            "Batch 1030 / 3125 - Loss:  1.0034 \n",
            "Batch 1040 / 3125 - Loss:  1.0746 \n",
            "Batch 1050 / 3125 - Loss:  1.0937 \n",
            "Batch 1060 / 3125 - Loss:  0.9331 \n",
            "Batch 1070 / 3125 - Loss:  0.9858 \n",
            "Batch 1080 / 3125 - Loss:  1.0517 \n",
            "Batch 1090 / 3125 - Loss:  0.9896 \n",
            "Batch 1100 / 3125 - Loss:  1.0147 \n",
            "Batch 1110 / 3125 - Loss:  1.0015 \n",
            "Batch 1120 / 3125 - Loss:  1.0464 \n",
            "Batch 1130 / 3125 - Loss:  0.9426 \n",
            "Batch 1140 / 3125 - Loss:  1.0222 \n",
            "Batch 1150 / 3125 - Loss:  1.1187 \n",
            "Batch 1160 / 3125 - Loss:  1.0786 \n",
            "Batch 1170 / 3125 - Loss:  1.0541 \n",
            "Batch 1180 / 3125 - Loss:  0.9882 \n",
            "Batch 1190 / 3125 - Loss:  1.0854 \n",
            "Batch 1200 / 3125 - Loss:  0.9264 \n",
            "Batch 1210 / 3125 - Loss:  1.0863 \n",
            "Batch 1220 / 3125 - Loss:  0.9714 \n",
            "Batch 1230 / 3125 - Loss:  0.9208 \n",
            "Batch 1240 / 3125 - Loss:  1.0276 \n",
            "Batch 1250 / 3125 - Loss:  1.0216 \n",
            "Batch 1260 / 3125 - Loss:  0.9413 \n",
            "Batch 1270 / 3125 - Loss:  1.0021 \n",
            "Batch 1280 / 3125 - Loss:  0.9794 \n",
            "Batch 1290 / 3125 - Loss:  1.0113 \n",
            "Batch 1300 / 3125 - Loss:  1.0379 \n",
            "Batch 1310 / 3125 - Loss:  0.9797 \n",
            "Batch 1320 / 3125 - Loss:  1.0234 \n",
            "Batch 1330 / 3125 - Loss:  1.2084 \n",
            "Batch 1340 / 3125 - Loss:  1.1119 \n",
            "Batch 1350 / 3125 - Loss:  0.9692 \n",
            "Batch 1360 / 3125 - Loss:  0.9833 \n",
            "Batch 1370 / 3125 - Loss:  1.0591 \n",
            "Batch 1380 / 3125 - Loss:  1.1109 \n",
            "Batch 1390 / 3125 - Loss:  0.9955 \n",
            "Batch 1400 / 3125 - Loss:  1.1507 \n",
            "Batch 1410 / 3125 - Loss:  1.0130 \n",
            "Batch 1420 / 3125 - Loss:  1.0234 \n",
            "Batch 1430 / 3125 - Loss:  0.9725 \n",
            "Batch 1440 / 3125 - Loss:  1.0527 \n",
            "Batch 1450 / 3125 - Loss:  1.0181 \n",
            "Batch 1460 / 3125 - Loss:  0.9737 \n",
            "Batch 1470 / 3125 - Loss:  0.9550 \n",
            "Batch 1480 / 3125 - Loss:  1.0224 \n",
            "Batch 1490 / 3125 - Loss:  1.1184 \n",
            "Batch 1500 / 3125 - Loss:  1.0241 \n",
            "Batch 1510 / 3125 - Loss:  1.0170 \n",
            "Batch 1520 / 3125 - Loss:  1.1626 \n",
            "Batch 1530 / 3125 - Loss:  1.1082 \n",
            "Batch 1540 / 3125 - Loss:  1.0279 \n",
            "Batch 1550 / 3125 - Loss:  1.0808 \n",
            "Batch 1560 / 3125 - Loss:  1.0222 \n",
            "Batch 1570 / 3125 - Loss:  0.9464 \n",
            "Batch 1580 / 3125 - Loss:  1.1141 \n",
            "Batch 1590 / 3125 - Loss:  1.0999 \n",
            "Batch 1600 / 3125 - Loss:  1.0918 \n",
            "Batch 1610 / 3125 - Loss:  1.1439 \n",
            "Batch 1620 / 3125 - Loss:  1.0004 \n",
            "Batch 1630 / 3125 - Loss:  1.0232 \n",
            "Batch 1640 / 3125 - Loss:  1.0117 \n",
            "Batch 1650 / 3125 - Loss:  1.0985 \n",
            "Batch 1660 / 3125 - Loss:  0.9070 \n",
            "Batch 1670 / 3125 - Loss:  1.0992 \n",
            "Batch 1680 / 3125 - Loss:  1.0288 \n",
            "Batch 1690 / 3125 - Loss:  1.0440 \n",
            "Batch 1700 / 3125 - Loss:  1.0107 \n",
            "Batch 1710 / 3125 - Loss:  1.0191 \n",
            "Batch 1720 / 3125 - Loss:  1.0541 \n",
            "Batch 1730 / 3125 - Loss:  1.1599 \n",
            "Batch 1740 / 3125 - Loss:  1.0986 \n",
            "Batch 1750 / 3125 - Loss:  1.0809 \n",
            "Batch 1760 / 3125 - Loss:  1.0380 \n",
            "Batch 1770 / 3125 - Loss:  1.1835 \n",
            "Batch 1780 / 3125 - Loss:  0.9995 \n",
            "Batch 1790 / 3125 - Loss:  1.0261 \n",
            "Batch 1800 / 3125 - Loss:  1.0244 \n",
            "Batch 1810 / 3125 - Loss:  1.0134 \n",
            "Batch 1820 / 3125 - Loss:  1.0177 \n",
            "Batch 1830 / 3125 - Loss:  1.0602 \n",
            "Batch 1840 / 3125 - Loss:  0.9661 \n",
            "Batch 1850 / 3125 - Loss:  1.0912 \n",
            "Batch 1860 / 3125 - Loss:  0.9872 \n",
            "Batch 1870 / 3125 - Loss:  0.9879 \n",
            "Batch 1880 / 3125 - Loss:  0.9587 \n",
            "Batch 1890 / 3125 - Loss:  1.0089 \n",
            "Batch 1900 / 3125 - Loss:  1.1244 \n",
            "Batch 1910 / 3125 - Loss:  0.9684 \n",
            "Batch 1920 / 3125 - Loss:  1.0760 \n",
            "Batch 1930 / 3125 - Loss:  1.0627 \n",
            "Batch 1940 / 3125 - Loss:  0.9972 \n",
            "Batch 1950 / 3125 - Loss:  0.9999 \n",
            "Batch 1960 / 3125 - Loss:  0.9706 \n",
            "Batch 1970 / 3125 - Loss:  1.0778 \n",
            "Batch 1980 / 3125 - Loss:  1.1854 \n",
            "Batch 1990 / 3125 - Loss:  0.9704 \n",
            "Batch 2000 / 3125 - Loss:  1.0742 \n",
            "Batch 2010 / 3125 - Loss:  0.9363 \n",
            "Batch 2020 / 3125 - Loss:  1.0640 \n",
            "Batch 2030 / 3125 - Loss:  0.9267 \n",
            "Batch 2040 / 3125 - Loss:  0.9870 \n",
            "Batch 2050 / 3125 - Loss:  0.9169 \n",
            "Batch 2060 / 3125 - Loss:  1.0541 \n",
            "Batch 2070 / 3125 - Loss:  1.1010 \n",
            "Batch 2080 / 3125 - Loss:  1.0941 \n",
            "Batch 2090 / 3125 - Loss:  1.0189 \n",
            "Batch 2100 / 3125 - Loss:  0.9859 \n",
            "Batch 2110 / 3125 - Loss:  1.1202 \n",
            "Batch 2120 / 3125 - Loss:  1.1116 \n",
            "Batch 2130 / 3125 - Loss:  1.1354 \n",
            "Batch 2140 / 3125 - Loss:  1.1324 \n",
            "Batch 2150 / 3125 - Loss:  1.1120 \n",
            "Batch 2160 / 3125 - Loss:  0.9857 \n",
            "Batch 2170 / 3125 - Loss:  1.0464 \n",
            "Batch 2180 / 3125 - Loss:  1.1460 \n",
            "Batch 2190 / 3125 - Loss:  1.1197 \n",
            "Batch 2200 / 3125 - Loss:  1.0969 \n",
            "Batch 2210 / 3125 - Loss:  1.0875 \n",
            "Batch 2220 / 3125 - Loss:  1.0976 \n",
            "Batch 2230 / 3125 - Loss:  1.0863 \n",
            "Batch 2240 / 3125 - Loss:  1.0222 \n",
            "Batch 2250 / 3125 - Loss:  1.0836 \n",
            "Batch 2260 / 3125 - Loss:  1.0410 \n",
            "Batch 2270 / 3125 - Loss:  1.0487 \n",
            "Batch 2280 / 3125 - Loss:  1.0253 \n",
            "Batch 2290 / 3125 - Loss:  1.0385 \n",
            "Batch 2300 / 3125 - Loss:  1.0106 \n",
            "Batch 2310 / 3125 - Loss:  0.9964 \n",
            "Batch 2320 / 3125 - Loss:  0.9727 \n",
            "Batch 2330 / 3125 - Loss:  1.0329 \n",
            "Batch 2340 / 3125 - Loss:  1.0802 \n",
            "Batch 2350 / 3125 - Loss:  1.0106 \n",
            "Batch 2360 / 3125 - Loss:  1.0257 \n",
            "Batch 2370 / 3125 - Loss:  1.1211 \n",
            "Batch 2380 / 3125 - Loss:  0.9743 \n",
            "Batch 2390 / 3125 - Loss:  1.1585 \n",
            "Batch 2400 / 3125 - Loss:  1.1443 \n",
            "Batch 2410 / 3125 - Loss:  0.9913 \n",
            "Batch 2420 / 3125 - Loss:  1.0236 \n",
            "Batch 2430 / 3125 - Loss:  1.1495 \n",
            "Batch 2440 / 3125 - Loss:  0.9924 \n",
            "Batch 2450 / 3125 - Loss:  1.0056 \n",
            "Batch 2460 / 3125 - Loss:  1.1557 \n",
            "Batch 2470 / 3125 - Loss:  1.0288 \n",
            "Batch 2480 / 3125 - Loss:  0.9831 \n",
            "Batch 2490 / 3125 - Loss:  1.0708 \n",
            "Batch 2500 / 3125 - Loss:  1.0345 \n",
            "Batch 2510 / 3125 - Loss:  0.9788 \n",
            "Batch 2520 / 3125 - Loss:  1.0344 \n",
            "Batch 2530 / 3125 - Loss:  1.0166 \n",
            "Batch 2540 / 3125 - Loss:  0.9481 \n",
            "Batch 2550 / 3125 - Loss:  1.2056 \n",
            "Batch 2560 / 3125 - Loss:  1.0549 \n",
            "Batch 2570 / 3125 - Loss:  1.0449 \n",
            "Batch 2580 / 3125 - Loss:  1.0226 \n",
            "Batch 2590 / 3125 - Loss:  1.0658 \n",
            "Batch 2600 / 3125 - Loss:  1.0713 \n",
            "Batch 2610 / 3125 - Loss:  1.1380 \n",
            "Batch 2620 / 3125 - Loss:  0.9232 \n",
            "Batch 2630 / 3125 - Loss:  1.0846 \n",
            "Batch 2640 / 3125 - Loss:  0.9942 \n",
            "Batch 2650 / 3125 - Loss:  1.0747 \n",
            "Batch 2660 / 3125 - Loss:  1.0603 \n",
            "Batch 2670 / 3125 - Loss:  1.0093 \n",
            "Batch 2680 / 3125 - Loss:  1.0758 \n",
            "Batch 2690 / 3125 - Loss:  1.0449 \n",
            "Batch 2700 / 3125 - Loss:  1.1195 \n",
            "Batch 2710 / 3125 - Loss:  1.1592 \n",
            "Batch 2720 / 3125 - Loss:  1.0636 \n",
            "Batch 2730 / 3125 - Loss:  1.0067 \n",
            "Batch 2740 / 3125 - Loss:  1.0965 \n",
            "Batch 2750 / 3125 - Loss:  1.0620 \n",
            "Batch 2760 / 3125 - Loss:  1.0987 \n",
            "Batch 2770 / 3125 - Loss:  1.0633 \n",
            "Batch 2780 / 3125 - Loss:  1.0105 \n",
            "Batch 2790 / 3125 - Loss:  1.0875 \n",
            "Batch 2800 / 3125 - Loss:  1.0653 \n",
            "Batch 2810 / 3125 - Loss:  1.0801 \n",
            "Batch 2820 / 3125 - Loss:  1.1528 \n",
            "Batch 2830 / 3125 - Loss:  0.9144 \n",
            "Batch 2840 / 3125 - Loss:  1.0901 \n",
            "Batch 2850 / 3125 - Loss:  1.1152 \n",
            "Batch 2860 / 3125 - Loss:  1.0996 \n",
            "Batch 2870 / 3125 - Loss:  1.1121 \n",
            "Batch 2880 / 3125 - Loss:  0.9426 \n",
            "Batch 2890 / 3125 - Loss:  1.1471 \n",
            "Batch 2900 / 3125 - Loss:  1.1475 \n",
            "Batch 2910 / 3125 - Loss:  1.0020 \n",
            "Batch 2920 / 3125 - Loss:  1.0648 \n",
            "Batch 2930 / 3125 - Loss:  0.9479 \n",
            "Batch 2940 / 3125 - Loss:  1.0626 \n",
            "Batch 2950 / 3125 - Loss:  1.0334 \n",
            "Batch 2960 / 3125 - Loss:  1.0195 \n",
            "Batch 2970 / 3125 - Loss:  1.1938 \n",
            "Batch 2980 / 3125 - Loss:  1.0230 \n",
            "Batch 2990 / 3125 - Loss:  1.0094 \n",
            "Batch 3000 / 3125 - Loss:  1.0849 \n",
            "Batch 3010 / 3125 - Loss:  1.0029 \n",
            "Batch 3020 / 3125 - Loss:  1.0893 \n",
            "Batch 3030 / 3125 - Loss:  1.0297 \n",
            "Batch 3040 / 3125 - Loss:  1.0096 \n",
            "Batch 3050 / 3125 - Loss:  1.0192 \n",
            "Batch 3060 / 3125 - Loss:  1.0460 \n",
            "Batch 3070 / 3125 - Loss:  1.0461 \n",
            "Batch 3080 / 3125 - Loss:  1.0644 \n",
            "Batch 3090 / 3125 - Loss:  0.9896 \n",
            "Batch 3100 / 3125 - Loss:  1.1276 \n",
            "Batch 3110 / 3125 - Loss:  0.9341 \n",
            "Batch 3120 / 3125 - Loss:  1.0695 \n",
            "Epoch 36 / 50 \n",
            "Batch 0 / 3125 - Loss:  0.9594 \n",
            "Batch 10 / 3125 - Loss:  0.9504 \n",
            "Batch 20 / 3125 - Loss:  1.0241 \n",
            "Batch 30 / 3125 - Loss:  1.0580 \n",
            "Batch 40 / 3125 - Loss:  0.9397 \n",
            "Batch 50 / 3125 - Loss:  1.0662 \n",
            "Batch 60 / 3125 - Loss:  0.8900 \n",
            "Batch 70 / 3125 - Loss:  0.8754 \n",
            "Batch 80 / 3125 - Loss:  1.0889 \n",
            "Batch 90 / 3125 - Loss:  1.0533 \n",
            "Batch 100 / 3125 - Loss:  1.1966 \n",
            "Batch 110 / 3125 - Loss:  1.1247 \n",
            "Batch 120 / 3125 - Loss:  0.9653 \n",
            "Batch 130 / 3125 - Loss:  0.9696 \n",
            "Batch 140 / 3125 - Loss:  1.0132 \n",
            "Batch 150 / 3125 - Loss:  0.9343 \n",
            "Batch 160 / 3125 - Loss:  0.9782 \n",
            "Batch 170 / 3125 - Loss:  1.1618 \n",
            "Batch 180 / 3125 - Loss:  1.0860 \n",
            "Batch 190 / 3125 - Loss:  1.0408 \n",
            "Batch 200 / 3125 - Loss:  1.1064 \n",
            "Batch 210 / 3125 - Loss:  0.9065 \n",
            "Batch 220 / 3125 - Loss:  1.0167 \n",
            "Batch 230 / 3125 - Loss:  1.0936 \n",
            "Batch 240 / 3125 - Loss:  0.9442 \n",
            "Batch 250 / 3125 - Loss:  0.9798 \n",
            "Batch 260 / 3125 - Loss:  0.9499 \n",
            "Batch 270 / 3125 - Loss:  0.9709 \n",
            "Batch 280 / 3125 - Loss:  1.0973 \n",
            "Batch 290 / 3125 - Loss:  1.0564 \n",
            "Batch 300 / 3125 - Loss:  1.1092 \n",
            "Batch 310 / 3125 - Loss:  1.0563 \n",
            "Batch 320 / 3125 - Loss:  0.9505 \n",
            "Batch 330 / 3125 - Loss:  1.0642 \n",
            "Batch 340 / 3125 - Loss:  1.0939 \n",
            "Batch 350 / 3125 - Loss:  0.9823 \n",
            "Batch 360 / 3125 - Loss:  1.0512 \n",
            "Batch 370 / 3125 - Loss:  1.1355 \n",
            "Batch 380 / 3125 - Loss:  1.1452 \n",
            "Batch 390 / 3125 - Loss:  1.1316 \n",
            "Batch 400 / 3125 - Loss:  1.0059 \n",
            "Batch 410 / 3125 - Loss:  1.0261 \n",
            "Batch 420 / 3125 - Loss:  1.0480 \n",
            "Batch 430 / 3125 - Loss:  0.9800 \n",
            "Batch 440 / 3125 - Loss:  1.0178 \n",
            "Batch 450 / 3125 - Loss:  0.8920 \n",
            "Batch 460 / 3125 - Loss:  0.9743 \n",
            "Batch 470 / 3125 - Loss:  1.0365 \n",
            "Batch 480 / 3125 - Loss:  0.9024 \n",
            "Batch 490 / 3125 - Loss:  1.0997 \n",
            "Batch 500 / 3125 - Loss:  1.1001 \n",
            "Batch 510 / 3125 - Loss:  0.9877 \n",
            "Batch 520 / 3125 - Loss:  1.1028 \n",
            "Batch 530 / 3125 - Loss:  0.9795 \n",
            "Batch 540 / 3125 - Loss:  0.9929 \n",
            "Batch 550 / 3125 - Loss:  0.9919 \n",
            "Batch 560 / 3125 - Loss:  1.0124 \n",
            "Batch 570 / 3125 - Loss:  1.0291 \n",
            "Batch 580 / 3125 - Loss:  1.1084 \n",
            "Batch 590 / 3125 - Loss:  1.0764 \n",
            "Batch 600 / 3125 - Loss:  1.0127 \n",
            "Batch 610 / 3125 - Loss:  1.0266 \n",
            "Batch 620 / 3125 - Loss:  1.0548 \n",
            "Batch 630 / 3125 - Loss:  0.9322 \n",
            "Batch 640 / 3125 - Loss:  1.0118 \n",
            "Batch 650 / 3125 - Loss:  0.9795 \n",
            "Batch 660 / 3125 - Loss:  1.0830 \n",
            "Batch 670 / 3125 - Loss:  1.0269 \n",
            "Batch 680 / 3125 - Loss:  0.9153 \n",
            "Batch 690 / 3125 - Loss:  1.0161 \n",
            "Batch 700 / 3125 - Loss:  0.9990 \n",
            "Batch 710 / 3125 - Loss:  1.0359 \n",
            "Batch 720 / 3125 - Loss:  1.0560 \n",
            "Batch 730 / 3125 - Loss:  1.0561 \n",
            "Batch 740 / 3125 - Loss:  1.0328 \n",
            "Batch 750 / 3125 - Loss:  0.9327 \n",
            "Batch 760 / 3125 - Loss:  1.0108 \n",
            "Batch 770 / 3125 - Loss:  1.0093 \n",
            "Batch 780 / 3125 - Loss:  1.0068 \n",
            "Batch 790 / 3125 - Loss:  0.9266 \n",
            "Batch 800 / 3125 - Loss:  1.1306 \n",
            "Batch 810 / 3125 - Loss:  1.0759 \n",
            "Batch 820 / 3125 - Loss:  1.1283 \n",
            "Batch 830 / 3125 - Loss:  0.9710 \n",
            "Batch 840 / 3125 - Loss:  1.0384 \n",
            "Batch 850 / 3125 - Loss:  1.0159 \n",
            "Batch 860 / 3125 - Loss:  1.1742 \n",
            "Batch 870 / 3125 - Loss:  0.9134 \n",
            "Batch 880 / 3125 - Loss:  1.0162 \n",
            "Batch 890 / 3125 - Loss:  1.0174 \n",
            "Batch 900 / 3125 - Loss:  0.9778 \n",
            "Batch 910 / 3125 - Loss:  1.0563 \n",
            "Batch 920 / 3125 - Loss:  0.9706 \n",
            "Batch 930 / 3125 - Loss:  0.9822 \n",
            "Batch 940 / 3125 - Loss:  1.0304 \n",
            "Batch 950 / 3125 - Loss:  1.1011 \n",
            "Batch 960 / 3125 - Loss:  1.1491 \n",
            "Batch 970 / 3125 - Loss:  1.1486 \n",
            "Batch 980 / 3125 - Loss:  0.9793 \n",
            "Batch 990 / 3125 - Loss:  1.0727 \n",
            "Batch 1000 / 3125 - Loss:  1.1480 \n",
            "Batch 1010 / 3125 - Loss:  1.0287 \n",
            "Batch 1020 / 3125 - Loss:  0.9123 \n",
            "Batch 1030 / 3125 - Loss:  1.0222 \n",
            "Batch 1040 / 3125 - Loss:  1.1419 \n",
            "Batch 1050 / 3125 - Loss:  0.9778 \n",
            "Batch 1060 / 3125 - Loss:  1.0396 \n",
            "Batch 1070 / 3125 - Loss:  1.1433 \n",
            "Batch 1080 / 3125 - Loss:  1.1604 \n",
            "Batch 1090 / 3125 - Loss:  1.0536 \n",
            "Batch 1100 / 3125 - Loss:  0.9485 \n",
            "Batch 1110 / 3125 - Loss:  1.0996 \n",
            "Batch 1120 / 3125 - Loss:  1.0297 \n",
            "Batch 1130 / 3125 - Loss:  1.0686 \n",
            "Batch 1140 / 3125 - Loss:  0.8879 \n",
            "Batch 1150 / 3125 - Loss:  1.0446 \n",
            "Batch 1160 / 3125 - Loss:  0.9552 \n",
            "Batch 1170 / 3125 - Loss:  1.1146 \n",
            "Batch 1180 / 3125 - Loss:  1.0503 \n",
            "Batch 1190 / 3125 - Loss:  1.0239 \n",
            "Batch 1200 / 3125 - Loss:  1.1089 \n",
            "Batch 1210 / 3125 - Loss:  1.0484 \n",
            "Batch 1220 / 3125 - Loss:  0.9723 \n",
            "Batch 1230 / 3125 - Loss:  0.9575 \n",
            "Batch 1240 / 3125 - Loss:  1.0831 \n",
            "Batch 1250 / 3125 - Loss:  1.0745 \n",
            "Batch 1260 / 3125 - Loss:  0.9599 \n",
            "Batch 1270 / 3125 - Loss:  1.0421 \n",
            "Batch 1280 / 3125 - Loss:  1.0386 \n",
            "Batch 1290 / 3125 - Loss:  1.0082 \n",
            "Batch 1300 / 3125 - Loss:  0.9935 \n",
            "Batch 1310 / 3125 - Loss:  1.0482 \n",
            "Batch 1320 / 3125 - Loss:  1.0008 \n",
            "Batch 1330 / 3125 - Loss:  1.0726 \n",
            "Batch 1340 / 3125 - Loss:  1.1086 \n",
            "Batch 1350 / 3125 - Loss:  0.9505 \n",
            "Batch 1360 / 3125 - Loss:  1.0605 \n",
            "Batch 1370 / 3125 - Loss:  1.0821 \n",
            "Batch 1380 / 3125 - Loss:  1.0955 \n",
            "Batch 1390 / 3125 - Loss:  1.0260 \n",
            "Batch 1400 / 3125 - Loss:  1.1794 \n",
            "Batch 1410 / 3125 - Loss:  1.1208 \n",
            "Batch 1420 / 3125 - Loss:  1.0102 \n",
            "Batch 1430 / 3125 - Loss:  1.0356 \n",
            "Batch 1440 / 3125 - Loss:  0.9894 \n",
            "Batch 1450 / 3125 - Loss:  0.9862 \n",
            "Batch 1460 / 3125 - Loss:  1.1232 \n",
            "Batch 1470 / 3125 - Loss:  1.0246 \n",
            "Batch 1480 / 3125 - Loss:  1.0838 \n",
            "Batch 1490 / 3125 - Loss:  0.9703 \n",
            "Batch 1500 / 3125 - Loss:  1.0538 \n",
            "Batch 1510 / 3125 - Loss:  0.9149 \n",
            "Batch 1520 / 3125 - Loss:  0.9775 \n",
            "Batch 1530 / 3125 - Loss:  1.0080 \n",
            "Batch 1540 / 3125 - Loss:  0.9535 \n",
            "Batch 1550 / 3125 - Loss:  1.0212 \n",
            "Batch 1560 / 3125 - Loss:  0.9496 \n",
            "Batch 1570 / 3125 - Loss:  1.0207 \n",
            "Batch 1580 / 3125 - Loss:  1.0163 \n",
            "Batch 1590 / 3125 - Loss:  0.9922 \n",
            "Batch 1600 / 3125 - Loss:  1.0849 \n",
            "Batch 1610 / 3125 - Loss:  0.9727 \n",
            "Batch 1620 / 3125 - Loss:  1.1228 \n",
            "Batch 1630 / 3125 - Loss:  1.0293 \n",
            "Batch 1640 / 3125 - Loss:  1.0358 \n",
            "Batch 1650 / 3125 - Loss:  1.0150 \n",
            "Batch 1660 / 3125 - Loss:  1.0774 \n",
            "Batch 1670 / 3125 - Loss:  0.9031 \n",
            "Batch 1680 / 3125 - Loss:  1.0730 \n",
            "Batch 1690 / 3125 - Loss:  1.0163 \n",
            "Batch 1700 / 3125 - Loss:  1.0300 \n",
            "Batch 1710 / 3125 - Loss:  1.1001 \n",
            "Batch 1720 / 3125 - Loss:  1.0948 \n",
            "Batch 1730 / 3125 - Loss:  1.0334 \n",
            "Batch 1740 / 3125 - Loss:  0.9449 \n",
            "Batch 1750 / 3125 - Loss:  0.8808 \n",
            "Batch 1760 / 3125 - Loss:  1.0545 \n",
            "Batch 1770 / 3125 - Loss:  1.1330 \n",
            "Batch 1780 / 3125 - Loss:  0.9711 \n",
            "Batch 1790 / 3125 - Loss:  1.0113 \n",
            "Batch 1800 / 3125 - Loss:  0.9937 \n",
            "Batch 1810 / 3125 - Loss:  1.0761 \n",
            "Batch 1820 / 3125 - Loss:  0.9633 \n",
            "Batch 1830 / 3125 - Loss:  1.0821 \n",
            "Batch 1840 / 3125 - Loss:  1.1065 \n",
            "Batch 1850 / 3125 - Loss:  1.0054 \n",
            "Batch 1860 / 3125 - Loss:  1.0007 \n",
            "Batch 1870 / 3125 - Loss:  1.0267 \n",
            "Batch 1880 / 3125 - Loss:  0.9647 \n",
            "Batch 1890 / 3125 - Loss:  1.0992 \n",
            "Batch 1900 / 3125 - Loss:  1.0022 \n",
            "Batch 1910 / 3125 - Loss:  1.0894 \n",
            "Batch 1920 / 3125 - Loss:  1.0970 \n",
            "Batch 1930 / 3125 - Loss:  1.0090 \n",
            "Batch 1940 / 3125 - Loss:  1.0673 \n",
            "Batch 1950 / 3125 - Loss:  0.9582 \n",
            "Batch 1960 / 3125 - Loss:  0.8242 \n",
            "Batch 1970 / 3125 - Loss:  1.0119 \n",
            "Batch 1980 / 3125 - Loss:  0.9854 \n",
            "Batch 1990 / 3125 - Loss:  1.1088 \n",
            "Batch 2000 / 3125 - Loss:  1.0898 \n",
            "Batch 2010 / 3125 - Loss:  1.1293 \n",
            "Batch 2020 / 3125 - Loss:  0.9712 \n",
            "Batch 2030 / 3125 - Loss:  1.0020 \n",
            "Batch 2040 / 3125 - Loss:  1.2328 \n",
            "Batch 2050 / 3125 - Loss:  0.9801 \n",
            "Batch 2060 / 3125 - Loss:  0.9965 \n",
            "Batch 2070 / 3125 - Loss:  1.0951 \n",
            "Batch 2080 / 3125 - Loss:  0.9716 \n",
            "Batch 2090 / 3125 - Loss:  1.0786 \n",
            "Batch 2100 / 3125 - Loss:  1.1486 \n",
            "Batch 2110 / 3125 - Loss:  1.0451 \n",
            "Batch 2120 / 3125 - Loss:  1.0767 \n",
            "Batch 2130 / 3125 - Loss:  1.0012 \n",
            "Batch 2140 / 3125 - Loss:  1.1471 \n",
            "Batch 2150 / 3125 - Loss:  1.0130 \n",
            "Batch 2160 / 3125 - Loss:  1.1690 \n",
            "Batch 2170 / 3125 - Loss:  1.0134 \n",
            "Batch 2180 / 3125 - Loss:  0.9684 \n",
            "Batch 2190 / 3125 - Loss:  1.0135 \n",
            "Batch 2200 / 3125 - Loss:  1.0603 \n",
            "Batch 2210 / 3125 - Loss:  1.0507 \n",
            "Batch 2220 / 3125 - Loss:  1.0392 \n",
            "Batch 2230 / 3125 - Loss:  0.9976 \n",
            "Batch 2240 / 3125 - Loss:  1.0915 \n",
            "Batch 2250 / 3125 - Loss:  0.9991 \n",
            "Batch 2260 / 3125 - Loss:  1.0061 \n",
            "Batch 2270 / 3125 - Loss:  0.8413 \n",
            "Batch 2280 / 3125 - Loss:  1.0114 \n",
            "Batch 2290 / 3125 - Loss:  0.9301 \n",
            "Batch 2300 / 3125 - Loss:  0.9902 \n",
            "Batch 2310 / 3125 - Loss:  1.0333 \n",
            "Batch 2320 / 3125 - Loss:  1.0545 \n",
            "Batch 2330 / 3125 - Loss:  1.0780 \n",
            "Batch 2340 / 3125 - Loss:  1.0622 \n",
            "Batch 2350 / 3125 - Loss:  0.9971 \n",
            "Batch 2360 / 3125 - Loss:  1.0893 \n",
            "Batch 2370 / 3125 - Loss:  1.1194 \n",
            "Batch 2380 / 3125 - Loss:  1.1753 \n",
            "Batch 2390 / 3125 - Loss:  0.9754 \n",
            "Batch 2400 / 3125 - Loss:  0.9393 \n",
            "Batch 2410 / 3125 - Loss:  0.9756 \n",
            "Batch 2420 / 3125 - Loss:  1.0568 \n",
            "Batch 2430 / 3125 - Loss:  1.0388 \n",
            "Batch 2440 / 3125 - Loss:  1.1781 \n",
            "Batch 2450 / 3125 - Loss:  0.9625 \n",
            "Batch 2460 / 3125 - Loss:  1.0005 \n",
            "Batch 2470 / 3125 - Loss:  1.0123 \n",
            "Batch 2480 / 3125 - Loss:  1.0099 \n",
            "Batch 2490 / 3125 - Loss:  1.1395 \n",
            "Batch 2500 / 3125 - Loss:  0.9855 \n",
            "Batch 2510 / 3125 - Loss:  0.9666 \n",
            "Batch 2520 / 3125 - Loss:  1.0599 \n",
            "Batch 2530 / 3125 - Loss:  1.0043 \n",
            "Batch 2540 / 3125 - Loss:  1.1068 \n",
            "Batch 2550 / 3125 - Loss:  1.0268 \n",
            "Batch 2560 / 3125 - Loss:  0.9826 \n",
            "Batch 2570 / 3125 - Loss:  1.0083 \n",
            "Batch 2580 / 3125 - Loss:  0.9819 \n",
            "Batch 2590 / 3125 - Loss:  0.9471 \n",
            "Batch 2600 / 3125 - Loss:  0.9974 \n",
            "Batch 2610 / 3125 - Loss:  0.9723 \n",
            "Batch 2620 / 3125 - Loss:  1.0389 \n",
            "Batch 2630 / 3125 - Loss:  1.0289 \n",
            "Batch 2640 / 3125 - Loss:  0.9830 \n",
            "Batch 2650 / 3125 - Loss:  1.0084 \n",
            "Batch 2660 / 3125 - Loss:  1.0851 \n",
            "Batch 2670 / 3125 - Loss:  0.8966 \n",
            "Batch 2680 / 3125 - Loss:  1.0220 \n",
            "Batch 2690 / 3125 - Loss:  1.0000 \n",
            "Batch 2700 / 3125 - Loss:  1.0405 \n",
            "Batch 2710 / 3125 - Loss:  1.0252 \n",
            "Batch 2720 / 3125 - Loss:  1.1082 \n",
            "Batch 2730 / 3125 - Loss:  0.9663 \n",
            "Batch 2740 / 3125 - Loss:  1.0128 \n",
            "Batch 2750 / 3125 - Loss:  0.9797 \n",
            "Batch 2760 / 3125 - Loss:  1.0391 \n",
            "Batch 2770 / 3125 - Loss:  0.9980 \n",
            "Batch 2780 / 3125 - Loss:  1.1435 \n",
            "Batch 2790 / 3125 - Loss:  1.1133 \n",
            "Batch 2800 / 3125 - Loss:  0.9235 \n",
            "Batch 2810 / 3125 - Loss:  1.0221 \n",
            "Batch 2820 / 3125 - Loss:  1.0069 \n",
            "Batch 2830 / 3125 - Loss:  1.0323 \n",
            "Batch 2840 / 3125 - Loss:  1.0823 \n",
            "Batch 2850 / 3125 - Loss:  1.0532 \n",
            "Batch 2860 / 3125 - Loss:  1.0704 \n",
            "Batch 2870 / 3125 - Loss:  1.0140 \n",
            "Batch 2880 / 3125 - Loss:  1.0473 \n",
            "Batch 2890 / 3125 - Loss:  1.1092 \n",
            "Batch 2900 / 3125 - Loss:  1.0480 \n",
            "Batch 2910 / 3125 - Loss:  1.0741 \n",
            "Batch 2920 / 3125 - Loss:  0.9976 \n",
            "Batch 2930 / 3125 - Loss:  1.0755 \n",
            "Batch 2940 / 3125 - Loss:  0.9741 \n",
            "Batch 2950 / 3125 - Loss:  1.0742 \n",
            "Batch 2960 / 3125 - Loss:  1.0089 \n",
            "Batch 2970 / 3125 - Loss:  1.0234 \n",
            "Batch 2980 / 3125 - Loss:  1.0212 \n",
            "Batch 2990 / 3125 - Loss:  1.0109 \n",
            "Batch 3000 / 3125 - Loss:  0.9895 \n",
            "Batch 3010 / 3125 - Loss:  1.0515 \n",
            "Batch 3020 / 3125 - Loss:  1.0066 \n",
            "Batch 3030 / 3125 - Loss:  1.0614 \n",
            "Batch 3040 / 3125 - Loss:  1.0627 \n",
            "Batch 3050 / 3125 - Loss:  1.0546 \n",
            "Batch 3060 / 3125 - Loss:  1.0536 \n",
            "Batch 3070 / 3125 - Loss:  0.9450 \n",
            "Batch 3080 / 3125 - Loss:  0.9615 \n",
            "Batch 3090 / 3125 - Loss:  1.0850 \n",
            "Batch 3100 / 3125 - Loss:  1.0132 \n",
            "Batch 3110 / 3125 - Loss:  0.9417 \n",
            "Batch 3120 / 3125 - Loss:  1.0070 \n",
            "Epoch 37 / 50 \n",
            "Batch 0 / 3125 - Loss:  0.9542 \n",
            "Batch 10 / 3125 - Loss:  0.8960 \n",
            "Batch 20 / 3125 - Loss:  1.0285 \n",
            "Batch 30 / 3125 - Loss:  1.0241 \n",
            "Batch 40 / 3125 - Loss:  1.0378 \n",
            "Batch 50 / 3125 - Loss:  0.9695 \n",
            "Batch 60 / 3125 - Loss:  1.0370 \n",
            "Batch 70 / 3125 - Loss:  1.0361 \n",
            "Batch 80 / 3125 - Loss:  1.0472 \n",
            "Batch 90 / 3125 - Loss:  0.9808 \n",
            "Batch 100 / 3125 - Loss:  1.0885 \n",
            "Batch 110 / 3125 - Loss:  1.0429 \n",
            "Batch 120 / 3125 - Loss:  1.0949 \n",
            "Batch 130 / 3125 - Loss:  1.1138 \n",
            "Batch 140 / 3125 - Loss:  1.0381 \n",
            "Batch 150 / 3125 - Loss:  0.9937 \n",
            "Batch 160 / 3125 - Loss:  1.0581 \n",
            "Batch 170 / 3125 - Loss:  1.1004 \n",
            "Batch 180 / 3125 - Loss:  0.9155 \n",
            "Batch 190 / 3125 - Loss:  0.9935 \n",
            "Batch 200 / 3125 - Loss:  0.9944 \n",
            "Batch 210 / 3125 - Loss:  0.9981 \n",
            "Batch 220 / 3125 - Loss:  1.1154 \n",
            "Batch 230 / 3125 - Loss:  1.0799 \n",
            "Batch 240 / 3125 - Loss:  1.0493 \n",
            "Batch 250 / 3125 - Loss:  1.1936 \n",
            "Batch 260 / 3125 - Loss:  1.0805 \n",
            "Batch 270 / 3125 - Loss:  1.0885 \n",
            "Batch 280 / 3125 - Loss:  1.0106 \n",
            "Batch 290 / 3125 - Loss:  1.1919 \n",
            "Batch 300 / 3125 - Loss:  1.1245 \n",
            "Batch 310 / 3125 - Loss:  1.0471 \n",
            "Batch 320 / 3125 - Loss:  1.0413 \n",
            "Batch 330 / 3125 - Loss:  1.0591 \n",
            "Batch 340 / 3125 - Loss:  1.1155 \n",
            "Batch 350 / 3125 - Loss:  0.9464 \n",
            "Batch 360 / 3125 - Loss:  1.0661 \n",
            "Batch 370 / 3125 - Loss:  1.0141 \n",
            "Batch 380 / 3125 - Loss:  1.0736 \n",
            "Batch 390 / 3125 - Loss:  1.0418 \n",
            "Batch 400 / 3125 - Loss:  1.0395 \n",
            "Batch 410 / 3125 - Loss:  1.0598 \n",
            "Batch 420 / 3125 - Loss:  1.0951 \n",
            "Batch 430 / 3125 - Loss:  1.1032 \n",
            "Batch 440 / 3125 - Loss:  1.0388 \n",
            "Batch 450 / 3125 - Loss:  0.9916 \n",
            "Batch 460 / 3125 - Loss:  1.0094 \n",
            "Batch 470 / 3125 - Loss:  1.1080 \n",
            "Batch 480 / 3125 - Loss:  0.9959 \n",
            "Batch 490 / 3125 - Loss:  1.0920 \n",
            "Batch 500 / 3125 - Loss:  1.2361 \n",
            "Batch 510 / 3125 - Loss:  0.9735 \n",
            "Batch 520 / 3125 - Loss:  1.1577 \n",
            "Batch 530 / 3125 - Loss:  0.9942 \n",
            "Batch 540 / 3125 - Loss:  0.9896 \n",
            "Batch 550 / 3125 - Loss:  1.0084 \n",
            "Batch 560 / 3125 - Loss:  1.1167 \n",
            "Batch 570 / 3125 - Loss:  0.9842 \n",
            "Batch 580 / 3125 - Loss:  1.0806 \n",
            "Batch 590 / 3125 - Loss:  0.9906 \n",
            "Batch 600 / 3125 - Loss:  0.9580 \n",
            "Batch 610 / 3125 - Loss:  1.0328 \n",
            "Batch 620 / 3125 - Loss:  0.9859 \n",
            "Batch 630 / 3125 - Loss:  1.0315 \n",
            "Batch 640 / 3125 - Loss:  1.0167 \n",
            "Batch 650 / 3125 - Loss:  0.9717 \n",
            "Batch 660 / 3125 - Loss:  0.9437 \n",
            "Batch 670 / 3125 - Loss:  1.0828 \n",
            "Batch 680 / 3125 - Loss:  1.0501 \n",
            "Batch 690 / 3125 - Loss:  1.1771 \n",
            "Batch 700 / 3125 - Loss:  1.0944 \n",
            "Batch 710 / 3125 - Loss:  1.0685 \n",
            "Batch 720 / 3125 - Loss:  1.1259 \n",
            "Batch 730 / 3125 - Loss:  1.0282 \n",
            "Batch 740 / 3125 - Loss:  1.0627 \n",
            "Batch 750 / 3125 - Loss:  0.9496 \n",
            "Batch 760 / 3125 - Loss:  1.0213 \n",
            "Batch 770 / 3125 - Loss:  0.9813 \n",
            "Batch 780 / 3125 - Loss:  1.0980 \n",
            "Batch 790 / 3125 - Loss:  1.0321 \n",
            "Batch 800 / 3125 - Loss:  1.1864 \n",
            "Batch 810 / 3125 - Loss:  0.9386 \n",
            "Batch 820 / 3125 - Loss:  1.0323 \n",
            "Batch 830 / 3125 - Loss:  1.0835 \n",
            "Batch 840 / 3125 - Loss:  0.9642 \n",
            "Batch 850 / 3125 - Loss:  1.0184 \n",
            "Batch 860 / 3125 - Loss:  0.9507 \n",
            "Batch 870 / 3125 - Loss:  0.9376 \n",
            "Batch 880 / 3125 - Loss:  1.1175 \n",
            "Batch 890 / 3125 - Loss:  1.0528 \n",
            "Batch 900 / 3125 - Loss:  0.9767 \n",
            "Batch 910 / 3125 - Loss:  0.9514 \n",
            "Batch 920 / 3125 - Loss:  1.1504 \n",
            "Batch 930 / 3125 - Loss:  1.1219 \n",
            "Batch 940 / 3125 - Loss:  0.9821 \n",
            "Batch 950 / 3125 - Loss:  0.8946 \n",
            "Batch 960 / 3125 - Loss:  0.9870 \n",
            "Batch 970 / 3125 - Loss:  0.9911 \n",
            "Batch 980 / 3125 - Loss:  1.1576 \n",
            "Batch 990 / 3125 - Loss:  1.0151 \n",
            "Batch 1000 / 3125 - Loss:  1.0550 \n",
            "Batch 1010 / 3125 - Loss:  0.9831 \n",
            "Batch 1020 / 3125 - Loss:  0.9713 \n",
            "Batch 1030 / 3125 - Loss:  1.0210 \n",
            "Batch 1040 / 3125 - Loss:  1.0554 \n",
            "Batch 1050 / 3125 - Loss:  0.9439 \n",
            "Batch 1060 / 3125 - Loss:  0.9639 \n",
            "Batch 1070 / 3125 - Loss:  0.9770 \n",
            "Batch 1080 / 3125 - Loss:  1.0499 \n",
            "Batch 1090 / 3125 - Loss:  1.0421 \n",
            "Batch 1100 / 3125 - Loss:  1.0072 \n",
            "Batch 1110 / 3125 - Loss:  0.9412 \n",
            "Batch 1120 / 3125 - Loss:  0.9936 \n",
            "Batch 1130 / 3125 - Loss:  1.0712 \n",
            "Batch 1140 / 3125 - Loss:  0.9486 \n",
            "Batch 1150 / 3125 - Loss:  1.0800 \n",
            "Batch 1160 / 3125 - Loss:  1.1797 \n",
            "Batch 1170 / 3125 - Loss:  1.0525 \n",
            "Batch 1180 / 3125 - Loss:  1.0153 \n",
            "Batch 1190 / 3125 - Loss:  1.0525 \n",
            "Batch 1200 / 3125 - Loss:  1.0330 \n",
            "Batch 1210 / 3125 - Loss:  0.9972 \n",
            "Batch 1220 / 3125 - Loss:  1.0116 \n",
            "Batch 1230 / 3125 - Loss:  0.9807 \n",
            "Batch 1240 / 3125 - Loss:  1.0256 \n",
            "Batch 1250 / 3125 - Loss:  0.9582 \n",
            "Batch 1260 / 3125 - Loss:  0.8967 \n",
            "Batch 1270 / 3125 - Loss:  1.0580 \n",
            "Batch 1280 / 3125 - Loss:  0.9819 \n",
            "Batch 1290 / 3125 - Loss:  1.0111 \n",
            "Batch 1300 / 3125 - Loss:  1.1133 \n",
            "Batch 1310 / 3125 - Loss:  0.9871 \n",
            "Batch 1320 / 3125 - Loss:  1.1343 \n",
            "Batch 1330 / 3125 - Loss:  1.0103 \n",
            "Batch 1340 / 3125 - Loss:  1.0842 \n",
            "Batch 1350 / 3125 - Loss:  0.9964 \n",
            "Batch 1360 / 3125 - Loss:  1.0594 \n",
            "Batch 1370 / 3125 - Loss:  0.9745 \n",
            "Batch 1380 / 3125 - Loss:  1.0223 \n",
            "Batch 1390 / 3125 - Loss:  1.0954 \n",
            "Batch 1400 / 3125 - Loss:  1.0410 \n",
            "Batch 1410 / 3125 - Loss:  0.9487 \n",
            "Batch 1420 / 3125 - Loss:  1.1120 \n",
            "Batch 1430 / 3125 - Loss:  1.0756 \n",
            "Batch 1440 / 3125 - Loss:  1.0648 \n",
            "Batch 1450 / 3125 - Loss:  0.9593 \n",
            "Batch 1460 / 3125 - Loss:  0.9610 \n",
            "Batch 1470 / 3125 - Loss:  1.0876 \n",
            "Batch 1480 / 3125 - Loss:  1.0988 \n",
            "Batch 1490 / 3125 - Loss:  1.0689 \n",
            "Batch 1500 / 3125 - Loss:  1.0062 \n",
            "Batch 1510 / 3125 - Loss:  1.0590 \n",
            "Batch 1520 / 3125 - Loss:  0.9337 \n",
            "Batch 1530 / 3125 - Loss:  0.9976 \n",
            "Batch 1540 / 3125 - Loss:  1.0367 \n",
            "Batch 1550 / 3125 - Loss:  1.1117 \n",
            "Batch 1560 / 3125 - Loss:  1.0990 \n",
            "Batch 1570 / 3125 - Loss:  0.9877 \n",
            "Batch 1580 / 3125 - Loss:  1.0572 \n",
            "Batch 1590 / 3125 - Loss:  1.0837 \n",
            "Batch 1600 / 3125 - Loss:  0.9753 \n",
            "Batch 1610 / 3125 - Loss:  1.0031 \n",
            "Batch 1620 / 3125 - Loss:  1.0862 \n",
            "Batch 1630 / 3125 - Loss:  0.9831 \n",
            "Batch 1640 / 3125 - Loss:  1.1112 \n",
            "Batch 1650 / 3125 - Loss:  0.9781 \n",
            "Batch 1660 / 3125 - Loss:  1.0150 \n",
            "Batch 1670 / 3125 - Loss:  1.0630 \n",
            "Batch 1680 / 3125 - Loss:  1.0552 \n",
            "Batch 1690 / 3125 - Loss:  1.0216 \n",
            "Batch 1700 / 3125 - Loss:  1.0756 \n",
            "Batch 1710 / 3125 - Loss:  1.1217 \n",
            "Batch 1720 / 3125 - Loss:  1.0703 \n",
            "Batch 1730 / 3125 - Loss:  1.0165 \n",
            "Batch 1740 / 3125 - Loss:  1.0181 \n",
            "Batch 1750 / 3125 - Loss:  1.0584 \n",
            "Batch 1760 / 3125 - Loss:  0.9954 \n",
            "Batch 1770 / 3125 - Loss:  1.0544 \n",
            "Batch 1780 / 3125 - Loss:  1.0013 \n",
            "Batch 1790 / 3125 - Loss:  0.9390 \n",
            "Batch 1800 / 3125 - Loss:  1.0147 \n",
            "Batch 1810 / 3125 - Loss:  1.0970 \n",
            "Batch 1820 / 3125 - Loss:  1.0490 \n",
            "Batch 1830 / 3125 - Loss:  1.0838 \n",
            "Batch 1840 / 3125 - Loss:  1.0131 \n",
            "Batch 1850 / 3125 - Loss:  1.0574 \n",
            "Batch 1860 / 3125 - Loss:  0.9871 \n",
            "Batch 1870 / 3125 - Loss:  1.0845 \n",
            "Batch 1880 / 3125 - Loss:  1.0748 \n",
            "Batch 1890 / 3125 - Loss:  1.0440 \n",
            "Batch 1900 / 3125 - Loss:  1.0474 \n",
            "Batch 1910 / 3125 - Loss:  0.9950 \n",
            "Batch 1920 / 3125 - Loss:  0.9989 \n",
            "Batch 1930 / 3125 - Loss:  0.9854 \n",
            "Batch 1940 / 3125 - Loss:  0.9686 \n",
            "Batch 1950 / 3125 - Loss:  0.9664 \n",
            "Batch 1960 / 3125 - Loss:  1.0183 \n",
            "Batch 1970 / 3125 - Loss:  0.9916 \n",
            "Batch 1980 / 3125 - Loss:  0.9646 \n",
            "Batch 1990 / 3125 - Loss:  0.9533 \n",
            "Batch 2000 / 3125 - Loss:  1.0380 \n",
            "Batch 2010 / 3125 - Loss:  0.9722 \n",
            "Batch 2020 / 3125 - Loss:  1.1004 \n",
            "Batch 2030 / 3125 - Loss:  1.0789 \n",
            "Batch 2040 / 3125 - Loss:  0.9443 \n",
            "Batch 2050 / 3125 - Loss:  0.9721 \n",
            "Batch 2060 / 3125 - Loss:  1.0801 \n",
            "Batch 2070 / 3125 - Loss:  1.1451 \n",
            "Batch 2080 / 3125 - Loss:  1.0138 \n",
            "Batch 2090 / 3125 - Loss:  1.0768 \n",
            "Batch 2100 / 3125 - Loss:  0.9294 \n",
            "Batch 2110 / 3125 - Loss:  0.9784 \n",
            "Batch 2120 / 3125 - Loss:  1.0209 \n",
            "Batch 2130 / 3125 - Loss:  0.9699 \n",
            "Batch 2140 / 3125 - Loss:  1.0487 \n",
            "Batch 2150 / 3125 - Loss:  1.0855 \n",
            "Batch 2160 / 3125 - Loss:  1.0753 \n",
            "Batch 2170 / 3125 - Loss:  1.0189 \n",
            "Batch 2180 / 3125 - Loss:  1.0080 \n",
            "Batch 2190 / 3125 - Loss:  1.1045 \n",
            "Batch 2200 / 3125 - Loss:  0.9272 \n",
            "Batch 2210 / 3125 - Loss:  0.9436 \n",
            "Batch 2220 / 3125 - Loss:  1.0537 \n",
            "Batch 2230 / 3125 - Loss:  1.0281 \n",
            "Batch 2240 / 3125 - Loss:  1.0204 \n",
            "Batch 2250 / 3125 - Loss:  1.1369 \n",
            "Batch 2260 / 3125 - Loss:  1.0540 \n",
            "Batch 2270 / 3125 - Loss:  1.0227 \n",
            "Batch 2280 / 3125 - Loss:  0.9701 \n",
            "Batch 2290 / 3125 - Loss:  1.0209 \n",
            "Batch 2300 / 3125 - Loss:  0.9695 \n",
            "Batch 2310 / 3125 - Loss:  1.0196 \n",
            "Batch 2320 / 3125 - Loss:  1.1284 \n",
            "Batch 2330 / 3125 - Loss:  1.0676 \n",
            "Batch 2340 / 3125 - Loss:  1.1014 \n",
            "Batch 2350 / 3125 - Loss:  1.0327 \n",
            "Batch 2360 / 3125 - Loss:  1.0785 \n",
            "Batch 2370 / 3125 - Loss:  0.9916 \n",
            "Batch 2380 / 3125 - Loss:  1.0415 \n",
            "Batch 2390 / 3125 - Loss:  0.9790 \n",
            "Batch 2400 / 3125 - Loss:  1.0166 \n",
            "Batch 2410 / 3125 - Loss:  1.1212 \n",
            "Batch 2420 / 3125 - Loss:  1.0560 \n",
            "Batch 2430 / 3125 - Loss:  1.0516 \n",
            "Batch 2440 / 3125 - Loss:  1.0479 \n",
            "Batch 2450 / 3125 - Loss:  1.1495 \n",
            "Batch 2460 / 3125 - Loss:  1.0488 \n",
            "Batch 2470 / 3125 - Loss:  0.9870 \n",
            "Batch 2480 / 3125 - Loss:  1.0767 \n",
            "Batch 2490 / 3125 - Loss:  0.9325 \n",
            "Batch 2500 / 3125 - Loss:  0.9617 \n",
            "Batch 2510 / 3125 - Loss:  1.0418 \n",
            "Batch 2520 / 3125 - Loss:  1.0135 \n",
            "Batch 2530 / 3125 - Loss:  1.1190 \n",
            "Batch 2540 / 3125 - Loss:  1.0085 \n",
            "Batch 2550 / 3125 - Loss:  1.0527 \n",
            "Batch 2560 / 3125 - Loss:  0.8964 \n",
            "Batch 2570 / 3125 - Loss:  1.0625 \n",
            "Batch 2580 / 3125 - Loss:  1.0308 \n",
            "Batch 2590 / 3125 - Loss:  1.0899 \n",
            "Batch 2600 / 3125 - Loss:  1.1777 \n",
            "Batch 2610 / 3125 - Loss:  0.9210 \n",
            "Batch 2620 / 3125 - Loss:  1.0653 \n",
            "Batch 2630 / 3125 - Loss:  1.0538 \n",
            "Batch 2640 / 3125 - Loss:  0.9974 \n",
            "Batch 2650 / 3125 - Loss:  1.0220 \n",
            "Batch 2660 / 3125 - Loss:  0.9773 \n",
            "Batch 2670 / 3125 - Loss:  1.0630 \n",
            "Batch 2680 / 3125 - Loss:  1.0346 \n",
            "Batch 2690 / 3125 - Loss:  1.0408 \n",
            "Batch 2700 / 3125 - Loss:  1.0426 \n",
            "Batch 2710 / 3125 - Loss:  0.9640 \n",
            "Batch 2720 / 3125 - Loss:  1.0941 \n",
            "Batch 2730 / 3125 - Loss:  0.9878 \n",
            "Batch 2740 / 3125 - Loss:  1.0416 \n",
            "Batch 2750 / 3125 - Loss:  0.9469 \n",
            "Batch 2760 / 3125 - Loss:  1.1244 \n",
            "Batch 2770 / 3125 - Loss:  1.0129 \n",
            "Batch 2780 / 3125 - Loss:  0.9593 \n",
            "Batch 2790 / 3125 - Loss:  1.1710 \n",
            "Batch 2800 / 3125 - Loss:  1.0540 \n",
            "Batch 2810 / 3125 - Loss:  1.0203 \n",
            "Batch 2820 / 3125 - Loss:  1.1431 \n",
            "Batch 2830 / 3125 - Loss:  0.9866 \n",
            "Batch 2840 / 3125 - Loss:  1.0991 \n",
            "Batch 2850 / 3125 - Loss:  0.9697 \n",
            "Batch 2860 / 3125 - Loss:  1.0902 \n",
            "Batch 2870 / 3125 - Loss:  0.9203 \n",
            "Batch 2880 / 3125 - Loss:  1.0099 \n",
            "Batch 2890 / 3125 - Loss:  1.1599 \n",
            "Batch 2900 / 3125 - Loss:  1.0092 \n",
            "Batch 2910 / 3125 - Loss:  1.0280 \n",
            "Batch 2920 / 3125 - Loss:  1.0897 \n",
            "Batch 2930 / 3125 - Loss:  1.1146 \n",
            "Batch 2940 / 3125 - Loss:  0.9936 \n",
            "Batch 2950 / 3125 - Loss:  1.1404 \n",
            "Batch 2960 / 3125 - Loss:  0.9864 \n",
            "Batch 2970 / 3125 - Loss:  1.0699 \n",
            "Batch 2980 / 3125 - Loss:  0.9619 \n",
            "Batch 2990 / 3125 - Loss:  1.0819 \n",
            "Batch 3000 / 3125 - Loss:  1.0244 \n",
            "Batch 3010 / 3125 - Loss:  1.0477 \n",
            "Batch 3020 / 3125 - Loss:  0.9644 \n",
            "Batch 3030 / 3125 - Loss:  1.0867 \n",
            "Batch 3040 / 3125 - Loss:  1.0297 \n",
            "Batch 3050 / 3125 - Loss:  1.0060 \n",
            "Batch 3060 / 3125 - Loss:  1.0698 \n",
            "Batch 3070 / 3125 - Loss:  1.0072 \n",
            "Batch 3080 / 3125 - Loss:  1.0568 \n",
            "Batch 3090 / 3125 - Loss:  1.0263 \n",
            "Batch 3100 / 3125 - Loss:  0.9432 \n",
            "Batch 3110 / 3125 - Loss:  1.0407 \n",
            "Batch 3120 / 3125 - Loss:  1.1241 \n",
            "Epoch 38 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.1197 \n",
            "Batch 10 / 3125 - Loss:  0.9964 \n",
            "Batch 20 / 3125 - Loss:  0.9978 \n",
            "Batch 30 / 3125 - Loss:  1.0524 \n",
            "Batch 40 / 3125 - Loss:  1.0176 \n",
            "Batch 50 / 3125 - Loss:  1.0549 \n",
            "Batch 60 / 3125 - Loss:  0.9567 \n",
            "Batch 70 / 3125 - Loss:  1.0156 \n",
            "Batch 80 / 3125 - Loss:  1.1071 \n",
            "Batch 90 / 3125 - Loss:  0.9845 \n",
            "Batch 100 / 3125 - Loss:  0.9238 \n",
            "Batch 110 / 3125 - Loss:  0.9936 \n",
            "Batch 120 / 3125 - Loss:  0.9894 \n",
            "Batch 130 / 3125 - Loss:  0.9436 \n",
            "Batch 140 / 3125 - Loss:  1.0332 \n",
            "Batch 150 / 3125 - Loss:  1.0062 \n",
            "Batch 160 / 3125 - Loss:  1.0323 \n",
            "Batch 170 / 3125 - Loss:  1.0708 \n",
            "Batch 180 / 3125 - Loss:  1.0353 \n",
            "Batch 190 / 3125 - Loss:  1.0665 \n",
            "Batch 200 / 3125 - Loss:  1.0077 \n",
            "Batch 210 / 3125 - Loss:  1.0694 \n",
            "Batch 220 / 3125 - Loss:  1.0627 \n",
            "Batch 230 / 3125 - Loss:  1.1691 \n",
            "Batch 240 / 3125 - Loss:  1.0480 \n",
            "Batch 250 / 3125 - Loss:  0.9849 \n",
            "Batch 260 / 3125 - Loss:  1.0518 \n",
            "Batch 270 / 3125 - Loss:  1.0350 \n",
            "Batch 280 / 3125 - Loss:  0.9994 \n",
            "Batch 290 / 3125 - Loss:  1.0293 \n",
            "Batch 300 / 3125 - Loss:  1.1338 \n",
            "Batch 310 / 3125 - Loss:  1.1147 \n",
            "Batch 320 / 3125 - Loss:  1.0746 \n",
            "Batch 330 / 3125 - Loss:  1.0230 \n",
            "Batch 340 / 3125 - Loss:  1.0588 \n",
            "Batch 350 / 3125 - Loss:  1.0233 \n",
            "Batch 360 / 3125 - Loss:  0.9884 \n",
            "Batch 370 / 3125 - Loss:  1.0477 \n",
            "Batch 380 / 3125 - Loss:  1.1236 \n",
            "Batch 390 / 3125 - Loss:  1.0888 \n",
            "Batch 400 / 3125 - Loss:  1.0087 \n",
            "Batch 410 / 3125 - Loss:  1.0397 \n",
            "Batch 420 / 3125 - Loss:  1.0420 \n",
            "Batch 430 / 3125 - Loss:  1.0403 \n",
            "Batch 440 / 3125 - Loss:  1.0379 \n",
            "Batch 450 / 3125 - Loss:  1.0525 \n",
            "Batch 460 / 3125 - Loss:  1.0558 \n",
            "Batch 470 / 3125 - Loss:  0.9760 \n",
            "Batch 480 / 3125 - Loss:  0.9974 \n",
            "Batch 490 / 3125 - Loss:  1.0668 \n",
            "Batch 500 / 3125 - Loss:  1.0622 \n",
            "Batch 510 / 3125 - Loss:  1.0888 \n",
            "Batch 520 / 3125 - Loss:  1.0065 \n",
            "Batch 530 / 3125 - Loss:  1.0484 \n",
            "Batch 540 / 3125 - Loss:  0.9771 \n",
            "Batch 550 / 3125 - Loss:  1.0927 \n",
            "Batch 560 / 3125 - Loss:  0.9565 \n",
            "Batch 570 / 3125 - Loss:  1.0090 \n",
            "Batch 580 / 3125 - Loss:  0.9783 \n",
            "Batch 590 / 3125 - Loss:  1.0962 \n",
            "Batch 600 / 3125 - Loss:  1.0697 \n",
            "Batch 610 / 3125 - Loss:  1.0928 \n",
            "Batch 620 / 3125 - Loss:  1.0043 \n",
            "Batch 630 / 3125 - Loss:  0.9840 \n",
            "Batch 640 / 3125 - Loss:  0.9282 \n",
            "Batch 650 / 3125 - Loss:  1.0769 \n",
            "Batch 660 / 3125 - Loss:  1.1318 \n",
            "Batch 670 / 3125 - Loss:  1.0320 \n",
            "Batch 680 / 3125 - Loss:  1.0155 \n",
            "Batch 690 / 3125 - Loss:  0.9293 \n",
            "Batch 700 / 3125 - Loss:  1.0386 \n",
            "Batch 710 / 3125 - Loss:  0.8963 \n",
            "Batch 720 / 3125 - Loss:  1.0011 \n",
            "Batch 730 / 3125 - Loss:  1.0755 \n",
            "Batch 740 / 3125 - Loss:  0.9807 \n",
            "Batch 750 / 3125 - Loss:  0.9732 \n",
            "Batch 760 / 3125 - Loss:  0.9785 \n",
            "Batch 770 / 3125 - Loss:  1.1037 \n",
            "Batch 780 / 3125 - Loss:  1.0994 \n",
            "Batch 790 / 3125 - Loss:  1.0395 \n",
            "Batch 800 / 3125 - Loss:  1.0139 \n",
            "Batch 810 / 3125 - Loss:  1.0756 \n",
            "Batch 820 / 3125 - Loss:  0.9614 \n",
            "Batch 830 / 3125 - Loss:  0.9974 \n",
            "Batch 840 / 3125 - Loss:  1.1379 \n",
            "Batch 850 / 3125 - Loss:  0.9970 \n",
            "Batch 860 / 3125 - Loss:  0.9837 \n",
            "Batch 870 / 3125 - Loss:  0.9877 \n",
            "Batch 880 / 3125 - Loss:  1.0176 \n",
            "Batch 890 / 3125 - Loss:  1.0075 \n",
            "Batch 900 / 3125 - Loss:  1.1108 \n",
            "Batch 910 / 3125 - Loss:  1.0976 \n",
            "Batch 920 / 3125 - Loss:  1.0137 \n",
            "Batch 930 / 3125 - Loss:  0.9924 \n",
            "Batch 940 / 3125 - Loss:  1.0298 \n",
            "Batch 950 / 3125 - Loss:  1.1006 \n",
            "Batch 960 / 3125 - Loss:  1.1636 \n",
            "Batch 970 / 3125 - Loss:  1.1825 \n",
            "Batch 980 / 3125 - Loss:  1.1634 \n",
            "Batch 990 / 3125 - Loss:  0.9988 \n",
            "Batch 1000 / 3125 - Loss:  0.8981 \n",
            "Batch 1010 / 3125 - Loss:  0.8886 \n",
            "Batch 1020 / 3125 - Loss:  0.9717 \n",
            "Batch 1030 / 3125 - Loss:  1.0313 \n",
            "Batch 1040 / 3125 - Loss:  1.0652 \n",
            "Batch 1050 / 3125 - Loss:  0.9983 \n",
            "Batch 1060 / 3125 - Loss:  1.0228 \n",
            "Batch 1070 / 3125 - Loss:  1.1235 \n",
            "Batch 1080 / 3125 - Loss:  0.8957 \n",
            "Batch 1090 / 3125 - Loss:  0.9942 \n",
            "Batch 1100 / 3125 - Loss:  1.1042 \n",
            "Batch 1110 / 3125 - Loss:  1.0709 \n",
            "Batch 1120 / 3125 - Loss:  0.9721 \n",
            "Batch 1130 / 3125 - Loss:  1.0161 \n",
            "Batch 1140 / 3125 - Loss:  0.9946 \n",
            "Batch 1150 / 3125 - Loss:  1.0198 \n",
            "Batch 1160 / 3125 - Loss:  1.0529 \n",
            "Batch 1170 / 3125 - Loss:  1.0634 \n",
            "Batch 1180 / 3125 - Loss:  1.0330 \n",
            "Batch 1190 / 3125 - Loss:  0.9445 \n",
            "Batch 1200 / 3125 - Loss:  1.1022 \n",
            "Batch 1210 / 3125 - Loss:  0.9480 \n",
            "Batch 1220 / 3125 - Loss:  1.0610 \n",
            "Batch 1230 / 3125 - Loss:  1.1303 \n",
            "Batch 1240 / 3125 - Loss:  0.9369 \n",
            "Batch 1250 / 3125 - Loss:  0.9782 \n",
            "Batch 1260 / 3125 - Loss:  0.9443 \n",
            "Batch 1270 / 3125 - Loss:  0.9918 \n",
            "Batch 1280 / 3125 - Loss:  0.8935 \n",
            "Batch 1290 / 3125 - Loss:  1.0885 \n",
            "Batch 1300 / 3125 - Loss:  1.0250 \n",
            "Batch 1310 / 3125 - Loss:  1.1449 \n",
            "Batch 1320 / 3125 - Loss:  1.1448 \n",
            "Batch 1330 / 3125 - Loss:  1.1573 \n",
            "Batch 1340 / 3125 - Loss:  1.0023 \n",
            "Batch 1350 / 3125 - Loss:  1.0717 \n",
            "Batch 1360 / 3125 - Loss:  1.1496 \n",
            "Batch 1370 / 3125 - Loss:  1.0072 \n",
            "Batch 1380 / 3125 - Loss:  0.9416 \n",
            "Batch 1390 / 3125 - Loss:  1.1973 \n",
            "Batch 1400 / 3125 - Loss:  1.0238 \n",
            "Batch 1410 / 3125 - Loss:  1.0668 \n",
            "Batch 1420 / 3125 - Loss:  1.0052 \n",
            "Batch 1430 / 3125 - Loss:  1.1064 \n",
            "Batch 1440 / 3125 - Loss:  1.1508 \n",
            "Batch 1450 / 3125 - Loss:  1.0972 \n",
            "Batch 1460 / 3125 - Loss:  1.0025 \n",
            "Batch 1470 / 3125 - Loss:  1.0061 \n",
            "Batch 1480 / 3125 - Loss:  1.1120 \n",
            "Batch 1490 / 3125 - Loss:  0.9667 \n",
            "Batch 1500 / 3125 - Loss:  1.0258 \n",
            "Batch 1510 / 3125 - Loss:  1.0909 \n",
            "Batch 1520 / 3125 - Loss:  1.0424 \n",
            "Batch 1530 / 3125 - Loss:  0.8747 \n",
            "Batch 1540 / 3125 - Loss:  0.9637 \n",
            "Batch 1550 / 3125 - Loss:  1.0445 \n",
            "Batch 1560 / 3125 - Loss:  0.9358 \n",
            "Batch 1570 / 3125 - Loss:  1.0291 \n",
            "Batch 1580 / 3125 - Loss:  1.0116 \n",
            "Batch 1590 / 3125 - Loss:  0.9667 \n",
            "Batch 1600 / 3125 - Loss:  0.9495 \n",
            "Batch 1610 / 3125 - Loss:  1.0493 \n",
            "Batch 1620 / 3125 - Loss:  1.1154 \n",
            "Batch 1630 / 3125 - Loss:  1.1035 \n",
            "Batch 1640 / 3125 - Loss:  1.0720 \n",
            "Batch 1650 / 3125 - Loss:  1.0872 \n",
            "Batch 1660 / 3125 - Loss:  1.0943 \n",
            "Batch 1670 / 3125 - Loss:  0.9811 \n",
            "Batch 1680 / 3125 - Loss:  1.0524 \n",
            "Batch 1690 / 3125 - Loss:  1.1274 \n",
            "Batch 1700 / 3125 - Loss:  1.0301 \n",
            "Batch 1710 / 3125 - Loss:  1.0498 \n",
            "Batch 1720 / 3125 - Loss:  0.9836 \n",
            "Batch 1730 / 3125 - Loss:  1.0470 \n",
            "Batch 1740 / 3125 - Loss:  0.9910 \n",
            "Batch 1750 / 3125 - Loss:  0.9658 \n",
            "Batch 1760 / 3125 - Loss:  1.1030 \n",
            "Batch 1770 / 3125 - Loss:  1.0978 \n",
            "Batch 1780 / 3125 - Loss:  1.0123 \n",
            "Batch 1790 / 3125 - Loss:  1.0215 \n",
            "Batch 1800 / 3125 - Loss:  0.9484 \n",
            "Batch 1810 / 3125 - Loss:  1.0002 \n",
            "Batch 1820 / 3125 - Loss:  1.0196 \n",
            "Batch 1830 / 3125 - Loss:  1.0448 \n",
            "Batch 1840 / 3125 - Loss:  1.0231 \n",
            "Batch 1850 / 3125 - Loss:  1.0383 \n",
            "Batch 1860 / 3125 - Loss:  1.0557 \n",
            "Batch 1870 / 3125 - Loss:  1.0303 \n",
            "Batch 1880 / 3125 - Loss:  1.0146 \n",
            "Batch 1890 / 3125 - Loss:  1.0010 \n",
            "Batch 1900 / 3125 - Loss:  1.0240 \n",
            "Batch 1910 / 3125 - Loss:  1.0265 \n",
            "Batch 1920 / 3125 - Loss:  1.0718 \n",
            "Batch 1930 / 3125 - Loss:  1.0651 \n",
            "Batch 1940 / 3125 - Loss:  0.9812 \n",
            "Batch 1950 / 3125 - Loss:  1.0726 \n",
            "Batch 1960 / 3125 - Loss:  1.1580 \n",
            "Batch 1970 / 3125 - Loss:  1.0169 \n",
            "Batch 1980 / 3125 - Loss:  1.0588 \n",
            "Batch 1990 / 3125 - Loss:  1.1045 \n",
            "Batch 2000 / 3125 - Loss:  1.1017 \n",
            "Batch 2010 / 3125 - Loss:  1.1399 \n",
            "Batch 2020 / 3125 - Loss:  0.9699 \n",
            "Batch 2030 / 3125 - Loss:  1.0963 \n",
            "Batch 2040 / 3125 - Loss:  1.0043 \n",
            "Batch 2050 / 3125 - Loss:  1.0106 \n",
            "Batch 2060 / 3125 - Loss:  0.9360 \n",
            "Batch 2070 / 3125 - Loss:  1.1304 \n",
            "Batch 2080 / 3125 - Loss:  1.0175 \n",
            "Batch 2090 / 3125 - Loss:  0.9924 \n",
            "Batch 2100 / 3125 - Loss:  0.9267 \n",
            "Batch 2110 / 3125 - Loss:  1.1101 \n",
            "Batch 2120 / 3125 - Loss:  0.9812 \n",
            "Batch 2130 / 3125 - Loss:  1.0026 \n",
            "Batch 2140 / 3125 - Loss:  1.0454 \n",
            "Batch 2150 / 3125 - Loss:  0.9872 \n",
            "Batch 2160 / 3125 - Loss:  1.0926 \n",
            "Batch 2170 / 3125 - Loss:  1.0014 \n",
            "Batch 2180 / 3125 - Loss:  1.0582 \n",
            "Batch 2190 / 3125 - Loss:  1.0420 \n",
            "Batch 2200 / 3125 - Loss:  1.0137 \n",
            "Batch 2210 / 3125 - Loss:  1.1092 \n",
            "Batch 2220 / 3125 - Loss:  0.9620 \n",
            "Batch 2230 / 3125 - Loss:  1.0021 \n",
            "Batch 2240 / 3125 - Loss:  0.9860 \n",
            "Batch 2250 / 3125 - Loss:  0.9498 \n",
            "Batch 2260 / 3125 - Loss:  1.0616 \n",
            "Batch 2270 / 3125 - Loss:  1.0224 \n",
            "Batch 2280 / 3125 - Loss:  1.0630 \n",
            "Batch 2290 / 3125 - Loss:  0.9978 \n",
            "Batch 2300 / 3125 - Loss:  0.9215 \n",
            "Batch 2310 / 3125 - Loss:  1.0212 \n",
            "Batch 2320 / 3125 - Loss:  0.9926 \n",
            "Batch 2330 / 3125 - Loss:  1.0858 \n",
            "Batch 2340 / 3125 - Loss:  0.9977 \n",
            "Batch 2350 / 3125 - Loss:  1.0425 \n",
            "Batch 2360 / 3125 - Loss:  1.1000 \n",
            "Batch 2370 / 3125 - Loss:  1.0064 \n",
            "Batch 2380 / 3125 - Loss:  1.0840 \n",
            "Batch 2390 / 3125 - Loss:  0.9390 \n",
            "Batch 2400 / 3125 - Loss:  1.0499 \n",
            "Batch 2410 / 3125 - Loss:  1.0558 \n",
            "Batch 2420 / 3125 - Loss:  1.0907 \n",
            "Batch 2430 / 3125 - Loss:  1.0245 \n",
            "Batch 2440 / 3125 - Loss:  1.0149 \n",
            "Batch 2450 / 3125 - Loss:  1.0476 \n",
            "Batch 2460 / 3125 - Loss:  0.9830 \n",
            "Batch 2470 / 3125 - Loss:  0.9555 \n",
            "Batch 2480 / 3125 - Loss:  1.0147 \n",
            "Batch 2490 / 3125 - Loss:  1.0023 \n",
            "Batch 2500 / 3125 - Loss:  1.0933 \n",
            "Batch 2510 / 3125 - Loss:  1.0603 \n",
            "Batch 2520 / 3125 - Loss:  1.0118 \n",
            "Batch 2530 / 3125 - Loss:  0.9857 \n",
            "Batch 2540 / 3125 - Loss:  1.0844 \n",
            "Batch 2550 / 3125 - Loss:  1.0964 \n",
            "Batch 2560 / 3125 - Loss:  0.9677 \n",
            "Batch 2570 / 3125 - Loss:  1.0038 \n",
            "Batch 2580 / 3125 - Loss:  1.0333 \n",
            "Batch 2590 / 3125 - Loss:  1.1162 \n",
            "Batch 2600 / 3125 - Loss:  1.0357 \n",
            "Batch 2610 / 3125 - Loss:  1.1212 \n",
            "Batch 2620 / 3125 - Loss:  1.1076 \n",
            "Batch 2630 / 3125 - Loss:  1.0668 \n",
            "Batch 2640 / 3125 - Loss:  0.9995 \n",
            "Batch 2650 / 3125 - Loss:  1.0415 \n",
            "Batch 2660 / 3125 - Loss:  1.0677 \n",
            "Batch 2670 / 3125 - Loss:  1.0770 \n",
            "Batch 2680 / 3125 - Loss:  0.9978 \n",
            "Batch 2690 / 3125 - Loss:  1.0952 \n",
            "Batch 2700 / 3125 - Loss:  1.0251 \n",
            "Batch 2710 / 3125 - Loss:  0.9758 \n",
            "Batch 2720 / 3125 - Loss:  0.9511 \n",
            "Batch 2730 / 3125 - Loss:  1.0017 \n",
            "Batch 2740 / 3125 - Loss:  0.9641 \n",
            "Batch 2750 / 3125 - Loss:  0.9844 \n",
            "Batch 2760 / 3125 - Loss:  1.0817 \n",
            "Batch 2770 / 3125 - Loss:  1.0235 \n",
            "Batch 2780 / 3125 - Loss:  1.0200 \n",
            "Batch 2790 / 3125 - Loss:  0.9711 \n",
            "Batch 2800 / 3125 - Loss:  1.0683 \n",
            "Batch 2810 / 3125 - Loss:  1.0437 \n",
            "Batch 2820 / 3125 - Loss:  1.0905 \n",
            "Batch 2830 / 3125 - Loss:  0.9882 \n",
            "Batch 2840 / 3125 - Loss:  1.0150 \n",
            "Batch 2850 / 3125 - Loss:  1.0230 \n",
            "Batch 2860 / 3125 - Loss:  1.1147 \n",
            "Batch 2870 / 3125 - Loss:  1.1280 \n",
            "Batch 2880 / 3125 - Loss:  1.1235 \n",
            "Batch 2890 / 3125 - Loss:  0.9644 \n",
            "Batch 2900 / 3125 - Loss:  1.0053 \n",
            "Batch 2910 / 3125 - Loss:  1.0306 \n",
            "Batch 2920 / 3125 - Loss:  0.9489 \n",
            "Batch 2930 / 3125 - Loss:  1.0712 \n",
            "Batch 2940 / 3125 - Loss:  1.0511 \n",
            "Batch 2950 / 3125 - Loss:  0.9555 \n",
            "Batch 2960 / 3125 - Loss:  0.9872 \n",
            "Batch 2970 / 3125 - Loss:  1.0563 \n",
            "Batch 2980 / 3125 - Loss:  1.0621 \n",
            "Batch 2990 / 3125 - Loss:  1.0557 \n",
            "Batch 3000 / 3125 - Loss:  1.0774 \n",
            "Batch 3010 / 3125 - Loss:  0.9633 \n",
            "Batch 3020 / 3125 - Loss:  0.9298 \n",
            "Batch 3030 / 3125 - Loss:  1.0828 \n",
            "Batch 3040 / 3125 - Loss:  0.9956 \n",
            "Batch 3050 / 3125 - Loss:  1.1159 \n",
            "Batch 3060 / 3125 - Loss:  1.0822 \n",
            "Batch 3070 / 3125 - Loss:  0.9692 \n",
            "Batch 3080 / 3125 - Loss:  1.0241 \n",
            "Batch 3090 / 3125 - Loss:  1.1380 \n",
            "Batch 3100 / 3125 - Loss:  0.8437 \n",
            "Batch 3110 / 3125 - Loss:  0.9429 \n",
            "Batch 3120 / 3125 - Loss:  0.9716 \n",
            "Epoch 39 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0646 \n",
            "Batch 10 / 3125 - Loss:  1.1014 \n",
            "Batch 20 / 3125 - Loss:  0.9712 \n",
            "Batch 30 / 3125 - Loss:  0.9465 \n",
            "Batch 40 / 3125 - Loss:  1.0117 \n",
            "Batch 50 / 3125 - Loss:  1.0748 \n",
            "Batch 60 / 3125 - Loss:  0.9672 \n",
            "Batch 70 / 3125 - Loss:  1.0793 \n",
            "Batch 80 / 3125 - Loss:  1.1205 \n",
            "Batch 90 / 3125 - Loss:  1.0370 \n",
            "Batch 100 / 3125 - Loss:  1.0138 \n",
            "Batch 110 / 3125 - Loss:  0.9484 \n",
            "Batch 120 / 3125 - Loss:  1.0597 \n",
            "Batch 130 / 3125 - Loss:  1.0843 \n",
            "Batch 140 / 3125 - Loss:  0.9911 \n",
            "Batch 150 / 3125 - Loss:  1.0000 \n",
            "Batch 160 / 3125 - Loss:  1.1138 \n",
            "Batch 170 / 3125 - Loss:  0.9867 \n",
            "Batch 180 / 3125 - Loss:  0.9895 \n",
            "Batch 190 / 3125 - Loss:  1.0499 \n",
            "Batch 200 / 3125 - Loss:  1.0893 \n",
            "Batch 210 / 3125 - Loss:  1.0451 \n",
            "Batch 220 / 3125 - Loss:  1.0380 \n",
            "Batch 230 / 3125 - Loss:  1.0645 \n",
            "Batch 240 / 3125 - Loss:  1.0463 \n",
            "Batch 250 / 3125 - Loss:  1.0297 \n",
            "Batch 260 / 3125 - Loss:  1.1069 \n",
            "Batch 270 / 3125 - Loss:  0.9879 \n",
            "Batch 280 / 3125 - Loss:  1.0454 \n",
            "Batch 290 / 3125 - Loss:  1.0710 \n",
            "Batch 300 / 3125 - Loss:  0.9821 \n",
            "Batch 310 / 3125 - Loss:  1.0207 \n",
            "Batch 320 / 3125 - Loss:  1.0629 \n",
            "Batch 330 / 3125 - Loss:  1.0569 \n",
            "Batch 340 / 3125 - Loss:  0.9721 \n",
            "Batch 350 / 3125 - Loss:  1.1265 \n",
            "Batch 360 / 3125 - Loss:  1.1346 \n",
            "Batch 370 / 3125 - Loss:  1.0695 \n",
            "Batch 380 / 3125 - Loss:  1.1118 \n",
            "Batch 390 / 3125 - Loss:  0.9329 \n",
            "Batch 400 / 3125 - Loss:  1.0814 \n",
            "Batch 410 / 3125 - Loss:  0.9808 \n",
            "Batch 420 / 3125 - Loss:  1.0483 \n",
            "Batch 430 / 3125 - Loss:  1.0231 \n",
            "Batch 440 / 3125 - Loss:  0.9795 \n",
            "Batch 450 / 3125 - Loss:  1.1540 \n",
            "Batch 460 / 3125 - Loss:  1.1898 \n",
            "Batch 470 / 3125 - Loss:  0.9985 \n",
            "Batch 480 / 3125 - Loss:  1.0426 \n",
            "Batch 490 / 3125 - Loss:  1.0556 \n",
            "Batch 500 / 3125 - Loss:  1.0360 \n",
            "Batch 510 / 3125 - Loss:  1.1148 \n",
            "Batch 520 / 3125 - Loss:  1.0499 \n",
            "Batch 530 / 3125 - Loss:  1.1129 \n",
            "Batch 540 / 3125 - Loss:  1.0366 \n",
            "Batch 550 / 3125 - Loss:  0.9934 \n",
            "Batch 560 / 3125 - Loss:  1.1108 \n",
            "Batch 570 / 3125 - Loss:  1.0036 \n",
            "Batch 580 / 3125 - Loss:  0.9863 \n",
            "Batch 590 / 3125 - Loss:  1.0445 \n",
            "Batch 600 / 3125 - Loss:  0.8959 \n",
            "Batch 610 / 3125 - Loss:  1.1919 \n",
            "Batch 620 / 3125 - Loss:  1.1104 \n",
            "Batch 630 / 3125 - Loss:  1.0748 \n",
            "Batch 640 / 3125 - Loss:  1.1232 \n",
            "Batch 650 / 3125 - Loss:  0.9902 \n",
            "Batch 660 / 3125 - Loss:  1.0989 \n",
            "Batch 670 / 3125 - Loss:  1.0739 \n",
            "Batch 680 / 3125 - Loss:  0.9610 \n",
            "Batch 690 / 3125 - Loss:  0.9432 \n",
            "Batch 700 / 3125 - Loss:  1.0927 \n",
            "Batch 710 / 3125 - Loss:  1.0214 \n",
            "Batch 720 / 3125 - Loss:  1.0481 \n",
            "Batch 730 / 3125 - Loss:  1.0206 \n",
            "Batch 740 / 3125 - Loss:  1.1458 \n",
            "Batch 750 / 3125 - Loss:  0.9936 \n",
            "Batch 760 / 3125 - Loss:  0.9786 \n",
            "Batch 770 / 3125 - Loss:  1.0678 \n",
            "Batch 780 / 3125 - Loss:  0.9741 \n",
            "Batch 790 / 3125 - Loss:  0.9583 \n",
            "Batch 800 / 3125 - Loss:  1.1178 \n",
            "Batch 810 / 3125 - Loss:  1.0081 \n",
            "Batch 820 / 3125 - Loss:  1.1178 \n",
            "Batch 830 / 3125 - Loss:  1.0191 \n",
            "Batch 840 / 3125 - Loss:  0.9627 \n",
            "Batch 850 / 3125 - Loss:  1.0102 \n",
            "Batch 860 / 3125 - Loss:  1.1567 \n",
            "Batch 870 / 3125 - Loss:  1.0004 \n",
            "Batch 880 / 3125 - Loss:  0.8966 \n",
            "Batch 890 / 3125 - Loss:  0.9938 \n",
            "Batch 900 / 3125 - Loss:  1.1064 \n",
            "Batch 910 / 3125 - Loss:  1.0535 \n",
            "Batch 920 / 3125 - Loss:  1.0410 \n",
            "Batch 930 / 3125 - Loss:  1.0379 \n",
            "Batch 940 / 3125 - Loss:  1.0151 \n",
            "Batch 950 / 3125 - Loss:  1.0610 \n",
            "Batch 960 / 3125 - Loss:  1.0198 \n",
            "Batch 970 / 3125 - Loss:  1.1571 \n",
            "Batch 980 / 3125 - Loss:  0.8959 \n",
            "Batch 990 / 3125 - Loss:  1.0852 \n",
            "Batch 1000 / 3125 - Loss:  1.0310 \n",
            "Batch 1010 / 3125 - Loss:  1.0385 \n",
            "Batch 1020 / 3125 - Loss:  1.0057 \n",
            "Batch 1030 / 3125 - Loss:  1.0061 \n",
            "Batch 1040 / 3125 - Loss:  0.9734 \n",
            "Batch 1050 / 3125 - Loss:  1.0210 \n",
            "Batch 1060 / 3125 - Loss:  0.9783 \n",
            "Batch 1070 / 3125 - Loss:  1.0243 \n",
            "Batch 1080 / 3125 - Loss:  1.0716 \n",
            "Batch 1090 / 3125 - Loss:  1.0068 \n",
            "Batch 1100 / 3125 - Loss:  0.8581 \n",
            "Batch 1110 / 3125 - Loss:  0.9319 \n",
            "Batch 1120 / 3125 - Loss:  0.9530 \n",
            "Batch 1130 / 3125 - Loss:  1.0503 \n",
            "Batch 1140 / 3125 - Loss:  0.9455 \n",
            "Batch 1150 / 3125 - Loss:  1.0932 \n",
            "Batch 1160 / 3125 - Loss:  1.0273 \n",
            "Batch 1170 / 3125 - Loss:  1.0160 \n",
            "Batch 1180 / 3125 - Loss:  1.1370 \n",
            "Batch 1190 / 3125 - Loss:  0.9929 \n",
            "Batch 1200 / 3125 - Loss:  1.0218 \n",
            "Batch 1210 / 3125 - Loss:  0.9171 \n",
            "Batch 1220 / 3125 - Loss:  1.1157 \n",
            "Batch 1230 / 3125 - Loss:  1.0554 \n",
            "Batch 1240 / 3125 - Loss:  1.0198 \n",
            "Batch 1250 / 3125 - Loss:  1.1418 \n",
            "Batch 1260 / 3125 - Loss:  1.0470 \n",
            "Batch 1270 / 3125 - Loss:  0.9832 \n",
            "Batch 1280 / 3125 - Loss:  1.0518 \n",
            "Batch 1290 / 3125 - Loss:  1.1165 \n",
            "Batch 1300 / 3125 - Loss:  0.9475 \n",
            "Batch 1310 / 3125 - Loss:  1.0147 \n",
            "Batch 1320 / 3125 - Loss:  1.1288 \n",
            "Batch 1330 / 3125 - Loss:  1.0199 \n",
            "Batch 1340 / 3125 - Loss:  1.1076 \n",
            "Batch 1350 / 3125 - Loss:  0.9850 \n",
            "Batch 1360 / 3125 - Loss:  0.9571 \n",
            "Batch 1370 / 3125 - Loss:  1.0338 \n",
            "Batch 1380 / 3125 - Loss:  0.9994 \n",
            "Batch 1390 / 3125 - Loss:  1.0745 \n",
            "Batch 1400 / 3125 - Loss:  1.0656 \n",
            "Batch 1410 / 3125 - Loss:  0.9753 \n",
            "Batch 1420 / 3125 - Loss:  0.9843 \n",
            "Batch 1430 / 3125 - Loss:  1.0192 \n",
            "Batch 1440 / 3125 - Loss:  1.0443 \n",
            "Batch 1450 / 3125 - Loss:  1.0439 \n",
            "Batch 1460 / 3125 - Loss:  1.0609 \n",
            "Batch 1470 / 3125 - Loss:  0.9964 \n",
            "Batch 1480 / 3125 - Loss:  1.1709 \n",
            "Batch 1490 / 3125 - Loss:  1.1252 \n",
            "Batch 1500 / 3125 - Loss:  0.9868 \n",
            "Batch 1510 / 3125 - Loss:  1.0212 \n",
            "Batch 1520 / 3125 - Loss:  1.1181 \n",
            "Batch 1530 / 3125 - Loss:  1.1024 \n",
            "Batch 1540 / 3125 - Loss:  0.9177 \n",
            "Batch 1550 / 3125 - Loss:  0.9920 \n",
            "Batch 1560 / 3125 - Loss:  1.0388 \n",
            "Batch 1570 / 3125 - Loss:  0.9641 \n",
            "Batch 1580 / 3125 - Loss:  1.0317 \n",
            "Batch 1590 / 3125 - Loss:  0.9915 \n",
            "Batch 1600 / 3125 - Loss:  0.9743 \n",
            "Batch 1610 / 3125 - Loss:  1.0280 \n",
            "Batch 1620 / 3125 - Loss:  0.9925 \n",
            "Batch 1630 / 3125 - Loss:  1.0583 \n",
            "Batch 1640 / 3125 - Loss:  1.0106 \n",
            "Batch 1650 / 3125 - Loss:  1.0712 \n",
            "Batch 1660 / 3125 - Loss:  1.0452 \n",
            "Batch 1670 / 3125 - Loss:  1.0654 \n",
            "Batch 1680 / 3125 - Loss:  0.9892 \n",
            "Batch 1690 / 3125 - Loss:  1.0175 \n",
            "Batch 1700 / 3125 - Loss:  1.0154 \n",
            "Batch 1710 / 3125 - Loss:  0.9797 \n",
            "Batch 1720 / 3125 - Loss:  0.9944 \n",
            "Batch 1730 / 3125 - Loss:  1.1180 \n",
            "Batch 1740 / 3125 - Loss:  0.9223 \n",
            "Batch 1750 / 3125 - Loss:  1.0396 \n",
            "Batch 1760 / 3125 - Loss:  1.0928 \n",
            "Batch 1770 / 3125 - Loss:  1.0539 \n",
            "Batch 1780 / 3125 - Loss:  0.9351 \n",
            "Batch 1790 / 3125 - Loss:  1.0248 \n",
            "Batch 1800 / 3125 - Loss:  1.1493 \n",
            "Batch 1810 / 3125 - Loss:  1.0063 \n",
            "Batch 1820 / 3125 - Loss:  1.0224 \n",
            "Batch 1830 / 3125 - Loss:  1.0211 \n",
            "Batch 1840 / 3125 - Loss:  0.9651 \n",
            "Batch 1850 / 3125 - Loss:  0.9802 \n",
            "Batch 1860 / 3125 - Loss:  1.0495 \n",
            "Batch 1870 / 3125 - Loss:  0.9789 \n",
            "Batch 1880 / 3125 - Loss:  1.0420 \n",
            "Batch 1890 / 3125 - Loss:  0.9602 \n",
            "Batch 1900 / 3125 - Loss:  0.9966 \n",
            "Batch 1910 / 3125 - Loss:  0.9431 \n",
            "Batch 1920 / 3125 - Loss:  0.9948 \n",
            "Batch 1930 / 3125 - Loss:  0.9428 \n",
            "Batch 1940 / 3125 - Loss:  1.0651 \n",
            "Batch 1950 / 3125 - Loss:  1.0137 \n",
            "Batch 1960 / 3125 - Loss:  1.0148 \n",
            "Batch 1970 / 3125 - Loss:  1.0671 \n",
            "Batch 1980 / 3125 - Loss:  1.0512 \n",
            "Batch 1990 / 3125 - Loss:  1.0407 \n",
            "Batch 2000 / 3125 - Loss:  0.9576 \n",
            "Batch 2010 / 3125 - Loss:  1.0516 \n",
            "Batch 2020 / 3125 - Loss:  0.9961 \n",
            "Batch 2030 / 3125 - Loss:  1.0233 \n",
            "Batch 2040 / 3125 - Loss:  0.9419 \n",
            "Batch 2050 / 3125 - Loss:  0.9938 \n",
            "Batch 2060 / 3125 - Loss:  0.8773 \n",
            "Batch 2070 / 3125 - Loss:  1.0516 \n",
            "Batch 2080 / 3125 - Loss:  1.0173 \n",
            "Batch 2090 / 3125 - Loss:  1.0414 \n",
            "Batch 2100 / 3125 - Loss:  0.9520 \n",
            "Batch 2110 / 3125 - Loss:  1.1741 \n",
            "Batch 2120 / 3125 - Loss:  0.9970 \n",
            "Batch 2130 / 3125 - Loss:  1.1200 \n",
            "Batch 2140 / 3125 - Loss:  1.0160 \n",
            "Batch 2150 / 3125 - Loss:  1.0789 \n",
            "Batch 2160 / 3125 - Loss:  0.9543 \n",
            "Batch 2170 / 3125 - Loss:  0.9045 \n",
            "Batch 2180 / 3125 - Loss:  0.9658 \n",
            "Batch 2190 / 3125 - Loss:  1.0175 \n",
            "Batch 2200 / 3125 - Loss:  1.0176 \n",
            "Batch 2210 / 3125 - Loss:  0.9911 \n",
            "Batch 2220 / 3125 - Loss:  1.0108 \n",
            "Batch 2230 / 3125 - Loss:  0.9007 \n",
            "Batch 2240 / 3125 - Loss:  0.9889 \n",
            "Batch 2250 / 3125 - Loss:  1.0382 \n",
            "Batch 2260 / 3125 - Loss:  1.1366 \n",
            "Batch 2270 / 3125 - Loss:  0.9919 \n",
            "Batch 2280 / 3125 - Loss:  1.0901 \n",
            "Batch 2290 / 3125 - Loss:  1.1323 \n",
            "Batch 2300 / 3125 - Loss:  0.9507 \n",
            "Batch 2310 / 3125 - Loss:  1.0144 \n",
            "Batch 2320 / 3125 - Loss:  0.9860 \n",
            "Batch 2330 / 3125 - Loss:  0.9476 \n",
            "Batch 2340 / 3125 - Loss:  1.0447 \n",
            "Batch 2350 / 3125 - Loss:  1.0583 \n",
            "Batch 2360 / 3125 - Loss:  0.9363 \n",
            "Batch 2370 / 3125 - Loss:  0.9983 \n",
            "Batch 2380 / 3125 - Loss:  1.1325 \n",
            "Batch 2390 / 3125 - Loss:  1.0852 \n",
            "Batch 2400 / 3125 - Loss:  0.9582 \n",
            "Batch 2410 / 3125 - Loss:  0.9670 \n",
            "Batch 2420 / 3125 - Loss:  1.0134 \n",
            "Batch 2430 / 3125 - Loss:  1.0010 \n",
            "Batch 2440 / 3125 - Loss:  0.9765 \n",
            "Batch 2450 / 3125 - Loss:  0.8979 \n",
            "Batch 2460 / 3125 - Loss:  1.0152 \n",
            "Batch 2470 / 3125 - Loss:  1.0061 \n",
            "Batch 2480 / 3125 - Loss:  1.0036 \n",
            "Batch 2490 / 3125 - Loss:  1.0516 \n",
            "Batch 2500 / 3125 - Loss:  1.0477 \n",
            "Batch 2510 / 3125 - Loss:  1.0915 \n",
            "Batch 2520 / 3125 - Loss:  1.0587 \n",
            "Batch 2530 / 3125 - Loss:  1.0185 \n",
            "Batch 2540 / 3125 - Loss:  1.0943 \n",
            "Batch 2550 / 3125 - Loss:  1.0659 \n",
            "Batch 2560 / 3125 - Loss:  1.0286 \n",
            "Batch 2570 / 3125 - Loss:  1.0542 \n",
            "Batch 2580 / 3125 - Loss:  1.0668 \n",
            "Batch 2590 / 3125 - Loss:  0.9933 \n",
            "Batch 2600 / 3125 - Loss:  0.9764 \n",
            "Batch 2610 / 3125 - Loss:  1.0597 \n",
            "Batch 2620 / 3125 - Loss:  1.0089 \n",
            "Batch 2630 / 3125 - Loss:  1.0195 \n",
            "Batch 2640 / 3125 - Loss:  0.9908 \n",
            "Batch 2650 / 3125 - Loss:  1.0818 \n",
            "Batch 2660 / 3125 - Loss:  0.9507 \n",
            "Batch 2670 / 3125 - Loss:  0.9014 \n",
            "Batch 2680 / 3125 - Loss:  1.1504 \n",
            "Batch 2690 / 3125 - Loss:  1.0471 \n",
            "Batch 2700 / 3125 - Loss:  0.9768 \n",
            "Batch 2710 / 3125 - Loss:  1.0354 \n",
            "Batch 2720 / 3125 - Loss:  0.9671 \n",
            "Batch 2730 / 3125 - Loss:  1.0074 \n",
            "Batch 2740 / 3125 - Loss:  1.0490 \n",
            "Batch 2750 / 3125 - Loss:  0.9911 \n",
            "Batch 2760 / 3125 - Loss:  0.9246 \n",
            "Batch 2770 / 3125 - Loss:  1.0836 \n",
            "Batch 2780 / 3125 - Loss:  1.0453 \n",
            "Batch 2790 / 3125 - Loss:  0.9768 \n",
            "Batch 2800 / 3125 - Loss:  0.9765 \n",
            "Batch 2810 / 3125 - Loss:  0.9833 \n",
            "Batch 2820 / 3125 - Loss:  1.1122 \n",
            "Batch 2830 / 3125 - Loss:  0.9800 \n",
            "Batch 2840 / 3125 - Loss:  1.0593 \n",
            "Batch 2850 / 3125 - Loss:  1.0135 \n",
            "Batch 2860 / 3125 - Loss:  1.0948 \n",
            "Batch 2870 / 3125 - Loss:  0.9453 \n",
            "Batch 2880 / 3125 - Loss:  0.9418 \n",
            "Batch 2890 / 3125 - Loss:  1.0551 \n",
            "Batch 2900 / 3125 - Loss:  1.1396 \n",
            "Batch 2910 / 3125 - Loss:  1.1414 \n",
            "Batch 2920 / 3125 - Loss:  1.0988 \n",
            "Batch 2930 / 3125 - Loss:  1.0206 \n",
            "Batch 2940 / 3125 - Loss:  1.1262 \n",
            "Batch 2950 / 3125 - Loss:  0.9774 \n",
            "Batch 2960 / 3125 - Loss:  1.0141 \n",
            "Batch 2970 / 3125 - Loss:  1.0026 \n",
            "Batch 2980 / 3125 - Loss:  0.9992 \n",
            "Batch 2990 / 3125 - Loss:  0.9351 \n",
            "Batch 3000 / 3125 - Loss:  1.0134 \n",
            "Batch 3010 / 3125 - Loss:  1.0838 \n",
            "Batch 3020 / 3125 - Loss:  1.0402 \n",
            "Batch 3030 / 3125 - Loss:  1.0023 \n",
            "Batch 3040 / 3125 - Loss:  0.9428 \n",
            "Batch 3050 / 3125 - Loss:  1.0291 \n",
            "Batch 3060 / 3125 - Loss:  1.0679 \n",
            "Batch 3070 / 3125 - Loss:  1.0135 \n",
            "Batch 3080 / 3125 - Loss:  1.0123 \n",
            "Batch 3090 / 3125 - Loss:  0.9509 \n",
            "Batch 3100 / 3125 - Loss:  0.9947 \n",
            "Batch 3110 / 3125 - Loss:  1.0853 \n",
            "Batch 3120 / 3125 - Loss:  0.9331 \n",
            "Epoch 40 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0545 \n",
            "Batch 10 / 3125 - Loss:  1.0294 \n",
            "Batch 20 / 3125 - Loss:  0.9383 \n",
            "Batch 30 / 3125 - Loss:  0.9421 \n",
            "Batch 40 / 3125 - Loss:  0.9977 \n",
            "Batch 50 / 3125 - Loss:  0.9409 \n",
            "Batch 60 / 3125 - Loss:  1.0590 \n",
            "Batch 70 / 3125 - Loss:  0.9487 \n",
            "Batch 80 / 3125 - Loss:  1.0694 \n",
            "Batch 90 / 3125 - Loss:  1.0400 \n",
            "Batch 100 / 3125 - Loss:  1.0313 \n",
            "Batch 110 / 3125 - Loss:  1.0171 \n",
            "Batch 120 / 3125 - Loss:  1.0259 \n",
            "Batch 130 / 3125 - Loss:  1.1030 \n",
            "Batch 140 / 3125 - Loss:  1.0192 \n",
            "Batch 150 / 3125 - Loss:  0.9982 \n",
            "Batch 160 / 3125 - Loss:  1.0204 \n",
            "Batch 170 / 3125 - Loss:  1.0128 \n",
            "Batch 180 / 3125 - Loss:  0.9641 \n",
            "Batch 190 / 3125 - Loss:  0.9741 \n",
            "Batch 200 / 3125 - Loss:  0.9909 \n",
            "Batch 210 / 3125 - Loss:  0.9766 \n",
            "Batch 220 / 3125 - Loss:  1.1165 \n",
            "Batch 230 / 3125 - Loss:  1.1415 \n",
            "Batch 240 / 3125 - Loss:  0.8601 \n",
            "Batch 250 / 3125 - Loss:  1.0436 \n",
            "Batch 260 / 3125 - Loss:  1.0001 \n",
            "Batch 270 / 3125 - Loss:  1.0379 \n",
            "Batch 280 / 3125 - Loss:  0.9442 \n",
            "Batch 290 / 3125 - Loss:  1.0482 \n",
            "Batch 300 / 3125 - Loss:  1.0157 \n",
            "Batch 310 / 3125 - Loss:  1.0827 \n",
            "Batch 320 / 3125 - Loss:  0.9985 \n",
            "Batch 330 / 3125 - Loss:  1.0393 \n",
            "Batch 340 / 3125 - Loss:  0.9953 \n",
            "Batch 350 / 3125 - Loss:  0.9643 \n",
            "Batch 360 / 3125 - Loss:  1.0304 \n",
            "Batch 370 / 3125 - Loss:  1.0449 \n",
            "Batch 380 / 3125 - Loss:  1.0674 \n",
            "Batch 390 / 3125 - Loss:  0.9910 \n",
            "Batch 400 / 3125 - Loss:  0.9223 \n",
            "Batch 410 / 3125 - Loss:  1.0567 \n",
            "Batch 420 / 3125 - Loss:  1.0803 \n",
            "Batch 430 / 3125 - Loss:  0.9766 \n",
            "Batch 440 / 3125 - Loss:  1.0050 \n",
            "Batch 450 / 3125 - Loss:  0.9478 \n",
            "Batch 460 / 3125 - Loss:  0.9313 \n",
            "Batch 470 / 3125 - Loss:  1.1688 \n",
            "Batch 480 / 3125 - Loss:  1.0300 \n",
            "Batch 490 / 3125 - Loss:  1.0719 \n",
            "Batch 500 / 3125 - Loss:  1.0392 \n",
            "Batch 510 / 3125 - Loss:  0.9971 \n",
            "Batch 520 / 3125 - Loss:  0.9551 \n",
            "Batch 530 / 3125 - Loss:  1.0893 \n",
            "Batch 540 / 3125 - Loss:  1.0197 \n",
            "Batch 550 / 3125 - Loss:  0.9857 \n",
            "Batch 560 / 3125 - Loss:  1.1153 \n",
            "Batch 570 / 3125 - Loss:  1.0472 \n",
            "Batch 580 / 3125 - Loss:  0.9695 \n",
            "Batch 590 / 3125 - Loss:  1.0426 \n",
            "Batch 600 / 3125 - Loss:  1.1130 \n",
            "Batch 610 / 3125 - Loss:  1.0236 \n",
            "Batch 620 / 3125 - Loss:  0.9581 \n",
            "Batch 630 / 3125 - Loss:  1.1350 \n",
            "Batch 640 / 3125 - Loss:  1.1343 \n",
            "Batch 650 / 3125 - Loss:  0.9940 \n",
            "Batch 660 / 3125 - Loss:  1.0718 \n",
            "Batch 670 / 3125 - Loss:  1.1706 \n",
            "Batch 680 / 3125 - Loss:  1.0562 \n",
            "Batch 690 / 3125 - Loss:  0.9949 \n",
            "Batch 700 / 3125 - Loss:  1.0090 \n",
            "Batch 710 / 3125 - Loss:  1.0937 \n",
            "Batch 720 / 3125 - Loss:  0.9950 \n",
            "Batch 730 / 3125 - Loss:  1.0269 \n",
            "Batch 740 / 3125 - Loss:  0.9677 \n",
            "Batch 750 / 3125 - Loss:  1.0923 \n",
            "Batch 760 / 3125 - Loss:  1.0374 \n",
            "Batch 770 / 3125 - Loss:  1.0050 \n",
            "Batch 780 / 3125 - Loss:  1.0424 \n",
            "Batch 790 / 3125 - Loss:  1.0149 \n",
            "Batch 800 / 3125 - Loss:  1.0522 \n",
            "Batch 810 / 3125 - Loss:  1.0748 \n",
            "Batch 820 / 3125 - Loss:  1.1189 \n",
            "Batch 830 / 3125 - Loss:  0.9247 \n",
            "Batch 840 / 3125 - Loss:  1.1140 \n",
            "Batch 850 / 3125 - Loss:  1.0515 \n",
            "Batch 860 / 3125 - Loss:  0.9837 \n",
            "Batch 870 / 3125 - Loss:  1.0473 \n",
            "Batch 880 / 3125 - Loss:  1.0640 \n",
            "Batch 890 / 3125 - Loss:  0.9558 \n",
            "Batch 900 / 3125 - Loss:  1.0197 \n",
            "Batch 910 / 3125 - Loss:  0.9723 \n",
            "Batch 920 / 3125 - Loss:  0.9705 \n",
            "Batch 930 / 3125 - Loss:  0.9702 \n",
            "Batch 940 / 3125 - Loss:  0.9711 \n",
            "Batch 950 / 3125 - Loss:  0.9540 \n",
            "Batch 960 / 3125 - Loss:  1.0319 \n",
            "Batch 970 / 3125 - Loss:  1.0972 \n",
            "Batch 980 / 3125 - Loss:  1.0707 \n",
            "Batch 990 / 3125 - Loss:  1.0836 \n",
            "Batch 1000 / 3125 - Loss:  0.9444 \n",
            "Batch 1010 / 3125 - Loss:  1.0848 \n",
            "Batch 1020 / 3125 - Loss:  0.9595 \n",
            "Batch 1030 / 3125 - Loss:  1.0592 \n",
            "Batch 1040 / 3125 - Loss:  0.9721 \n",
            "Batch 1050 / 3125 - Loss:  0.9539 \n",
            "Batch 1060 / 3125 - Loss:  1.1725 \n",
            "Batch 1070 / 3125 - Loss:  0.9568 \n",
            "Batch 1080 / 3125 - Loss:  1.0018 \n",
            "Batch 1090 / 3125 - Loss:  1.0378 \n",
            "Batch 1100 / 3125 - Loss:  1.0919 \n",
            "Batch 1110 / 3125 - Loss:  0.9622 \n",
            "Batch 1120 / 3125 - Loss:  1.0155 \n",
            "Batch 1130 / 3125 - Loss:  1.0156 \n",
            "Batch 1140 / 3125 - Loss:  0.9648 \n",
            "Batch 1150 / 3125 - Loss:  1.0368 \n",
            "Batch 1160 / 3125 - Loss:  1.0235 \n",
            "Batch 1170 / 3125 - Loss:  0.9408 \n",
            "Batch 1180 / 3125 - Loss:  1.0110 \n",
            "Batch 1190 / 3125 - Loss:  1.0861 \n",
            "Batch 1200 / 3125 - Loss:  1.0796 \n",
            "Batch 1210 / 3125 - Loss:  1.0004 \n",
            "Batch 1220 / 3125 - Loss:  1.0311 \n",
            "Batch 1230 / 3125 - Loss:  0.9912 \n",
            "Batch 1240 / 3125 - Loss:  1.0227 \n",
            "Batch 1250 / 3125 - Loss:  1.0985 \n",
            "Batch 1260 / 3125 - Loss:  1.0464 \n",
            "Batch 1270 / 3125 - Loss:  1.0679 \n",
            "Batch 1280 / 3125 - Loss:  1.0214 \n",
            "Batch 1290 / 3125 - Loss:  1.0003 \n",
            "Batch 1300 / 3125 - Loss:  1.0056 \n",
            "Batch 1310 / 3125 - Loss:  0.9664 \n",
            "Batch 1320 / 3125 - Loss:  0.8927 \n",
            "Batch 1330 / 3125 - Loss:  1.1119 \n",
            "Batch 1340 / 3125 - Loss:  0.9478 \n",
            "Batch 1350 / 3125 - Loss:  1.0536 \n",
            "Batch 1360 / 3125 - Loss:  0.9733 \n",
            "Batch 1370 / 3125 - Loss:  0.9549 \n",
            "Batch 1380 / 3125 - Loss:  0.9794 \n",
            "Batch 1390 / 3125 - Loss:  1.0285 \n",
            "Batch 1400 / 3125 - Loss:  0.9264 \n",
            "Batch 1410 / 3125 - Loss:  1.0112 \n",
            "Batch 1420 / 3125 - Loss:  1.0887 \n",
            "Batch 1430 / 3125 - Loss:  1.0874 \n",
            "Batch 1440 / 3125 - Loss:  1.0726 \n",
            "Batch 1450 / 3125 - Loss:  0.9984 \n",
            "Batch 1460 / 3125 - Loss:  1.0723 \n",
            "Batch 1470 / 3125 - Loss:  1.0325 \n",
            "Batch 1480 / 3125 - Loss:  1.1032 \n",
            "Batch 1490 / 3125 - Loss:  0.9675 \n",
            "Batch 1500 / 3125 - Loss:  1.0385 \n",
            "Batch 1510 / 3125 - Loss:  1.0579 \n",
            "Batch 1520 / 3125 - Loss:  1.0626 \n",
            "Batch 1530 / 3125 - Loss:  1.0358 \n",
            "Batch 1540 / 3125 - Loss:  0.9258 \n",
            "Batch 1550 / 3125 - Loss:  1.0287 \n",
            "Batch 1560 / 3125 - Loss:  1.0957 \n",
            "Batch 1570 / 3125 - Loss:  1.0516 \n",
            "Batch 1580 / 3125 - Loss:  0.9932 \n",
            "Batch 1590 / 3125 - Loss:  0.8901 \n",
            "Batch 1600 / 3125 - Loss:  0.9691 \n",
            "Batch 1610 / 3125 - Loss:  0.8454 \n",
            "Batch 1620 / 3125 - Loss:  1.0015 \n",
            "Batch 1630 / 3125 - Loss:  1.1769 \n",
            "Batch 1640 / 3125 - Loss:  1.0792 \n",
            "Batch 1650 / 3125 - Loss:  1.0229 \n",
            "Batch 1660 / 3125 - Loss:  1.0609 \n",
            "Batch 1670 / 3125 - Loss:  1.0740 \n",
            "Batch 1680 / 3125 - Loss:  1.1595 \n",
            "Batch 1690 / 3125 - Loss:  1.0626 \n",
            "Batch 1700 / 3125 - Loss:  0.9916 \n",
            "Batch 1710 / 3125 - Loss:  0.9229 \n",
            "Batch 1720 / 3125 - Loss:  1.0155 \n",
            "Batch 1730 / 3125 - Loss:  1.0583 \n",
            "Batch 1740 / 3125 - Loss:  1.0863 \n",
            "Batch 1750 / 3125 - Loss:  1.0337 \n",
            "Batch 1760 / 3125 - Loss:  0.9133 \n",
            "Batch 1770 / 3125 - Loss:  1.0489 \n",
            "Batch 1780 / 3125 - Loss:  0.8897 \n",
            "Batch 1790 / 3125 - Loss:  1.1305 \n",
            "Batch 1800 / 3125 - Loss:  1.0459 \n",
            "Batch 1810 / 3125 - Loss:  1.0773 \n",
            "Batch 1820 / 3125 - Loss:  0.9673 \n",
            "Batch 1830 / 3125 - Loss:  1.0989 \n",
            "Batch 1840 / 3125 - Loss:  0.9766 \n",
            "Batch 1850 / 3125 - Loss:  1.0348 \n",
            "Batch 1860 / 3125 - Loss:  0.9726 \n",
            "Batch 1870 / 3125 - Loss:  1.0493 \n",
            "Batch 1880 / 3125 - Loss:  0.9956 \n",
            "Batch 1890 / 3125 - Loss:  1.0079 \n",
            "Batch 1900 / 3125 - Loss:  1.1210 \n",
            "Batch 1910 / 3125 - Loss:  1.0452 \n",
            "Batch 1920 / 3125 - Loss:  0.8949 \n",
            "Batch 1930 / 3125 - Loss:  1.0861 \n",
            "Batch 1940 / 3125 - Loss:  1.0415 \n",
            "Batch 1950 / 3125 - Loss:  1.1816 \n",
            "Batch 1960 / 3125 - Loss:  1.0008 \n",
            "Batch 1970 / 3125 - Loss:  1.0497 \n",
            "Batch 1980 / 3125 - Loss:  0.9357 \n",
            "Batch 1990 / 3125 - Loss:  0.9588 \n",
            "Batch 2000 / 3125 - Loss:  1.0342 \n",
            "Batch 2010 / 3125 - Loss:  0.9990 \n",
            "Batch 2020 / 3125 - Loss:  1.0085 \n",
            "Batch 2030 / 3125 - Loss:  1.0105 \n",
            "Batch 2040 / 3125 - Loss:  0.8685 \n",
            "Batch 2050 / 3125 - Loss:  0.8870 \n",
            "Batch 2060 / 3125 - Loss:  1.0609 \n",
            "Batch 2070 / 3125 - Loss:  1.0806 \n",
            "Batch 2080 / 3125 - Loss:  1.0191 \n",
            "Batch 2090 / 3125 - Loss:  1.1056 \n",
            "Batch 2100 / 3125 - Loss:  0.9970 \n",
            "Batch 2110 / 3125 - Loss:  1.1232 \n",
            "Batch 2120 / 3125 - Loss:  0.9359 \n",
            "Batch 2130 / 3125 - Loss:  0.9058 \n",
            "Batch 2140 / 3125 - Loss:  1.0243 \n",
            "Batch 2150 / 3125 - Loss:  0.9298 \n",
            "Batch 2160 / 3125 - Loss:  1.0646 \n",
            "Batch 2170 / 3125 - Loss:  1.0548 \n",
            "Batch 2180 / 3125 - Loss:  1.0476 \n",
            "Batch 2190 / 3125 - Loss:  0.9637 \n",
            "Batch 2200 / 3125 - Loss:  1.1795 \n",
            "Batch 2210 / 3125 - Loss:  1.1049 \n",
            "Batch 2220 / 3125 - Loss:  1.0379 \n",
            "Batch 2230 / 3125 - Loss:  0.9615 \n",
            "Batch 2240 / 3125 - Loss:  1.0647 \n",
            "Batch 2250 / 3125 - Loss:  1.0507 \n",
            "Batch 2260 / 3125 - Loss:  1.0789 \n",
            "Batch 2270 / 3125 - Loss:  1.0624 \n",
            "Batch 2280 / 3125 - Loss:  1.1288 \n",
            "Batch 2290 / 3125 - Loss:  0.9400 \n",
            "Batch 2300 / 3125 - Loss:  1.0965 \n",
            "Batch 2310 / 3125 - Loss:  1.1137 \n",
            "Batch 2320 / 3125 - Loss:  1.0700 \n",
            "Batch 2330 / 3125 - Loss:  0.9403 \n",
            "Batch 2340 / 3125 - Loss:  0.9358 \n",
            "Batch 2350 / 3125 - Loss:  1.0187 \n",
            "Batch 2360 / 3125 - Loss:  1.0281 \n",
            "Batch 2370 / 3125 - Loss:  1.1589 \n",
            "Batch 2380 / 3125 - Loss:  1.0568 \n",
            "Batch 2390 / 3125 - Loss:  0.9623 \n",
            "Batch 2400 / 3125 - Loss:  1.0369 \n",
            "Batch 2410 / 3125 - Loss:  0.9675 \n",
            "Batch 2420 / 3125 - Loss:  1.0700 \n",
            "Batch 2430 / 3125 - Loss:  0.9923 \n",
            "Batch 2440 / 3125 - Loss:  0.9261 \n",
            "Batch 2450 / 3125 - Loss:  1.0514 \n",
            "Batch 2460 / 3125 - Loss:  1.0297 \n",
            "Batch 2470 / 3125 - Loss:  0.9679 \n",
            "Batch 2480 / 3125 - Loss:  1.1194 \n",
            "Batch 2490 / 3125 - Loss:  1.0730 \n",
            "Batch 2500 / 3125 - Loss:  1.0893 \n",
            "Batch 2510 / 3125 - Loss:  0.9642 \n",
            "Batch 2520 / 3125 - Loss:  0.9290 \n",
            "Batch 2530 / 3125 - Loss:  1.0965 \n",
            "Batch 2540 / 3125 - Loss:  1.0367 \n",
            "Batch 2550 / 3125 - Loss:  1.0504 \n",
            "Batch 2560 / 3125 - Loss:  1.0979 \n",
            "Batch 2570 / 3125 - Loss:  1.0573 \n",
            "Batch 2580 / 3125 - Loss:  1.0266 \n",
            "Batch 2590 / 3125 - Loss:  1.0308 \n",
            "Batch 2600 / 3125 - Loss:  1.0418 \n",
            "Batch 2610 / 3125 - Loss:  0.9364 \n",
            "Batch 2620 / 3125 - Loss:  1.0769 \n",
            "Batch 2630 / 3125 - Loss:  1.0942 \n",
            "Batch 2640 / 3125 - Loss:  0.9780 \n",
            "Batch 2650 / 3125 - Loss:  0.9920 \n",
            "Batch 2660 / 3125 - Loss:  0.9267 \n",
            "Batch 2670 / 3125 - Loss:  1.0441 \n",
            "Batch 2680 / 3125 - Loss:  0.9703 \n",
            "Batch 2690 / 3125 - Loss:  1.0137 \n",
            "Batch 2700 / 3125 - Loss:  1.0795 \n",
            "Batch 2710 / 3125 - Loss:  1.1101 \n",
            "Batch 2720 / 3125 - Loss:  1.0507 \n",
            "Batch 2730 / 3125 - Loss:  0.9945 \n",
            "Batch 2740 / 3125 - Loss:  1.0291 \n",
            "Batch 2750 / 3125 - Loss:  1.0985 \n",
            "Batch 2760 / 3125 - Loss:  0.9440 \n",
            "Batch 2770 / 3125 - Loss:  1.0862 \n",
            "Batch 2780 / 3125 - Loss:  0.9839 \n",
            "Batch 2790 / 3125 - Loss:  1.0796 \n",
            "Batch 2800 / 3125 - Loss:  1.0009 \n",
            "Batch 2810 / 3125 - Loss:  1.0255 \n",
            "Batch 2820 / 3125 - Loss:  0.9730 \n",
            "Batch 2830 / 3125 - Loss:  1.0854 \n",
            "Batch 2840 / 3125 - Loss:  1.0267 \n",
            "Batch 2850 / 3125 - Loss:  0.9153 \n",
            "Batch 2860 / 3125 - Loss:  1.0132 \n",
            "Batch 2870 / 3125 - Loss:  1.1219 \n",
            "Batch 2880 / 3125 - Loss:  1.0277 \n",
            "Batch 2890 / 3125 - Loss:  0.9900 \n",
            "Batch 2900 / 3125 - Loss:  1.0756 \n",
            "Batch 2910 / 3125 - Loss:  1.1125 \n",
            "Batch 2920 / 3125 - Loss:  1.0994 \n",
            "Batch 2930 / 3125 - Loss:  1.0980 \n",
            "Batch 2940 / 3125 - Loss:  0.9479 \n",
            "Batch 2950 / 3125 - Loss:  1.1058 \n",
            "Batch 2960 / 3125 - Loss:  0.9779 \n",
            "Batch 2970 / 3125 - Loss:  1.0191 \n",
            "Batch 2980 / 3125 - Loss:  1.0601 \n",
            "Batch 2990 / 3125 - Loss:  1.0474 \n",
            "Batch 3000 / 3125 - Loss:  1.0544 \n",
            "Batch 3010 / 3125 - Loss:  1.0079 \n",
            "Batch 3020 / 3125 - Loss:  1.0700 \n",
            "Batch 3030 / 3125 - Loss:  1.0137 \n",
            "Batch 3040 / 3125 - Loss:  1.0770 \n",
            "Batch 3050 / 3125 - Loss:  0.9949 \n",
            "Batch 3060 / 3125 - Loss:  1.0017 \n",
            "Batch 3070 / 3125 - Loss:  1.0766 \n",
            "Batch 3080 / 3125 - Loss:  1.0948 \n",
            "Batch 3090 / 3125 - Loss:  1.1762 \n",
            "Batch 3100 / 3125 - Loss:  0.9229 \n",
            "Batch 3110 / 3125 - Loss:  1.0615 \n",
            "Batch 3120 / 3125 - Loss:  1.0307 \n",
            "Epoch 41 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0375 \n",
            "Batch 10 / 3125 - Loss:  1.1418 \n",
            "Batch 20 / 3125 - Loss:  0.9493 \n",
            "Batch 30 / 3125 - Loss:  0.9694 \n",
            "Batch 40 / 3125 - Loss:  1.0593 \n",
            "Batch 50 / 3125 - Loss:  0.9739 \n",
            "Batch 60 / 3125 - Loss:  1.0488 \n",
            "Batch 70 / 3125 - Loss:  1.0403 \n",
            "Batch 80 / 3125 - Loss:  0.9678 \n",
            "Batch 90 / 3125 - Loss:  1.0371 \n",
            "Batch 100 / 3125 - Loss:  0.9493 \n",
            "Batch 110 / 3125 - Loss:  1.0405 \n",
            "Batch 120 / 3125 - Loss:  1.0688 \n",
            "Batch 130 / 3125 - Loss:  1.0154 \n",
            "Batch 140 / 3125 - Loss:  1.0292 \n",
            "Batch 150 / 3125 - Loss:  1.1052 \n",
            "Batch 160 / 3125 - Loss:  1.0297 \n",
            "Batch 170 / 3125 - Loss:  1.0260 \n",
            "Batch 180 / 3125 - Loss:  1.0831 \n",
            "Batch 190 / 3125 - Loss:  1.0030 \n",
            "Batch 200 / 3125 - Loss:  1.1004 \n",
            "Batch 210 / 3125 - Loss:  1.0629 \n",
            "Batch 220 / 3125 - Loss:  1.0506 \n",
            "Batch 230 / 3125 - Loss:  1.0558 \n",
            "Batch 240 / 3125 - Loss:  1.0287 \n",
            "Batch 250 / 3125 - Loss:  1.0177 \n",
            "Batch 260 / 3125 - Loss:  1.0258 \n",
            "Batch 270 / 3125 - Loss:  1.0321 \n",
            "Batch 280 / 3125 - Loss:  0.9468 \n",
            "Batch 290 / 3125 - Loss:  1.0863 \n",
            "Batch 300 / 3125 - Loss:  0.9714 \n",
            "Batch 310 / 3125 - Loss:  0.8862 \n",
            "Batch 320 / 3125 - Loss:  0.9765 \n",
            "Batch 330 / 3125 - Loss:  1.0644 \n",
            "Batch 340 / 3125 - Loss:  0.9694 \n",
            "Batch 350 / 3125 - Loss:  1.0280 \n",
            "Batch 360 / 3125 - Loss:  0.9537 \n",
            "Batch 370 / 3125 - Loss:  1.0720 \n",
            "Batch 380 / 3125 - Loss:  0.9532 \n",
            "Batch 390 / 3125 - Loss:  0.9812 \n",
            "Batch 400 / 3125 - Loss:  1.1451 \n",
            "Batch 410 / 3125 - Loss:  1.0917 \n",
            "Batch 420 / 3125 - Loss:  1.0150 \n",
            "Batch 430 / 3125 - Loss:  1.0176 \n",
            "Batch 440 / 3125 - Loss:  0.9886 \n",
            "Batch 450 / 3125 - Loss:  1.0113 \n",
            "Batch 460 / 3125 - Loss:  0.9933 \n",
            "Batch 470 / 3125 - Loss:  1.0218 \n",
            "Batch 480 / 3125 - Loss:  0.9058 \n",
            "Batch 490 / 3125 - Loss:  0.9635 \n",
            "Batch 500 / 3125 - Loss:  0.9752 \n",
            "Batch 510 / 3125 - Loss:  1.1069 \n",
            "Batch 520 / 3125 - Loss:  1.0782 \n",
            "Batch 530 / 3125 - Loss:  1.0284 \n",
            "Batch 540 / 3125 - Loss:  1.0184 \n",
            "Batch 550 / 3125 - Loss:  1.0339 \n",
            "Batch 560 / 3125 - Loss:  1.0714 \n",
            "Batch 570 / 3125 - Loss:  1.0129 \n",
            "Batch 580 / 3125 - Loss:  1.0855 \n",
            "Batch 590 / 3125 - Loss:  1.0485 \n",
            "Batch 600 / 3125 - Loss:  0.9404 \n",
            "Batch 610 / 3125 - Loss:  1.0491 \n",
            "Batch 620 / 3125 - Loss:  1.0006 \n",
            "Batch 630 / 3125 - Loss:  1.0618 \n",
            "Batch 640 / 3125 - Loss:  1.0292 \n",
            "Batch 650 / 3125 - Loss:  1.1393 \n",
            "Batch 660 / 3125 - Loss:  1.0323 \n",
            "Batch 670 / 3125 - Loss:  1.0195 \n",
            "Batch 680 / 3125 - Loss:  0.9857 \n",
            "Batch 690 / 3125 - Loss:  1.0971 \n",
            "Batch 700 / 3125 - Loss:  1.1681 \n",
            "Batch 710 / 3125 - Loss:  1.1631 \n",
            "Batch 720 / 3125 - Loss:  1.0290 \n",
            "Batch 730 / 3125 - Loss:  0.9737 \n",
            "Batch 740 / 3125 - Loss:  1.1044 \n",
            "Batch 750 / 3125 - Loss:  1.1651 \n",
            "Batch 760 / 3125 - Loss:  1.0719 \n",
            "Batch 770 / 3125 - Loss:  1.1649 \n",
            "Batch 780 / 3125 - Loss:  0.9750 \n",
            "Batch 790 / 3125 - Loss:  1.0777 \n",
            "Batch 800 / 3125 - Loss:  0.9342 \n",
            "Batch 810 / 3125 - Loss:  0.9723 \n",
            "Batch 820 / 3125 - Loss:  0.9806 \n",
            "Batch 830 / 3125 - Loss:  0.9613 \n",
            "Batch 840 / 3125 - Loss:  0.9798 \n",
            "Batch 850 / 3125 - Loss:  0.9611 \n",
            "Batch 860 / 3125 - Loss:  1.0429 \n",
            "Batch 870 / 3125 - Loss:  1.0186 \n",
            "Batch 880 / 3125 - Loss:  0.9787 \n",
            "Batch 890 / 3125 - Loss:  1.0652 \n",
            "Batch 900 / 3125 - Loss:  0.9988 \n",
            "Batch 910 / 3125 - Loss:  0.9841 \n",
            "Batch 920 / 3125 - Loss:  0.9670 \n",
            "Batch 930 / 3125 - Loss:  1.0293 \n",
            "Batch 940 / 3125 - Loss:  1.0444 \n",
            "Batch 950 / 3125 - Loss:  1.0612 \n",
            "Batch 960 / 3125 - Loss:  0.9816 \n",
            "Batch 970 / 3125 - Loss:  1.1751 \n",
            "Batch 980 / 3125 - Loss:  1.0677 \n",
            "Batch 990 / 3125 - Loss:  0.9622 \n",
            "Batch 1000 / 3125 - Loss:  1.0680 \n",
            "Batch 1010 / 3125 - Loss:  1.1322 \n",
            "Batch 1020 / 3125 - Loss:  0.9413 \n",
            "Batch 1030 / 3125 - Loss:  1.0649 \n",
            "Batch 1040 / 3125 - Loss:  1.0547 \n",
            "Batch 1050 / 3125 - Loss:  1.0293 \n",
            "Batch 1060 / 3125 - Loss:  0.9916 \n",
            "Batch 1070 / 3125 - Loss:  1.1142 \n",
            "Batch 1080 / 3125 - Loss:  0.9714 \n",
            "Batch 1090 / 3125 - Loss:  1.0800 \n",
            "Batch 1100 / 3125 - Loss:  0.9867 \n",
            "Batch 1110 / 3125 - Loss:  0.9531 \n",
            "Batch 1120 / 3125 - Loss:  0.8910 \n",
            "Batch 1130 / 3125 - Loss:  1.0525 \n",
            "Batch 1140 / 3125 - Loss:  1.0791 \n",
            "Batch 1150 / 3125 - Loss:  0.9964 \n",
            "Batch 1160 / 3125 - Loss:  0.9536 \n",
            "Batch 1170 / 3125 - Loss:  1.0812 \n",
            "Batch 1180 / 3125 - Loss:  1.0430 \n",
            "Batch 1190 / 3125 - Loss:  1.1000 \n",
            "Batch 1200 / 3125 - Loss:  1.0410 \n",
            "Batch 1210 / 3125 - Loss:  1.0484 \n",
            "Batch 1220 / 3125 - Loss:  1.0849 \n",
            "Batch 1230 / 3125 - Loss:  1.0449 \n",
            "Batch 1240 / 3125 - Loss:  1.0405 \n",
            "Batch 1250 / 3125 - Loss:  1.0594 \n",
            "Batch 1260 / 3125 - Loss:  1.0371 \n",
            "Batch 1270 / 3125 - Loss:  0.9260 \n",
            "Batch 1280 / 3125 - Loss:  1.0294 \n",
            "Batch 1290 / 3125 - Loss:  1.0054 \n",
            "Batch 1300 / 3125 - Loss:  1.0982 \n",
            "Batch 1310 / 3125 - Loss:  0.9610 \n",
            "Batch 1320 / 3125 - Loss:  0.9390 \n",
            "Batch 1330 / 3125 - Loss:  0.9192 \n",
            "Batch 1340 / 3125 - Loss:  1.0476 \n",
            "Batch 1350 / 3125 - Loss:  0.9498 \n",
            "Batch 1360 / 3125 - Loss:  1.0874 \n",
            "Batch 1370 / 3125 - Loss:  0.9113 \n",
            "Batch 1380 / 3125 - Loss:  1.0081 \n",
            "Batch 1390 / 3125 - Loss:  1.0678 \n",
            "Batch 1400 / 3125 - Loss:  1.0262 \n",
            "Batch 1410 / 3125 - Loss:  0.9756 \n",
            "Batch 1420 / 3125 - Loss:  1.0422 \n",
            "Batch 1430 / 3125 - Loss:  0.9927 \n",
            "Batch 1440 / 3125 - Loss:  1.0878 \n",
            "Batch 1450 / 3125 - Loss:  1.0360 \n",
            "Batch 1460 / 3125 - Loss:  0.9199 \n",
            "Batch 1470 / 3125 - Loss:  1.0410 \n",
            "Batch 1480 / 3125 - Loss:  1.0529 \n",
            "Batch 1490 / 3125 - Loss:  0.9092 \n",
            "Batch 1500 / 3125 - Loss:  0.9699 \n",
            "Batch 1510 / 3125 - Loss:  1.0954 \n",
            "Batch 1520 / 3125 - Loss:  1.0391 \n",
            "Batch 1530 / 3125 - Loss:  1.0228 \n",
            "Batch 1540 / 3125 - Loss:  1.1528 \n",
            "Batch 1550 / 3125 - Loss:  1.0815 \n",
            "Batch 1560 / 3125 - Loss:  1.0657 \n",
            "Batch 1570 / 3125 - Loss:  1.0529 \n",
            "Batch 1580 / 3125 - Loss:  0.9450 \n",
            "Batch 1590 / 3125 - Loss:  1.0792 \n",
            "Batch 1600 / 3125 - Loss:  1.0547 \n",
            "Batch 1610 / 3125 - Loss:  0.9796 \n",
            "Batch 1620 / 3125 - Loss:  0.9694 \n",
            "Batch 1630 / 3125 - Loss:  0.9823 \n",
            "Batch 1640 / 3125 - Loss:  1.0493 \n",
            "Batch 1650 / 3125 - Loss:  0.9774 \n",
            "Batch 1660 / 3125 - Loss:  1.0124 \n",
            "Batch 1670 / 3125 - Loss:  1.1305 \n",
            "Batch 1680 / 3125 - Loss:  0.9964 \n",
            "Batch 1690 / 3125 - Loss:  1.0065 \n",
            "Batch 1700 / 3125 - Loss:  1.0297 \n",
            "Batch 1710 / 3125 - Loss:  0.9508 \n",
            "Batch 1720 / 3125 - Loss:  1.0496 \n",
            "Batch 1730 / 3125 - Loss:  1.0420 \n",
            "Batch 1740 / 3125 - Loss:  1.0188 \n",
            "Batch 1750 / 3125 - Loss:  1.0690 \n",
            "Batch 1760 / 3125 - Loss:  1.0157 \n",
            "Batch 1770 / 3125 - Loss:  0.9647 \n",
            "Batch 1780 / 3125 - Loss:  0.9782 \n",
            "Batch 1790 / 3125 - Loss:  0.9392 \n",
            "Batch 1800 / 3125 - Loss:  1.1602 \n",
            "Batch 1810 / 3125 - Loss:  0.9979 \n",
            "Batch 1820 / 3125 - Loss:  1.1146 \n",
            "Batch 1830 / 3125 - Loss:  1.1380 \n",
            "Batch 1840 / 3125 - Loss:  1.0592 \n",
            "Batch 1850 / 3125 - Loss:  0.9617 \n",
            "Batch 1860 / 3125 - Loss:  1.0345 \n",
            "Batch 1870 / 3125 - Loss:  1.0548 \n",
            "Batch 1880 / 3125 - Loss:  0.9725 \n",
            "Batch 1890 / 3125 - Loss:  1.2196 \n",
            "Batch 1900 / 3125 - Loss:  1.0008 \n",
            "Batch 1910 / 3125 - Loss:  1.1400 \n",
            "Batch 1920 / 3125 - Loss:  0.9889 \n",
            "Batch 1930 / 3125 - Loss:  1.0111 \n",
            "Batch 1940 / 3125 - Loss:  1.0622 \n",
            "Batch 1950 / 3125 - Loss:  1.0310 \n",
            "Batch 1960 / 3125 - Loss:  0.9706 \n",
            "Batch 1970 / 3125 - Loss:  0.9540 \n",
            "Batch 1980 / 3125 - Loss:  1.0437 \n",
            "Batch 1990 / 3125 - Loss:  0.9803 \n",
            "Batch 2000 / 3125 - Loss:  1.0323 \n",
            "Batch 2010 / 3125 - Loss:  1.0609 \n",
            "Batch 2020 / 3125 - Loss:  1.0317 \n",
            "Batch 2030 / 3125 - Loss:  1.0762 \n",
            "Batch 2040 / 3125 - Loss:  1.1048 \n",
            "Batch 2050 / 3125 - Loss:  0.9696 \n",
            "Batch 2060 / 3125 - Loss:  1.0274 \n",
            "Batch 2070 / 3125 - Loss:  0.9673 \n",
            "Batch 2080 / 3125 - Loss:  1.0279 \n",
            "Batch 2090 / 3125 - Loss:  1.0154 \n",
            "Batch 2100 / 3125 - Loss:  1.0596 \n",
            "Batch 2110 / 3125 - Loss:  1.0979 \n",
            "Batch 2120 / 3125 - Loss:  1.1099 \n",
            "Batch 2130 / 3125 - Loss:  1.0877 \n",
            "Batch 2140 / 3125 - Loss:  1.0555 \n",
            "Batch 2150 / 3125 - Loss:  1.0256 \n",
            "Batch 2160 / 3125 - Loss:  0.9943 \n",
            "Batch 2170 / 3125 - Loss:  1.0777 \n",
            "Batch 2180 / 3125 - Loss:  1.0301 \n",
            "Batch 2190 / 3125 - Loss:  1.0405 \n",
            "Batch 2200 / 3125 - Loss:  1.0758 \n",
            "Batch 2210 / 3125 - Loss:  1.0044 \n",
            "Batch 2220 / 3125 - Loss:  1.0975 \n",
            "Batch 2230 / 3125 - Loss:  1.0591 \n",
            "Batch 2240 / 3125 - Loss:  1.0747 \n",
            "Batch 2250 / 3125 - Loss:  1.1045 \n",
            "Batch 2260 / 3125 - Loss:  0.9908 \n",
            "Batch 2270 / 3125 - Loss:  0.9699 \n",
            "Batch 2280 / 3125 - Loss:  1.1162 \n",
            "Batch 2290 / 3125 - Loss:  1.0645 \n",
            "Batch 2300 / 3125 - Loss:  0.9845 \n",
            "Batch 2310 / 3125 - Loss:  0.9431 \n",
            "Batch 2320 / 3125 - Loss:  1.1912 \n",
            "Batch 2330 / 3125 - Loss:  0.9914 \n",
            "Batch 2340 / 3125 - Loss:  1.0873 \n",
            "Batch 2350 / 3125 - Loss:  0.9645 \n",
            "Batch 2360 / 3125 - Loss:  1.0229 \n",
            "Batch 2370 / 3125 - Loss:  1.0373 \n",
            "Batch 2380 / 3125 - Loss:  1.1220 \n",
            "Batch 2390 / 3125 - Loss:  1.0545 \n",
            "Batch 2400 / 3125 - Loss:  1.1138 \n",
            "Batch 2410 / 3125 - Loss:  1.1237 \n",
            "Batch 2420 / 3125 - Loss:  0.9632 \n",
            "Batch 2430 / 3125 - Loss:  0.9511 \n",
            "Batch 2440 / 3125 - Loss:  0.9539 \n",
            "Batch 2450 / 3125 - Loss:  1.0503 \n",
            "Batch 2460 / 3125 - Loss:  1.0782 \n",
            "Batch 2470 / 3125 - Loss:  0.9758 \n",
            "Batch 2480 / 3125 - Loss:  0.9770 \n",
            "Batch 2490 / 3125 - Loss:  0.9854 \n",
            "Batch 2500 / 3125 - Loss:  1.0418 \n",
            "Batch 2510 / 3125 - Loss:  1.0383 \n",
            "Batch 2520 / 3125 - Loss:  1.0703 \n",
            "Batch 2530 / 3125 - Loss:  1.0580 \n",
            "Batch 2540 / 3125 - Loss:  0.9877 \n",
            "Batch 2550 / 3125 - Loss:  0.9406 \n",
            "Batch 2560 / 3125 - Loss:  1.0500 \n",
            "Batch 2570 / 3125 - Loss:  1.0246 \n",
            "Batch 2580 / 3125 - Loss:  1.0920 \n",
            "Batch 2590 / 3125 - Loss:  1.0363 \n",
            "Batch 2600 / 3125 - Loss:  1.0775 \n",
            "Batch 2610 / 3125 - Loss:  0.9801 \n",
            "Batch 2620 / 3125 - Loss:  1.0587 \n",
            "Batch 2630 / 3125 - Loss:  0.9750 \n",
            "Batch 2640 / 3125 - Loss:  1.0180 \n",
            "Batch 2650 / 3125 - Loss:  1.0947 \n",
            "Batch 2660 / 3125 - Loss:  1.1557 \n",
            "Batch 2670 / 3125 - Loss:  0.9960 \n",
            "Batch 2680 / 3125 - Loss:  1.0546 \n",
            "Batch 2690 / 3125 - Loss:  0.9596 \n",
            "Batch 2700 / 3125 - Loss:  1.0318 \n",
            "Batch 2710 / 3125 - Loss:  1.0531 \n",
            "Batch 2720 / 3125 - Loss:  1.0657 \n",
            "Batch 2730 / 3125 - Loss:  1.0408 \n",
            "Batch 2740 / 3125 - Loss:  1.0209 \n",
            "Batch 2750 / 3125 - Loss:  1.0679 \n",
            "Batch 2760 / 3125 - Loss:  1.0127 \n",
            "Batch 2770 / 3125 - Loss:  1.0174 \n",
            "Batch 2780 / 3125 - Loss:  1.0499 \n",
            "Batch 2790 / 3125 - Loss:  1.0534 \n",
            "Batch 2800 / 3125 - Loss:  1.1073 \n",
            "Batch 2810 / 3125 - Loss:  1.0061 \n",
            "Batch 2820 / 3125 - Loss:  1.0605 \n",
            "Batch 2830 / 3125 - Loss:  1.0835 \n",
            "Batch 2840 / 3125 - Loss:  0.9329 \n",
            "Batch 2850 / 3125 - Loss:  1.0094 \n",
            "Batch 2860 / 3125 - Loss:  0.9960 \n",
            "Batch 2870 / 3125 - Loss:  1.0956 \n",
            "Batch 2880 / 3125 - Loss:  1.0456 \n",
            "Batch 2890 / 3125 - Loss:  1.1623 \n",
            "Batch 2900 / 3125 - Loss:  0.9670 \n",
            "Batch 2910 / 3125 - Loss:  0.9954 \n",
            "Batch 2920 / 3125 - Loss:  1.0561 \n",
            "Batch 2930 / 3125 - Loss:  1.1161 \n",
            "Batch 2940 / 3125 - Loss:  1.0073 \n",
            "Batch 2950 / 3125 - Loss:  1.0622 \n",
            "Batch 2960 / 3125 - Loss:  1.0125 \n",
            "Batch 2970 / 3125 - Loss:  1.0655 \n",
            "Batch 2980 / 3125 - Loss:  1.0762 \n",
            "Batch 2990 / 3125 - Loss:  1.0198 \n",
            "Batch 3000 / 3125 - Loss:  1.1356 \n",
            "Batch 3010 / 3125 - Loss:  1.0467 \n",
            "Batch 3020 / 3125 - Loss:  1.1185 \n",
            "Batch 3030 / 3125 - Loss:  1.0469 \n",
            "Batch 3040 / 3125 - Loss:  0.9729 \n",
            "Batch 3050 / 3125 - Loss:  1.0420 \n",
            "Batch 3060 / 3125 - Loss:  1.0245 \n",
            "Batch 3070 / 3125 - Loss:  1.0625 \n",
            "Batch 3080 / 3125 - Loss:  1.0059 \n",
            "Batch 3090 / 3125 - Loss:  0.9905 \n",
            "Batch 3100 / 3125 - Loss:  0.9228 \n",
            "Batch 3110 / 3125 - Loss:  0.9500 \n",
            "Batch 3120 / 3125 - Loss:  0.8847 \n",
            "Epoch 42 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0814 \n",
            "Batch 10 / 3125 - Loss:  1.0452 \n",
            "Batch 20 / 3125 - Loss:  1.0898 \n",
            "Batch 30 / 3125 - Loss:  1.1610 \n",
            "Batch 40 / 3125 - Loss:  0.9343 \n",
            "Batch 50 / 3125 - Loss:  1.0204 \n",
            "Batch 60 / 3125 - Loss:  1.0201 \n",
            "Batch 70 / 3125 - Loss:  0.9990 \n",
            "Batch 80 / 3125 - Loss:  0.9600 \n",
            "Batch 90 / 3125 - Loss:  1.0995 \n",
            "Batch 100 / 3125 - Loss:  1.0180 \n",
            "Batch 110 / 3125 - Loss:  0.9648 \n",
            "Batch 120 / 3125 - Loss:  0.9877 \n",
            "Batch 130 / 3125 - Loss:  1.1127 \n",
            "Batch 140 / 3125 - Loss:  1.0311 \n",
            "Batch 150 / 3125 - Loss:  1.0272 \n",
            "Batch 160 / 3125 - Loss:  1.0890 \n",
            "Batch 170 / 3125 - Loss:  1.0290 \n",
            "Batch 180 / 3125 - Loss:  0.9688 \n",
            "Batch 190 / 3125 - Loss:  1.0685 \n",
            "Batch 200 / 3125 - Loss:  1.0662 \n",
            "Batch 210 / 3125 - Loss:  0.9850 \n",
            "Batch 220 / 3125 - Loss:  1.0851 \n",
            "Batch 230 / 3125 - Loss:  1.0654 \n",
            "Batch 240 / 3125 - Loss:  1.0261 \n",
            "Batch 250 / 3125 - Loss:  1.0077 \n",
            "Batch 260 / 3125 - Loss:  0.9867 \n",
            "Batch 270 / 3125 - Loss:  1.0409 \n",
            "Batch 280 / 3125 - Loss:  1.1422 \n",
            "Batch 290 / 3125 - Loss:  0.8935 \n",
            "Batch 300 / 3125 - Loss:  1.1265 \n",
            "Batch 310 / 3125 - Loss:  1.0100 \n",
            "Batch 320 / 3125 - Loss:  0.9674 \n",
            "Batch 330 / 3125 - Loss:  1.1056 \n",
            "Batch 340 / 3125 - Loss:  1.0148 \n",
            "Batch 350 / 3125 - Loss:  0.8970 \n",
            "Batch 360 / 3125 - Loss:  1.1178 \n",
            "Batch 370 / 3125 - Loss:  0.9886 \n",
            "Batch 380 / 3125 - Loss:  1.0431 \n",
            "Batch 390 / 3125 - Loss:  0.9339 \n",
            "Batch 400 / 3125 - Loss:  0.9860 \n",
            "Batch 410 / 3125 - Loss:  0.9952 \n",
            "Batch 420 / 3125 - Loss:  1.1988 \n",
            "Batch 430 / 3125 - Loss:  1.0083 \n",
            "Batch 440 / 3125 - Loss:  1.0092 \n",
            "Batch 450 / 3125 - Loss:  0.9257 \n",
            "Batch 460 / 3125 - Loss:  0.9865 \n",
            "Batch 470 / 3125 - Loss:  0.9597 \n",
            "Batch 480 / 3125 - Loss:  0.9661 \n",
            "Batch 490 / 3125 - Loss:  1.0426 \n",
            "Batch 500 / 3125 - Loss:  1.0342 \n",
            "Batch 510 / 3125 - Loss:  1.0847 \n",
            "Batch 520 / 3125 - Loss:  1.0486 \n",
            "Batch 530 / 3125 - Loss:  1.0759 \n",
            "Batch 540 / 3125 - Loss:  1.0781 \n",
            "Batch 550 / 3125 - Loss:  1.0761 \n",
            "Batch 560 / 3125 - Loss:  1.1116 \n",
            "Batch 570 / 3125 - Loss:  0.9604 \n",
            "Batch 580 / 3125 - Loss:  0.9090 \n",
            "Batch 590 / 3125 - Loss:  1.0855 \n",
            "Batch 600 / 3125 - Loss:  1.0575 \n",
            "Batch 610 / 3125 - Loss:  0.8956 \n",
            "Batch 620 / 3125 - Loss:  1.0247 \n",
            "Batch 630 / 3125 - Loss:  1.0913 \n",
            "Batch 640 / 3125 - Loss:  1.1353 \n",
            "Batch 650 / 3125 - Loss:  1.0467 \n",
            "Batch 660 / 3125 - Loss:  1.0184 \n",
            "Batch 670 / 3125 - Loss:  1.0786 \n",
            "Batch 680 / 3125 - Loss:  0.9744 \n",
            "Batch 690 / 3125 - Loss:  0.9152 \n",
            "Batch 700 / 3125 - Loss:  0.9955 \n",
            "Batch 710 / 3125 - Loss:  0.9819 \n",
            "Batch 720 / 3125 - Loss:  0.9308 \n",
            "Batch 730 / 3125 - Loss:  1.0245 \n",
            "Batch 740 / 3125 - Loss:  0.9986 \n",
            "Batch 750 / 3125 - Loss:  0.9093 \n",
            "Batch 760 / 3125 - Loss:  0.9569 \n",
            "Batch 770 / 3125 - Loss:  1.0333 \n",
            "Batch 780 / 3125 - Loss:  1.1280 \n",
            "Batch 790 / 3125 - Loss:  0.9918 \n",
            "Batch 800 / 3125 - Loss:  1.0948 \n",
            "Batch 810 / 3125 - Loss:  1.1379 \n",
            "Batch 820 / 3125 - Loss:  1.0003 \n",
            "Batch 830 / 3125 - Loss:  1.0534 \n",
            "Batch 840 / 3125 - Loss:  1.0894 \n",
            "Batch 850 / 3125 - Loss:  0.9288 \n",
            "Batch 860 / 3125 - Loss:  1.0419 \n",
            "Batch 870 / 3125 - Loss:  1.0355 \n",
            "Batch 880 / 3125 - Loss:  0.9747 \n",
            "Batch 890 / 3125 - Loss:  1.0295 \n",
            "Batch 900 / 3125 - Loss:  0.9509 \n",
            "Batch 910 / 3125 - Loss:  0.9745 \n",
            "Batch 920 / 3125 - Loss:  0.9625 \n",
            "Batch 930 / 3125 - Loss:  1.0538 \n",
            "Batch 940 / 3125 - Loss:  1.0700 \n",
            "Batch 950 / 3125 - Loss:  1.0342 \n",
            "Batch 960 / 3125 - Loss:  0.9547 \n",
            "Batch 970 / 3125 - Loss:  1.0424 \n",
            "Batch 980 / 3125 - Loss:  1.0371 \n",
            "Batch 990 / 3125 - Loss:  1.0606 \n",
            "Batch 1000 / 3125 - Loss:  0.9798 \n",
            "Batch 1010 / 3125 - Loss:  0.9549 \n",
            "Batch 1020 / 3125 - Loss:  1.0993 \n",
            "Batch 1030 / 3125 - Loss:  0.9659 \n",
            "Batch 1040 / 3125 - Loss:  1.0339 \n",
            "Batch 1050 / 3125 - Loss:  1.0106 \n",
            "Batch 1060 / 3125 - Loss:  1.0142 \n",
            "Batch 1070 / 3125 - Loss:  1.0050 \n",
            "Batch 1080 / 3125 - Loss:  0.9063 \n",
            "Batch 1090 / 3125 - Loss:  0.9622 \n",
            "Batch 1100 / 3125 - Loss:  1.1123 \n",
            "Batch 1110 / 3125 - Loss:  1.0191 \n",
            "Batch 1120 / 3125 - Loss:  0.9904 \n",
            "Batch 1130 / 3125 - Loss:  1.0158 \n",
            "Batch 1140 / 3125 - Loss:  0.9411 \n",
            "Batch 1150 / 3125 - Loss:  1.0259 \n",
            "Batch 1160 / 3125 - Loss:  1.1027 \n",
            "Batch 1170 / 3125 - Loss:  0.9473 \n",
            "Batch 1180 / 3125 - Loss:  0.9566 \n",
            "Batch 1190 / 3125 - Loss:  1.0528 \n",
            "Batch 1200 / 3125 - Loss:  1.0322 \n",
            "Batch 1210 / 3125 - Loss:  0.9566 \n",
            "Batch 1220 / 3125 - Loss:  1.0928 \n",
            "Batch 1230 / 3125 - Loss:  1.0112 \n",
            "Batch 1240 / 3125 - Loss:  1.0322 \n",
            "Batch 1250 / 3125 - Loss:  1.0371 \n",
            "Batch 1260 / 3125 - Loss:  1.0024 \n",
            "Batch 1270 / 3125 - Loss:  1.0470 \n",
            "Batch 1280 / 3125 - Loss:  1.1367 \n",
            "Batch 1290 / 3125 - Loss:  1.0807 \n",
            "Batch 1300 / 3125 - Loss:  1.1100 \n",
            "Batch 1310 / 3125 - Loss:  1.0118 \n",
            "Batch 1320 / 3125 - Loss:  1.0149 \n",
            "Batch 1330 / 3125 - Loss:  1.0104 \n",
            "Batch 1340 / 3125 - Loss:  0.9103 \n",
            "Batch 1350 / 3125 - Loss:  0.9707 \n",
            "Batch 1360 / 3125 - Loss:  1.0625 \n",
            "Batch 1370 / 3125 - Loss:  0.9411 \n",
            "Batch 1380 / 3125 - Loss:  1.0598 \n",
            "Batch 1390 / 3125 - Loss:  0.9960 \n",
            "Batch 1400 / 3125 - Loss:  0.9102 \n",
            "Batch 1410 / 3125 - Loss:  1.0153 \n",
            "Batch 1420 / 3125 - Loss:  0.9867 \n",
            "Batch 1430 / 3125 - Loss:  0.9532 \n",
            "Batch 1440 / 3125 - Loss:  0.9734 \n",
            "Batch 1450 / 3125 - Loss:  1.0387 \n",
            "Batch 1460 / 3125 - Loss:  1.0388 \n",
            "Batch 1470 / 3125 - Loss:  1.0763 \n",
            "Batch 1480 / 3125 - Loss:  1.1480 \n",
            "Batch 1490 / 3125 - Loss:  1.0090 \n",
            "Batch 1500 / 3125 - Loss:  1.0400 \n",
            "Batch 1510 / 3125 - Loss:  1.0487 \n",
            "Batch 1520 / 3125 - Loss:  0.9807 \n",
            "Batch 1530 / 3125 - Loss:  1.0508 \n",
            "Batch 1540 / 3125 - Loss:  0.9950 \n",
            "Batch 1550 / 3125 - Loss:  0.9783 \n",
            "Batch 1560 / 3125 - Loss:  0.9726 \n",
            "Batch 1570 / 3125 - Loss:  1.0766 \n",
            "Batch 1580 / 3125 - Loss:  1.0557 \n",
            "Batch 1590 / 3125 - Loss:  0.9349 \n",
            "Batch 1600 / 3125 - Loss:  1.0367 \n",
            "Batch 1610 / 3125 - Loss:  1.0834 \n",
            "Batch 1620 / 3125 - Loss:  1.0781 \n",
            "Batch 1630 / 3125 - Loss:  1.0362 \n",
            "Batch 1640 / 3125 - Loss:  0.9384 \n",
            "Batch 1650 / 3125 - Loss:  0.9432 \n",
            "Batch 1660 / 3125 - Loss:  1.0478 \n",
            "Batch 1670 / 3125 - Loss:  1.1127 \n",
            "Batch 1680 / 3125 - Loss:  0.9334 \n",
            "Batch 1690 / 3125 - Loss:  1.1116 \n",
            "Batch 1700 / 3125 - Loss:  1.0714 \n",
            "Batch 1710 / 3125 - Loss:  0.9752 \n",
            "Batch 1720 / 3125 - Loss:  1.0937 \n",
            "Batch 1730 / 3125 - Loss:  1.1318 \n",
            "Batch 1740 / 3125 - Loss:  0.9844 \n",
            "Batch 1750 / 3125 - Loss:  1.0098 \n",
            "Batch 1760 / 3125 - Loss:  1.0525 \n",
            "Batch 1770 / 3125 - Loss:  1.0631 \n",
            "Batch 1780 / 3125 - Loss:  1.0161 \n",
            "Batch 1790 / 3125 - Loss:  0.9901 \n",
            "Batch 1800 / 3125 - Loss:  0.9982 \n",
            "Batch 1810 / 3125 - Loss:  0.9739 \n",
            "Batch 1820 / 3125 - Loss:  1.0523 \n",
            "Batch 1830 / 3125 - Loss:  0.9683 \n",
            "Batch 1840 / 3125 - Loss:  0.9893 \n",
            "Batch 1850 / 3125 - Loss:  1.0537 \n",
            "Batch 1860 / 3125 - Loss:  1.0839 \n",
            "Batch 1870 / 3125 - Loss:  1.0512 \n",
            "Batch 1880 / 3125 - Loss:  0.9793 \n",
            "Batch 1890 / 3125 - Loss:  1.0621 \n",
            "Batch 1900 / 3125 - Loss:  1.1007 \n",
            "Batch 1910 / 3125 - Loss:  1.1687 \n",
            "Batch 1920 / 3125 - Loss:  1.0464 \n",
            "Batch 1930 / 3125 - Loss:  1.0279 \n",
            "Batch 1940 / 3125 - Loss:  1.0983 \n",
            "Batch 1950 / 3125 - Loss:  1.0336 \n",
            "Batch 1960 / 3125 - Loss:  1.0387 \n",
            "Batch 1970 / 3125 - Loss:  0.9683 \n",
            "Batch 1980 / 3125 - Loss:  1.0147 \n",
            "Batch 1990 / 3125 - Loss:  1.1109 \n",
            "Batch 2000 / 3125 - Loss:  1.0378 \n",
            "Batch 2010 / 3125 - Loss:  1.0642 \n",
            "Batch 2020 / 3125 - Loss:  1.0744 \n",
            "Batch 2030 / 3125 - Loss:  1.0066 \n",
            "Batch 2040 / 3125 - Loss:  0.9596 \n",
            "Batch 2050 / 3125 - Loss:  1.1058 \n",
            "Batch 2060 / 3125 - Loss:  1.1836 \n",
            "Batch 2070 / 3125 - Loss:  1.0338 \n",
            "Batch 2080 / 3125 - Loss:  0.9949 \n",
            "Batch 2090 / 3125 - Loss:  1.0714 \n",
            "Batch 2100 / 3125 - Loss:  0.9295 \n",
            "Batch 2110 / 3125 - Loss:  1.0025 \n",
            "Batch 2120 / 3125 - Loss:  0.9432 \n",
            "Batch 2130 / 3125 - Loss:  1.0397 \n",
            "Batch 2140 / 3125 - Loss:  1.0577 \n",
            "Batch 2150 / 3125 - Loss:  1.0288 \n",
            "Batch 2160 / 3125 - Loss:  0.9239 \n",
            "Batch 2170 / 3125 - Loss:  1.0813 \n",
            "Batch 2180 / 3125 - Loss:  1.0863 \n",
            "Batch 2190 / 3125 - Loss:  1.0682 \n",
            "Batch 2200 / 3125 - Loss:  0.9882 \n",
            "Batch 2210 / 3125 - Loss:  1.0410 \n",
            "Batch 2220 / 3125 - Loss:  0.9699 \n",
            "Batch 2230 / 3125 - Loss:  1.0410 \n",
            "Batch 2240 / 3125 - Loss:  1.0568 \n",
            "Batch 2250 / 3125 - Loss:  1.0849 \n",
            "Batch 2260 / 3125 - Loss:  1.0422 \n",
            "Batch 2270 / 3125 - Loss:  1.0943 \n",
            "Batch 2280 / 3125 - Loss:  1.0403 \n",
            "Batch 2290 / 3125 - Loss:  1.0287 \n",
            "Batch 2300 / 3125 - Loss:  1.0098 \n",
            "Batch 2310 / 3125 - Loss:  1.0755 \n",
            "Batch 2320 / 3125 - Loss:  1.1872 \n",
            "Batch 2330 / 3125 - Loss:  1.0767 \n",
            "Batch 2340 / 3125 - Loss:  1.0763 \n",
            "Batch 2350 / 3125 - Loss:  0.9464 \n",
            "Batch 2360 / 3125 - Loss:  0.9058 \n",
            "Batch 2370 / 3125 - Loss:  0.9673 \n",
            "Batch 2380 / 3125 - Loss:  0.9987 \n",
            "Batch 2390 / 3125 - Loss:  0.9620 \n",
            "Batch 2400 / 3125 - Loss:  0.9942 \n",
            "Batch 2410 / 3125 - Loss:  1.0358 \n",
            "Batch 2420 / 3125 - Loss:  0.9917 \n",
            "Batch 2430 / 3125 - Loss:  0.9446 \n",
            "Batch 2440 / 3125 - Loss:  1.0523 \n",
            "Batch 2450 / 3125 - Loss:  1.0406 \n",
            "Batch 2460 / 3125 - Loss:  1.1151 \n",
            "Batch 2470 / 3125 - Loss:  1.0969 \n",
            "Batch 2480 / 3125 - Loss:  1.1306 \n",
            "Batch 2490 / 3125 - Loss:  0.9663 \n",
            "Batch 2500 / 3125 - Loss:  1.0089 \n",
            "Batch 2510 / 3125 - Loss:  0.9373 \n",
            "Batch 2520 / 3125 - Loss:  0.9439 \n",
            "Batch 2530 / 3125 - Loss:  1.1784 \n",
            "Batch 2540 / 3125 - Loss:  1.1734 \n",
            "Batch 2550 / 3125 - Loss:  1.0636 \n",
            "Batch 2560 / 3125 - Loss:  1.0468 \n",
            "Batch 2570 / 3125 - Loss:  1.0670 \n",
            "Batch 2580 / 3125 - Loss:  1.0801 \n",
            "Batch 2590 / 3125 - Loss:  0.9615 \n",
            "Batch 2600 / 3125 - Loss:  1.0757 \n",
            "Batch 2610 / 3125 - Loss:  1.0769 \n",
            "Batch 2620 / 3125 - Loss:  1.0523 \n",
            "Batch 2630 / 3125 - Loss:  1.0690 \n",
            "Batch 2640 / 3125 - Loss:  1.0050 \n",
            "Batch 2650 / 3125 - Loss:  0.9982 \n",
            "Batch 2660 / 3125 - Loss:  0.9549 \n",
            "Batch 2670 / 3125 - Loss:  1.1402 \n",
            "Batch 2680 / 3125 - Loss:  1.0001 \n",
            "Batch 2690 / 3125 - Loss:  1.0403 \n",
            "Batch 2700 / 3125 - Loss:  0.9868 \n",
            "Batch 2710 / 3125 - Loss:  0.9497 \n",
            "Batch 2720 / 3125 - Loss:  1.0257 \n",
            "Batch 2730 / 3125 - Loss:  1.0001 \n",
            "Batch 2740 / 3125 - Loss:  1.0315 \n",
            "Batch 2750 / 3125 - Loss:  0.9991 \n",
            "Batch 2760 / 3125 - Loss:  0.9249 \n",
            "Batch 2770 / 3125 - Loss:  1.0417 \n",
            "Batch 2780 / 3125 - Loss:  0.9205 \n",
            "Batch 2790 / 3125 - Loss:  0.9389 \n",
            "Batch 2800 / 3125 - Loss:  0.9206 \n",
            "Batch 2810 / 3125 - Loss:  1.0312 \n",
            "Batch 2820 / 3125 - Loss:  1.1251 \n",
            "Batch 2830 / 3125 - Loss:  0.9842 \n",
            "Batch 2840 / 3125 - Loss:  1.0028 \n",
            "Batch 2850 / 3125 - Loss:  1.0144 \n",
            "Batch 2860 / 3125 - Loss:  1.0069 \n",
            "Batch 2870 / 3125 - Loss:  1.0246 \n",
            "Batch 2880 / 3125 - Loss:  0.9901 \n",
            "Batch 2890 / 3125 - Loss:  0.9630 \n",
            "Batch 2900 / 3125 - Loss:  1.0422 \n",
            "Batch 2910 / 3125 - Loss:  1.0278 \n",
            "Batch 2920 / 3125 - Loss:  1.0207 \n",
            "Batch 2930 / 3125 - Loss:  0.9938 \n",
            "Batch 2940 / 3125 - Loss:  0.9975 \n",
            "Batch 2950 / 3125 - Loss:  1.0338 \n",
            "Batch 2960 / 3125 - Loss:  1.1719 \n",
            "Batch 2970 / 3125 - Loss:  1.0726 \n",
            "Batch 2980 / 3125 - Loss:  1.0243 \n",
            "Batch 2990 / 3125 - Loss:  1.0690 \n",
            "Batch 3000 / 3125 - Loss:  1.0181 \n",
            "Batch 3010 / 3125 - Loss:  0.9919 \n",
            "Batch 3020 / 3125 - Loss:  1.0258 \n",
            "Batch 3030 / 3125 - Loss:  1.0436 \n",
            "Batch 3040 / 3125 - Loss:  0.9621 \n",
            "Batch 3050 / 3125 - Loss:  0.9705 \n",
            "Batch 3060 / 3125 - Loss:  1.0768 \n",
            "Batch 3070 / 3125 - Loss:  1.0349 \n",
            "Batch 3080 / 3125 - Loss:  1.0907 \n",
            "Batch 3090 / 3125 - Loss:  1.0064 \n",
            "Batch 3100 / 3125 - Loss:  1.0446 \n",
            "Batch 3110 / 3125 - Loss:  1.0496 \n",
            "Batch 3120 / 3125 - Loss:  1.1301 \n",
            "Epoch 43 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.1105 \n",
            "Batch 10 / 3125 - Loss:  1.1355 \n",
            "Batch 20 / 3125 - Loss:  1.0402 \n",
            "Batch 30 / 3125 - Loss:  1.0375 \n",
            "Batch 40 / 3125 - Loss:  0.9527 \n",
            "Batch 50 / 3125 - Loss:  1.0640 \n",
            "Batch 60 / 3125 - Loss:  0.9930 \n",
            "Batch 70 / 3125 - Loss:  1.0033 \n",
            "Batch 80 / 3125 - Loss:  0.9325 \n",
            "Batch 90 / 3125 - Loss:  1.0607 \n",
            "Batch 100 / 3125 - Loss:  1.0457 \n",
            "Batch 110 / 3125 - Loss:  0.9817 \n",
            "Batch 120 / 3125 - Loss:  1.0229 \n",
            "Batch 130 / 3125 - Loss:  1.0150 \n",
            "Batch 140 / 3125 - Loss:  1.0547 \n",
            "Batch 150 / 3125 - Loss:  1.0449 \n",
            "Batch 160 / 3125 - Loss:  1.0944 \n",
            "Batch 170 / 3125 - Loss:  0.9972 \n",
            "Batch 180 / 3125 - Loss:  1.1705 \n",
            "Batch 190 / 3125 - Loss:  0.9438 \n",
            "Batch 200 / 3125 - Loss:  1.1486 \n",
            "Batch 210 / 3125 - Loss:  1.0663 \n",
            "Batch 220 / 3125 - Loss:  1.0233 \n",
            "Batch 230 / 3125 - Loss:  1.0333 \n",
            "Batch 240 / 3125 - Loss:  0.9211 \n",
            "Batch 250 / 3125 - Loss:  0.9320 \n",
            "Batch 260 / 3125 - Loss:  1.0468 \n",
            "Batch 270 / 3125 - Loss:  1.0037 \n",
            "Batch 280 / 3125 - Loss:  0.8984 \n",
            "Batch 290 / 3125 - Loss:  1.1404 \n",
            "Batch 300 / 3125 - Loss:  1.0447 \n",
            "Batch 310 / 3125 - Loss:  0.9656 \n",
            "Batch 320 / 3125 - Loss:  0.9744 \n",
            "Batch 330 / 3125 - Loss:  1.0895 \n",
            "Batch 340 / 3125 - Loss:  0.9163 \n",
            "Batch 350 / 3125 - Loss:  0.9277 \n",
            "Batch 360 / 3125 - Loss:  0.9865 \n",
            "Batch 370 / 3125 - Loss:  1.0542 \n",
            "Batch 380 / 3125 - Loss:  1.1288 \n",
            "Batch 390 / 3125 - Loss:  1.0022 \n",
            "Batch 400 / 3125 - Loss:  1.0984 \n",
            "Batch 410 / 3125 - Loss:  0.9294 \n",
            "Batch 420 / 3125 - Loss:  0.9973 \n",
            "Batch 430 / 3125 - Loss:  1.1050 \n",
            "Batch 440 / 3125 - Loss:  1.0550 \n",
            "Batch 450 / 3125 - Loss:  1.1060 \n",
            "Batch 460 / 3125 - Loss:  1.0378 \n",
            "Batch 470 / 3125 - Loss:  1.0437 \n",
            "Batch 480 / 3125 - Loss:  1.0655 \n",
            "Batch 490 / 3125 - Loss:  1.1416 \n",
            "Batch 500 / 3125 - Loss:  1.0410 \n",
            "Batch 510 / 3125 - Loss:  0.9899 \n",
            "Batch 520 / 3125 - Loss:  1.0596 \n",
            "Batch 530 / 3125 - Loss:  1.0573 \n",
            "Batch 540 / 3125 - Loss:  1.0498 \n",
            "Batch 550 / 3125 - Loss:  1.0838 \n",
            "Batch 560 / 3125 - Loss:  1.1295 \n",
            "Batch 570 / 3125 - Loss:  0.9679 \n",
            "Batch 580 / 3125 - Loss:  0.9749 \n",
            "Batch 590 / 3125 - Loss:  0.9799 \n",
            "Batch 600 / 3125 - Loss:  1.0219 \n",
            "Batch 610 / 3125 - Loss:  1.1550 \n",
            "Batch 620 / 3125 - Loss:  0.9648 \n",
            "Batch 630 / 3125 - Loss:  1.0952 \n",
            "Batch 640 / 3125 - Loss:  0.9692 \n",
            "Batch 650 / 3125 - Loss:  1.0012 \n",
            "Batch 660 / 3125 - Loss:  1.1805 \n",
            "Batch 670 / 3125 - Loss:  0.9847 \n",
            "Batch 680 / 3125 - Loss:  0.9510 \n",
            "Batch 690 / 3125 - Loss:  1.0494 \n",
            "Batch 700 / 3125 - Loss:  1.0290 \n",
            "Batch 710 / 3125 - Loss:  1.0552 \n",
            "Batch 720 / 3125 - Loss:  1.0890 \n",
            "Batch 730 / 3125 - Loss:  0.8708 \n",
            "Batch 740 / 3125 - Loss:  1.0611 \n",
            "Batch 750 / 3125 - Loss:  0.9896 \n",
            "Batch 760 / 3125 - Loss:  1.0171 \n",
            "Batch 770 / 3125 - Loss:  1.0274 \n",
            "Batch 780 / 3125 - Loss:  1.0602 \n",
            "Batch 790 / 3125 - Loss:  1.0950 \n",
            "Batch 800 / 3125 - Loss:  0.9737 \n",
            "Batch 810 / 3125 - Loss:  1.1199 \n",
            "Batch 820 / 3125 - Loss:  1.1103 \n",
            "Batch 830 / 3125 - Loss:  1.0301 \n",
            "Batch 840 / 3125 - Loss:  0.9519 \n",
            "Batch 850 / 3125 - Loss:  1.0359 \n",
            "Batch 860 / 3125 - Loss:  1.0251 \n",
            "Batch 870 / 3125 - Loss:  1.0214 \n",
            "Batch 880 / 3125 - Loss:  0.8923 \n",
            "Batch 890 / 3125 - Loss:  1.0087 \n",
            "Batch 900 / 3125 - Loss:  1.0751 \n",
            "Batch 910 / 3125 - Loss:  0.9835 \n",
            "Batch 920 / 3125 - Loss:  1.1066 \n",
            "Batch 930 / 3125 - Loss:  1.0100 \n",
            "Batch 940 / 3125 - Loss:  1.0544 \n",
            "Batch 950 / 3125 - Loss:  0.9107 \n",
            "Batch 960 / 3125 - Loss:  0.9406 \n",
            "Batch 970 / 3125 - Loss:  1.1515 \n",
            "Batch 980 / 3125 - Loss:  1.1094 \n",
            "Batch 990 / 3125 - Loss:  1.0210 \n",
            "Batch 1000 / 3125 - Loss:  1.0402 \n",
            "Batch 1010 / 3125 - Loss:  1.0611 \n",
            "Batch 1020 / 3125 - Loss:  1.0434 \n",
            "Batch 1030 / 3125 - Loss:  0.9034 \n",
            "Batch 1040 / 3125 - Loss:  1.0508 \n",
            "Batch 1050 / 3125 - Loss:  1.1497 \n",
            "Batch 1060 / 3125 - Loss:  1.0562 \n",
            "Batch 1070 / 3125 - Loss:  1.1347 \n",
            "Batch 1080 / 3125 - Loss:  0.9155 \n",
            "Batch 1090 / 3125 - Loss:  1.0001 \n",
            "Batch 1100 / 3125 - Loss:  1.1018 \n",
            "Batch 1110 / 3125 - Loss:  0.9755 \n",
            "Batch 1120 / 3125 - Loss:  0.9433 \n",
            "Batch 1130 / 3125 - Loss:  1.0924 \n",
            "Batch 1140 / 3125 - Loss:  1.0220 \n",
            "Batch 1150 / 3125 - Loss:  0.9605 \n",
            "Batch 1160 / 3125 - Loss:  0.9313 \n",
            "Batch 1170 / 3125 - Loss:  1.0987 \n",
            "Batch 1180 / 3125 - Loss:  1.1343 \n",
            "Batch 1190 / 3125 - Loss:  0.9474 \n",
            "Batch 1200 / 3125 - Loss:  0.9958 \n",
            "Batch 1210 / 3125 - Loss:  1.0357 \n",
            "Batch 1220 / 3125 - Loss:  1.0832 \n",
            "Batch 1230 / 3125 - Loss:  1.0574 \n",
            "Batch 1240 / 3125 - Loss:  1.0648 \n",
            "Batch 1250 / 3125 - Loss:  1.0584 \n",
            "Batch 1260 / 3125 - Loss:  1.0913 \n",
            "Batch 1270 / 3125 - Loss:  0.9661 \n",
            "Batch 1280 / 3125 - Loss:  1.1240 \n",
            "Batch 1290 / 3125 - Loss:  0.9892 \n",
            "Batch 1300 / 3125 - Loss:  1.0992 \n",
            "Batch 1310 / 3125 - Loss:  0.9111 \n",
            "Batch 1320 / 3125 - Loss:  1.0173 \n",
            "Batch 1330 / 3125 - Loss:  1.0099 \n",
            "Batch 1340 / 3125 - Loss:  1.0331 \n",
            "Batch 1350 / 3125 - Loss:  0.9117 \n",
            "Batch 1360 / 3125 - Loss:  1.0485 \n",
            "Batch 1370 / 3125 - Loss:  0.9939 \n",
            "Batch 1380 / 3125 - Loss:  1.0247 \n",
            "Batch 1390 / 3125 - Loss:  1.0264 \n",
            "Batch 1400 / 3125 - Loss:  1.0285 \n",
            "Batch 1410 / 3125 - Loss:  1.0264 \n",
            "Batch 1420 / 3125 - Loss:  1.1303 \n",
            "Batch 1430 / 3125 - Loss:  1.1715 \n",
            "Batch 1440 / 3125 - Loss:  1.0184 \n",
            "Batch 1450 / 3125 - Loss:  1.0433 \n",
            "Batch 1460 / 3125 - Loss:  1.0179 \n",
            "Batch 1470 / 3125 - Loss:  1.1525 \n",
            "Batch 1480 / 3125 - Loss:  1.0099 \n",
            "Batch 1490 / 3125 - Loss:  1.0635 \n",
            "Batch 1500 / 3125 - Loss:  0.9455 \n",
            "Batch 1510 / 3125 - Loss:  1.0246 \n",
            "Batch 1520 / 3125 - Loss:  0.9845 \n",
            "Batch 1530 / 3125 - Loss:  1.0044 \n",
            "Batch 1540 / 3125 - Loss:  0.9992 \n",
            "Batch 1550 / 3125 - Loss:  1.0981 \n",
            "Batch 1560 / 3125 - Loss:  1.0817 \n",
            "Batch 1570 / 3125 - Loss:  1.1144 \n",
            "Batch 1580 / 3125 - Loss:  1.0309 \n",
            "Batch 1590 / 3125 - Loss:  0.9375 \n",
            "Batch 1600 / 3125 - Loss:  1.0857 \n",
            "Batch 1610 / 3125 - Loss:  0.9675 \n",
            "Batch 1620 / 3125 - Loss:  1.0611 \n",
            "Batch 1630 / 3125 - Loss:  1.0397 \n",
            "Batch 1640 / 3125 - Loss:  1.0363 \n",
            "Batch 1650 / 3125 - Loss:  0.9363 \n",
            "Batch 1660 / 3125 - Loss:  1.0196 \n",
            "Batch 1670 / 3125 - Loss:  0.9773 \n",
            "Batch 1680 / 3125 - Loss:  0.9720 \n",
            "Batch 1690 / 3125 - Loss:  1.0732 \n",
            "Batch 1700 / 3125 - Loss:  1.0471 \n",
            "Batch 1710 / 3125 - Loss:  0.9789 \n",
            "Batch 1720 / 3125 - Loss:  1.0534 \n",
            "Batch 1730 / 3125 - Loss:  1.0447 \n",
            "Batch 1740 / 3125 - Loss:  1.0101 \n",
            "Batch 1750 / 3125 - Loss:  1.0490 \n",
            "Batch 1760 / 3125 - Loss:  0.9900 \n",
            "Batch 1770 / 3125 - Loss:  1.0294 \n",
            "Batch 1780 / 3125 - Loss:  0.9903 \n",
            "Batch 1790 / 3125 - Loss:  1.1821 \n",
            "Batch 1800 / 3125 - Loss:  1.0820 \n",
            "Batch 1810 / 3125 - Loss:  0.9981 \n",
            "Batch 1820 / 3125 - Loss:  1.0818 \n",
            "Batch 1830 / 3125 - Loss:  0.9368 \n",
            "Batch 1840 / 3125 - Loss:  1.0032 \n",
            "Batch 1850 / 3125 - Loss:  0.9552 \n",
            "Batch 1860 / 3125 - Loss:  1.0611 \n",
            "Batch 1870 / 3125 - Loss:  1.0727 \n",
            "Batch 1880 / 3125 - Loss:  0.9259 \n",
            "Batch 1890 / 3125 - Loss:  0.9882 \n",
            "Batch 1900 / 3125 - Loss:  1.1279 \n",
            "Batch 1910 / 3125 - Loss:  0.9605 \n",
            "Batch 1920 / 3125 - Loss:  0.9997 \n",
            "Batch 1930 / 3125 - Loss:  1.0407 \n",
            "Batch 1940 / 3125 - Loss:  0.9787 \n",
            "Batch 1950 / 3125 - Loss:  1.0636 \n",
            "Batch 1960 / 3125 - Loss:  1.0495 \n",
            "Batch 1970 / 3125 - Loss:  1.0473 \n",
            "Batch 1980 / 3125 - Loss:  0.9704 \n",
            "Batch 1990 / 3125 - Loss:  1.2066 \n",
            "Batch 2000 / 3125 - Loss:  0.9537 \n",
            "Batch 2010 / 3125 - Loss:  0.9655 \n",
            "Batch 2020 / 3125 - Loss:  1.0870 \n",
            "Batch 2030 / 3125 - Loss:  0.9696 \n",
            "Batch 2040 / 3125 - Loss:  1.0115 \n",
            "Batch 2050 / 3125 - Loss:  1.0056 \n",
            "Batch 2060 / 3125 - Loss:  0.9424 \n",
            "Batch 2070 / 3125 - Loss:  1.0555 \n",
            "Batch 2080 / 3125 - Loss:  1.0868 \n",
            "Batch 2090 / 3125 - Loss:  1.1063 \n",
            "Batch 2100 / 3125 - Loss:  1.1177 \n",
            "Batch 2110 / 3125 - Loss:  1.0510 \n",
            "Batch 2120 / 3125 - Loss:  0.9804 \n",
            "Batch 2130 / 3125 - Loss:  1.1123 \n",
            "Batch 2140 / 3125 - Loss:  1.0747 \n",
            "Batch 2150 / 3125 - Loss:  1.0106 \n",
            "Batch 2160 / 3125 - Loss:  0.9668 \n",
            "Batch 2170 / 3125 - Loss:  1.0283 \n",
            "Batch 2180 / 3125 - Loss:  0.9771 \n",
            "Batch 2190 / 3125 - Loss:  1.1099 \n",
            "Batch 2200 / 3125 - Loss:  1.0896 \n",
            "Batch 2210 / 3125 - Loss:  1.0656 \n",
            "Batch 2220 / 3125 - Loss:  1.0898 \n",
            "Batch 2230 / 3125 - Loss:  1.0171 \n",
            "Batch 2240 / 3125 - Loss:  1.0436 \n",
            "Batch 2250 / 3125 - Loss:  1.0816 \n",
            "Batch 2260 / 3125 - Loss:  0.9719 \n",
            "Batch 2270 / 3125 - Loss:  0.9312 \n",
            "Batch 2280 / 3125 - Loss:  1.0453 \n",
            "Batch 2290 / 3125 - Loss:  1.0920 \n",
            "Batch 2300 / 3125 - Loss:  1.0444 \n",
            "Batch 2310 / 3125 - Loss:  1.0319 \n",
            "Batch 2320 / 3125 - Loss:  1.0439 \n",
            "Batch 2330 / 3125 - Loss:  0.9684 \n",
            "Batch 2340 / 3125 - Loss:  1.0665 \n",
            "Batch 2350 / 3125 - Loss:  1.0322 \n",
            "Batch 2360 / 3125 - Loss:  1.0389 \n",
            "Batch 2370 / 3125 - Loss:  1.1000 \n",
            "Batch 2380 / 3125 - Loss:  0.9724 \n",
            "Batch 2390 / 3125 - Loss:  1.0562 \n",
            "Batch 2400 / 3125 - Loss:  1.0610 \n",
            "Batch 2410 / 3125 - Loss:  1.0167 \n",
            "Batch 2420 / 3125 - Loss:  0.9406 \n",
            "Batch 2430 / 3125 - Loss:  1.0117 \n",
            "Batch 2440 / 3125 - Loss:  1.0882 \n",
            "Batch 2450 / 3125 - Loss:  1.1000 \n",
            "Batch 2460 / 3125 - Loss:  0.8911 \n",
            "Batch 2470 / 3125 - Loss:  1.0656 \n",
            "Batch 2480 / 3125 - Loss:  1.0569 \n",
            "Batch 2490 / 3125 - Loss:  0.8966 \n",
            "Batch 2500 / 3125 - Loss:  1.0504 \n",
            "Batch 2510 / 3125 - Loss:  1.0148 \n",
            "Batch 2520 / 3125 - Loss:  1.1241 \n",
            "Batch 2530 / 3125 - Loss:  1.0267 \n",
            "Batch 2540 / 3125 - Loss:  0.9637 \n",
            "Batch 2550 / 3125 - Loss:  1.0398 \n",
            "Batch 2560 / 3125 - Loss:  1.1031 \n",
            "Batch 2570 / 3125 - Loss:  1.0091 \n",
            "Batch 2580 / 3125 - Loss:  0.9775 \n",
            "Batch 2590 / 3125 - Loss:  1.0989 \n",
            "Batch 2600 / 3125 - Loss:  1.1045 \n",
            "Batch 2610 / 3125 - Loss:  1.1421 \n",
            "Batch 2620 / 3125 - Loss:  0.9806 \n",
            "Batch 2630 / 3125 - Loss:  1.0080 \n",
            "Batch 2640 / 3125 - Loss:  1.1500 \n",
            "Batch 2650 / 3125 - Loss:  0.9909 \n",
            "Batch 2660 / 3125 - Loss:  1.0160 \n",
            "Batch 2670 / 3125 - Loss:  0.9796 \n",
            "Batch 2680 / 3125 - Loss:  1.0177 \n",
            "Batch 2690 / 3125 - Loss:  1.0595 \n",
            "Batch 2700 / 3125 - Loss:  0.9721 \n",
            "Batch 2710 / 3125 - Loss:  1.0114 \n",
            "Batch 2720 / 3125 - Loss:  0.9696 \n",
            "Batch 2730 / 3125 - Loss:  1.0637 \n",
            "Batch 2740 / 3125 - Loss:  1.0590 \n",
            "Batch 2750 / 3125 - Loss:  0.9542 \n",
            "Batch 2760 / 3125 - Loss:  0.9798 \n",
            "Batch 2770 / 3125 - Loss:  1.1173 \n",
            "Batch 2780 / 3125 - Loss:  1.0378 \n",
            "Batch 2790 / 3125 - Loss:  1.0496 \n",
            "Batch 2800 / 3125 - Loss:  1.1658 \n",
            "Batch 2810 / 3125 - Loss:  0.9938 \n",
            "Batch 2820 / 3125 - Loss:  0.9784 \n",
            "Batch 2830 / 3125 - Loss:  1.0130 \n",
            "Batch 2840 / 3125 - Loss:  0.9267 \n",
            "Batch 2850 / 3125 - Loss:  1.0097 \n",
            "Batch 2860 / 3125 - Loss:  0.9364 \n",
            "Batch 2870 / 3125 - Loss:  1.0117 \n",
            "Batch 2880 / 3125 - Loss:  1.1175 \n",
            "Batch 2890 / 3125 - Loss:  0.9566 \n",
            "Batch 2900 / 3125 - Loss:  1.1450 \n",
            "Batch 2910 / 3125 - Loss:  0.9753 \n",
            "Batch 2920 / 3125 - Loss:  0.9225 \n",
            "Batch 2930 / 3125 - Loss:  1.0712 \n",
            "Batch 2940 / 3125 - Loss:  1.0260 \n",
            "Batch 2950 / 3125 - Loss:  1.1322 \n",
            "Batch 2960 / 3125 - Loss:  1.0244 \n",
            "Batch 2970 / 3125 - Loss:  1.0105 \n",
            "Batch 2980 / 3125 - Loss:  0.9381 \n",
            "Batch 2990 / 3125 - Loss:  1.1252 \n",
            "Batch 3000 / 3125 - Loss:  1.0079 \n",
            "Batch 3010 / 3125 - Loss:  1.0386 \n",
            "Batch 3020 / 3125 - Loss:  1.0003 \n",
            "Batch 3030 / 3125 - Loss:  1.0259 \n",
            "Batch 3040 / 3125 - Loss:  1.0649 \n",
            "Batch 3050 / 3125 - Loss:  0.9944 \n",
            "Batch 3060 / 3125 - Loss:  1.0370 \n",
            "Batch 3070 / 3125 - Loss:  1.0454 \n",
            "Batch 3080 / 3125 - Loss:  1.0734 \n",
            "Batch 3090 / 3125 - Loss:  1.0141 \n",
            "Batch 3100 / 3125 - Loss:  1.0598 \n",
            "Batch 3110 / 3125 - Loss:  0.9053 \n",
            "Batch 3120 / 3125 - Loss:  0.9430 \n",
            "Epoch 44 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.1410 \n",
            "Batch 10 / 3125 - Loss:  1.0926 \n",
            "Batch 20 / 3125 - Loss:  1.0642 \n",
            "Batch 30 / 3125 - Loss:  1.0985 \n",
            "Batch 40 / 3125 - Loss:  0.9299 \n",
            "Batch 50 / 3125 - Loss:  1.0968 \n",
            "Batch 60 / 3125 - Loss:  0.9634 \n",
            "Batch 70 / 3125 - Loss:  1.1080 \n",
            "Batch 80 / 3125 - Loss:  1.1265 \n",
            "Batch 90 / 3125 - Loss:  1.0053 \n",
            "Batch 100 / 3125 - Loss:  1.0684 \n",
            "Batch 110 / 3125 - Loss:  1.1146 \n",
            "Batch 120 / 3125 - Loss:  1.0733 \n",
            "Batch 130 / 3125 - Loss:  1.0707 \n",
            "Batch 140 / 3125 - Loss:  1.0220 \n",
            "Batch 150 / 3125 - Loss:  1.0548 \n",
            "Batch 160 / 3125 - Loss:  0.9340 \n",
            "Batch 170 / 3125 - Loss:  1.0352 \n",
            "Batch 180 / 3125 - Loss:  1.0924 \n",
            "Batch 190 / 3125 - Loss:  1.1314 \n",
            "Batch 200 / 3125 - Loss:  1.0330 \n",
            "Batch 210 / 3125 - Loss:  1.0537 \n",
            "Batch 220 / 3125 - Loss:  1.0441 \n",
            "Batch 230 / 3125 - Loss:  0.9733 \n",
            "Batch 240 / 3125 - Loss:  1.0229 \n",
            "Batch 250 / 3125 - Loss:  1.0596 \n",
            "Batch 260 / 3125 - Loss:  0.9368 \n",
            "Batch 270 / 3125 - Loss:  1.1252 \n",
            "Batch 280 / 3125 - Loss:  0.9685 \n",
            "Batch 290 / 3125 - Loss:  1.0630 \n",
            "Batch 300 / 3125 - Loss:  1.0175 \n",
            "Batch 310 / 3125 - Loss:  1.1649 \n",
            "Batch 320 / 3125 - Loss:  1.0135 \n",
            "Batch 330 / 3125 - Loss:  1.1043 \n",
            "Batch 340 / 3125 - Loss:  0.9891 \n",
            "Batch 350 / 3125 - Loss:  0.9756 \n",
            "Batch 360 / 3125 - Loss:  1.0432 \n",
            "Batch 370 / 3125 - Loss:  0.9967 \n",
            "Batch 380 / 3125 - Loss:  1.0865 \n",
            "Batch 390 / 3125 - Loss:  1.0203 \n",
            "Batch 400 / 3125 - Loss:  0.8957 \n",
            "Batch 410 / 3125 - Loss:  0.8655 \n",
            "Batch 420 / 3125 - Loss:  1.0687 \n",
            "Batch 430 / 3125 - Loss:  1.0388 \n",
            "Batch 440 / 3125 - Loss:  0.9965 \n",
            "Batch 450 / 3125 - Loss:  0.9755 \n",
            "Batch 460 / 3125 - Loss:  0.9496 \n",
            "Batch 470 / 3125 - Loss:  0.9790 \n",
            "Batch 480 / 3125 - Loss:  0.9375 \n",
            "Batch 490 / 3125 - Loss:  1.1251 \n",
            "Batch 500 / 3125 - Loss:  1.0721 \n",
            "Batch 510 / 3125 - Loss:  1.1252 \n",
            "Batch 520 / 3125 - Loss:  0.9683 \n",
            "Batch 530 / 3125 - Loss:  0.9546 \n",
            "Batch 540 / 3125 - Loss:  1.0188 \n",
            "Batch 550 / 3125 - Loss:  0.9683 \n",
            "Batch 560 / 3125 - Loss:  1.0065 \n",
            "Batch 570 / 3125 - Loss:  1.1396 \n",
            "Batch 580 / 3125 - Loss:  1.0556 \n",
            "Batch 590 / 3125 - Loss:  0.8786 \n",
            "Batch 600 / 3125 - Loss:  1.0652 \n",
            "Batch 610 / 3125 - Loss:  1.0393 \n",
            "Batch 620 / 3125 - Loss:  1.0135 \n",
            "Batch 630 / 3125 - Loss:  1.1073 \n",
            "Batch 640 / 3125 - Loss:  1.0776 \n",
            "Batch 650 / 3125 - Loss:  1.0303 \n",
            "Batch 660 / 3125 - Loss:  0.9899 \n",
            "Batch 670 / 3125 - Loss:  0.9627 \n",
            "Batch 680 / 3125 - Loss:  1.0360 \n",
            "Batch 690 / 3125 - Loss:  0.9282 \n",
            "Batch 700 / 3125 - Loss:  1.0242 \n",
            "Batch 710 / 3125 - Loss:  1.0290 \n",
            "Batch 720 / 3125 - Loss:  1.1063 \n",
            "Batch 730 / 3125 - Loss:  0.9836 \n",
            "Batch 740 / 3125 - Loss:  1.0166 \n",
            "Batch 750 / 3125 - Loss:  1.0240 \n",
            "Batch 760 / 3125 - Loss:  1.0471 \n",
            "Batch 770 / 3125 - Loss:  1.0352 \n",
            "Batch 780 / 3125 - Loss:  0.9874 \n",
            "Batch 790 / 3125 - Loss:  0.9817 \n",
            "Batch 800 / 3125 - Loss:  1.0325 \n",
            "Batch 810 / 3125 - Loss:  1.0680 \n",
            "Batch 820 / 3125 - Loss:  1.1614 \n",
            "Batch 830 / 3125 - Loss:  0.9978 \n",
            "Batch 840 / 3125 - Loss:  0.9023 \n",
            "Batch 850 / 3125 - Loss:  0.9994 \n",
            "Batch 860 / 3125 - Loss:  1.0339 \n",
            "Batch 870 / 3125 - Loss:  1.0564 \n",
            "Batch 880 / 3125 - Loss:  1.1390 \n",
            "Batch 890 / 3125 - Loss:  1.0178 \n",
            "Batch 900 / 3125 - Loss:  0.9548 \n",
            "Batch 910 / 3125 - Loss:  0.9877 \n",
            "Batch 920 / 3125 - Loss:  1.0210 \n",
            "Batch 930 / 3125 - Loss:  1.0579 \n",
            "Batch 940 / 3125 - Loss:  1.0482 \n",
            "Batch 950 / 3125 - Loss:  1.1414 \n",
            "Batch 960 / 3125 - Loss:  1.0830 \n",
            "Batch 970 / 3125 - Loss:  0.9751 \n",
            "Batch 980 / 3125 - Loss:  0.9654 \n",
            "Batch 990 / 3125 - Loss:  1.0785 \n",
            "Batch 1000 / 3125 - Loss:  1.0310 \n",
            "Batch 1010 / 3125 - Loss:  1.0603 \n",
            "Batch 1020 / 3125 - Loss:  1.0274 \n",
            "Batch 1030 / 3125 - Loss:  1.0499 \n",
            "Batch 1040 / 3125 - Loss:  0.9628 \n",
            "Batch 1050 / 3125 - Loss:  1.1642 \n",
            "Batch 1060 / 3125 - Loss:  0.9030 \n",
            "Batch 1070 / 3125 - Loss:  0.9894 \n",
            "Batch 1080 / 3125 - Loss:  1.0400 \n",
            "Batch 1090 / 3125 - Loss:  1.0338 \n",
            "Batch 1100 / 3125 - Loss:  0.9190 \n",
            "Batch 1110 / 3125 - Loss:  0.9984 \n",
            "Batch 1120 / 3125 - Loss:  1.0664 \n",
            "Batch 1130 / 3125 - Loss:  1.0221 \n",
            "Batch 1140 / 3125 - Loss:  0.9746 \n",
            "Batch 1150 / 3125 - Loss:  1.1272 \n",
            "Batch 1160 / 3125 - Loss:  1.1323 \n",
            "Batch 1170 / 3125 - Loss:  1.0853 \n",
            "Batch 1180 / 3125 - Loss:  1.0291 \n",
            "Batch 1190 / 3125 - Loss:  1.0410 \n",
            "Batch 1200 / 3125 - Loss:  1.0053 \n",
            "Batch 1210 / 3125 - Loss:  1.0108 \n",
            "Batch 1220 / 3125 - Loss:  1.1207 \n",
            "Batch 1230 / 3125 - Loss:  1.1386 \n",
            "Batch 1240 / 3125 - Loss:  0.9396 \n",
            "Batch 1250 / 3125 - Loss:  0.9877 \n",
            "Batch 1260 / 3125 - Loss:  0.8633 \n",
            "Batch 1270 / 3125 - Loss:  1.0231 \n",
            "Batch 1280 / 3125 - Loss:  0.9518 \n",
            "Batch 1290 / 3125 - Loss:  1.0973 \n",
            "Batch 1300 / 3125 - Loss:  1.0580 \n",
            "Batch 1310 / 3125 - Loss:  1.0846 \n",
            "Batch 1320 / 3125 - Loss:  0.9711 \n",
            "Batch 1330 / 3125 - Loss:  0.9524 \n",
            "Batch 1340 / 3125 - Loss:  0.9798 \n",
            "Batch 1350 / 3125 - Loss:  1.0017 \n",
            "Batch 1360 / 3125 - Loss:  1.0291 \n",
            "Batch 1370 / 3125 - Loss:  0.9409 \n",
            "Batch 1380 / 3125 - Loss:  1.0403 \n",
            "Batch 1390 / 3125 - Loss:  1.1403 \n",
            "Batch 1400 / 3125 - Loss:  1.0185 \n",
            "Batch 1410 / 3125 - Loss:  1.1036 \n",
            "Batch 1420 / 3125 - Loss:  0.9884 \n",
            "Batch 1430 / 3125 - Loss:  1.1356 \n",
            "Batch 1440 / 3125 - Loss:  0.9892 \n",
            "Batch 1450 / 3125 - Loss:  1.1352 \n",
            "Batch 1460 / 3125 - Loss:  1.0904 \n",
            "Batch 1470 / 3125 - Loss:  1.0670 \n",
            "Batch 1480 / 3125 - Loss:  0.9422 \n",
            "Batch 1490 / 3125 - Loss:  0.9211 \n",
            "Batch 1500 / 3125 - Loss:  0.9654 \n",
            "Batch 1510 / 3125 - Loss:  1.1007 \n",
            "Batch 1520 / 3125 - Loss:  0.9415 \n",
            "Batch 1530 / 3125 - Loss:  1.0527 \n",
            "Batch 1540 / 3125 - Loss:  1.0033 \n",
            "Batch 1550 / 3125 - Loss:  0.9698 \n",
            "Batch 1560 / 3125 - Loss:  0.9325 \n",
            "Batch 1570 / 3125 - Loss:  1.1099 \n",
            "Batch 1580 / 3125 - Loss:  1.0036 \n",
            "Batch 1590 / 3125 - Loss:  0.9337 \n",
            "Batch 1600 / 3125 - Loss:  1.0716 \n",
            "Batch 1610 / 3125 - Loss:  1.0917 \n",
            "Batch 1620 / 3125 - Loss:  1.0232 \n",
            "Batch 1630 / 3125 - Loss:  0.9859 \n",
            "Batch 1640 / 3125 - Loss:  0.9854 \n",
            "Batch 1650 / 3125 - Loss:  1.0771 \n",
            "Batch 1660 / 3125 - Loss:  0.9050 \n",
            "Batch 1670 / 3125 - Loss:  0.9710 \n",
            "Batch 1680 / 3125 - Loss:  0.9940 \n",
            "Batch 1690 / 3125 - Loss:  0.9113 \n",
            "Batch 1700 / 3125 - Loss:  1.0742 \n",
            "Batch 1710 / 3125 - Loss:  0.9642 \n",
            "Batch 1720 / 3125 - Loss:  0.9785 \n",
            "Batch 1730 / 3125 - Loss:  1.0779 \n",
            "Batch 1740 / 3125 - Loss:  0.9692 \n",
            "Batch 1750 / 3125 - Loss:  1.0608 \n",
            "Batch 1760 / 3125 - Loss:  0.9351 \n",
            "Batch 1770 / 3125 - Loss:  1.0962 \n",
            "Batch 1780 / 3125 - Loss:  0.9960 \n",
            "Batch 1790 / 3125 - Loss:  1.0358 \n",
            "Batch 1800 / 3125 - Loss:  0.9266 \n",
            "Batch 1810 / 3125 - Loss:  0.9954 \n",
            "Batch 1820 / 3125 - Loss:  0.9566 \n",
            "Batch 1830 / 3125 - Loss:  1.0739 \n",
            "Batch 1840 / 3125 - Loss:  1.0034 \n",
            "Batch 1850 / 3125 - Loss:  0.9837 \n",
            "Batch 1860 / 3125 - Loss:  1.0030 \n",
            "Batch 1870 / 3125 - Loss:  1.0004 \n",
            "Batch 1880 / 3125 - Loss:  1.0335 \n",
            "Batch 1890 / 3125 - Loss:  1.0216 \n",
            "Batch 1900 / 3125 - Loss:  1.0477 \n",
            "Batch 1910 / 3125 - Loss:  1.0788 \n",
            "Batch 1920 / 3125 - Loss:  0.9608 \n",
            "Batch 1930 / 3125 - Loss:  1.0362 \n",
            "Batch 1940 / 3125 - Loss:  1.0172 \n",
            "Batch 1950 / 3125 - Loss:  0.9384 \n",
            "Batch 1960 / 3125 - Loss:  0.9318 \n",
            "Batch 1970 / 3125 - Loss:  1.0232 \n",
            "Batch 1980 / 3125 - Loss:  0.9465 \n",
            "Batch 1990 / 3125 - Loss:  1.1274 \n",
            "Batch 2000 / 3125 - Loss:  1.0532 \n",
            "Batch 2010 / 3125 - Loss:  1.0445 \n",
            "Batch 2020 / 3125 - Loss:  1.0060 \n",
            "Batch 2030 / 3125 - Loss:  1.0385 \n",
            "Batch 2040 / 3125 - Loss:  0.9755 \n",
            "Batch 2050 / 3125 - Loss:  0.9959 \n",
            "Batch 2060 / 3125 - Loss:  1.0530 \n",
            "Batch 2070 / 3125 - Loss:  1.0789 \n",
            "Batch 2080 / 3125 - Loss:  1.1386 \n",
            "Batch 2090 / 3125 - Loss:  0.9751 \n",
            "Batch 2100 / 3125 - Loss:  0.8852 \n",
            "Batch 2110 / 3125 - Loss:  1.0104 \n",
            "Batch 2120 / 3125 - Loss:  0.8881 \n",
            "Batch 2130 / 3125 - Loss:  1.0992 \n",
            "Batch 2140 / 3125 - Loss:  1.1255 \n",
            "Batch 2150 / 3125 - Loss:  0.9239 \n",
            "Batch 2160 / 3125 - Loss:  0.9628 \n",
            "Batch 2170 / 3125 - Loss:  1.1135 \n",
            "Batch 2180 / 3125 - Loss:  1.0902 \n",
            "Batch 2190 / 3125 - Loss:  1.0405 \n",
            "Batch 2200 / 3125 - Loss:  0.9402 \n",
            "Batch 2210 / 3125 - Loss:  0.8863 \n",
            "Batch 2220 / 3125 - Loss:  0.9596 \n",
            "Batch 2230 / 3125 - Loss:  1.0588 \n",
            "Batch 2240 / 3125 - Loss:  0.9553 \n",
            "Batch 2250 / 3125 - Loss:  0.9882 \n",
            "Batch 2260 / 3125 - Loss:  1.0175 \n",
            "Batch 2270 / 3125 - Loss:  0.8951 \n",
            "Batch 2280 / 3125 - Loss:  1.0628 \n",
            "Batch 2290 / 3125 - Loss:  1.1266 \n",
            "Batch 2300 / 3125 - Loss:  1.0092 \n",
            "Batch 2310 / 3125 - Loss:  1.0060 \n",
            "Batch 2320 / 3125 - Loss:  1.0807 \n",
            "Batch 2330 / 3125 - Loss:  1.0256 \n",
            "Batch 2340 / 3125 - Loss:  1.1174 \n",
            "Batch 2350 / 3125 - Loss:  1.0584 \n",
            "Batch 2360 / 3125 - Loss:  1.0585 \n",
            "Batch 2370 / 3125 - Loss:  0.9563 \n",
            "Batch 2380 / 3125 - Loss:  0.9458 \n",
            "Batch 2390 / 3125 - Loss:  1.0213 \n",
            "Batch 2400 / 3125 - Loss:  0.9728 \n",
            "Batch 2410 / 3125 - Loss:  0.9960 \n",
            "Batch 2420 / 3125 - Loss:  1.1284 \n",
            "Batch 2430 / 3125 - Loss:  1.0028 \n",
            "Batch 2440 / 3125 - Loss:  0.9138 \n",
            "Batch 2450 / 3125 - Loss:  1.0525 \n",
            "Batch 2460 / 3125 - Loss:  0.8662 \n",
            "Batch 2470 / 3125 - Loss:  1.0493 \n",
            "Batch 2480 / 3125 - Loss:  0.9480 \n",
            "Batch 2490 / 3125 - Loss:  0.9335 \n",
            "Batch 2500 / 3125 - Loss:  1.0640 \n",
            "Batch 2510 / 3125 - Loss:  0.9953 \n",
            "Batch 2520 / 3125 - Loss:  0.9319 \n",
            "Batch 2530 / 3125 - Loss:  1.0199 \n",
            "Batch 2540 / 3125 - Loss:  1.1116 \n",
            "Batch 2550 / 3125 - Loss:  1.0012 \n",
            "Batch 2560 / 3125 - Loss:  1.0826 \n",
            "Batch 2570 / 3125 - Loss:  0.9940 \n",
            "Batch 2580 / 3125 - Loss:  0.9975 \n",
            "Batch 2590 / 3125 - Loss:  0.9790 \n",
            "Batch 2600 / 3125 - Loss:  1.0493 \n",
            "Batch 2610 / 3125 - Loss:  0.9230 \n",
            "Batch 2620 / 3125 - Loss:  0.9508 \n",
            "Batch 2630 / 3125 - Loss:  1.1065 \n",
            "Batch 2640 / 3125 - Loss:  1.0360 \n",
            "Batch 2650 / 3125 - Loss:  0.9557 \n",
            "Batch 2660 / 3125 - Loss:  0.9631 \n",
            "Batch 2670 / 3125 - Loss:  1.0770 \n",
            "Batch 2680 / 3125 - Loss:  1.1072 \n",
            "Batch 2690 / 3125 - Loss:  0.9594 \n",
            "Batch 2700 / 3125 - Loss:  1.0523 \n",
            "Batch 2710 / 3125 - Loss:  0.9377 \n",
            "Batch 2720 / 3125 - Loss:  1.0699 \n",
            "Batch 2730 / 3125 - Loss:  1.0678 \n",
            "Batch 2740 / 3125 - Loss:  1.0142 \n",
            "Batch 2750 / 3125 - Loss:  0.9433 \n",
            "Batch 2760 / 3125 - Loss:  0.9702 \n",
            "Batch 2770 / 3125 - Loss:  0.9974 \n",
            "Batch 2780 / 3125 - Loss:  1.0133 \n",
            "Batch 2790 / 3125 - Loss:  1.0311 \n",
            "Batch 2800 / 3125 - Loss:  0.9496 \n",
            "Batch 2810 / 3125 - Loss:  1.0518 \n",
            "Batch 2820 / 3125 - Loss:  0.8729 \n",
            "Batch 2830 / 3125 - Loss:  0.9639 \n",
            "Batch 2840 / 3125 - Loss:  1.0225 \n",
            "Batch 2850 / 3125 - Loss:  1.1061 \n",
            "Batch 2860 / 3125 - Loss:  0.9200 \n",
            "Batch 2870 / 3125 - Loss:  1.0504 \n",
            "Batch 2880 / 3125 - Loss:  1.0825 \n",
            "Batch 2890 / 3125 - Loss:  1.0872 \n",
            "Batch 2900 / 3125 - Loss:  1.0139 \n",
            "Batch 2910 / 3125 - Loss:  1.0945 \n",
            "Batch 2920 / 3125 - Loss:  1.0194 \n",
            "Batch 2930 / 3125 - Loss:  1.0231 \n",
            "Batch 2940 / 3125 - Loss:  0.9469 \n",
            "Batch 2950 / 3125 - Loss:  1.0008 \n",
            "Batch 2960 / 3125 - Loss:  0.9743 \n",
            "Batch 2970 / 3125 - Loss:  1.1084 \n",
            "Batch 2980 / 3125 - Loss:  0.9939 \n",
            "Batch 2990 / 3125 - Loss:  0.9947 \n",
            "Batch 3000 / 3125 - Loss:  1.0052 \n",
            "Batch 3010 / 3125 - Loss:  1.0640 \n",
            "Batch 3020 / 3125 - Loss:  1.0510 \n",
            "Batch 3030 / 3125 - Loss:  1.0939 \n",
            "Batch 3040 / 3125 - Loss:  1.1164 \n",
            "Batch 3050 / 3125 - Loss:  0.9991 \n",
            "Batch 3060 / 3125 - Loss:  1.0463 \n",
            "Batch 3070 / 3125 - Loss:  1.1316 \n",
            "Batch 3080 / 3125 - Loss:  0.9135 \n",
            "Batch 3090 / 3125 - Loss:  0.9239 \n",
            "Batch 3100 / 3125 - Loss:  0.8728 \n",
            "Batch 3110 / 3125 - Loss:  1.1625 \n",
            "Batch 3120 / 3125 - Loss:  0.9841 \n",
            "Epoch 45 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0014 \n",
            "Batch 10 / 3125 - Loss:  0.9805 \n",
            "Batch 20 / 3125 - Loss:  0.9816 \n",
            "Batch 30 / 3125 - Loss:  0.9607 \n",
            "Batch 40 / 3125 - Loss:  1.0670 \n",
            "Batch 50 / 3125 - Loss:  1.0586 \n",
            "Batch 60 / 3125 - Loss:  1.0505 \n",
            "Batch 70 / 3125 - Loss:  0.9983 \n",
            "Batch 80 / 3125 - Loss:  0.9138 \n",
            "Batch 90 / 3125 - Loss:  1.0577 \n",
            "Batch 100 / 3125 - Loss:  1.0189 \n",
            "Batch 110 / 3125 - Loss:  1.0147 \n",
            "Batch 120 / 3125 - Loss:  0.9290 \n",
            "Batch 130 / 3125 - Loss:  1.0027 \n",
            "Batch 140 / 3125 - Loss:  0.9966 \n",
            "Batch 150 / 3125 - Loss:  0.9023 \n",
            "Batch 160 / 3125 - Loss:  1.1290 \n",
            "Batch 170 / 3125 - Loss:  0.9410 \n",
            "Batch 180 / 3125 - Loss:  1.0557 \n",
            "Batch 190 / 3125 - Loss:  0.9653 \n",
            "Batch 200 / 3125 - Loss:  0.9764 \n",
            "Batch 210 / 3125 - Loss:  1.0151 \n",
            "Batch 220 / 3125 - Loss:  1.0583 \n",
            "Batch 230 / 3125 - Loss:  0.9030 \n",
            "Batch 240 / 3125 - Loss:  0.9730 \n",
            "Batch 250 / 3125 - Loss:  0.9836 \n",
            "Batch 260 / 3125 - Loss:  1.0275 \n",
            "Batch 270 / 3125 - Loss:  1.1151 \n",
            "Batch 280 / 3125 - Loss:  1.0298 \n",
            "Batch 290 / 3125 - Loss:  0.8886 \n",
            "Batch 300 / 3125 - Loss:  1.1575 \n",
            "Batch 310 / 3125 - Loss:  1.0175 \n",
            "Batch 320 / 3125 - Loss:  1.0267 \n",
            "Batch 330 / 3125 - Loss:  1.0510 \n",
            "Batch 340 / 3125 - Loss:  1.1161 \n",
            "Batch 350 / 3125 - Loss:  1.0010 \n",
            "Batch 360 / 3125 - Loss:  0.9304 \n",
            "Batch 370 / 3125 - Loss:  1.0389 \n",
            "Batch 380 / 3125 - Loss:  1.0378 \n",
            "Batch 390 / 3125 - Loss:  1.0051 \n",
            "Batch 400 / 3125 - Loss:  0.9542 \n",
            "Batch 410 / 3125 - Loss:  1.0192 \n",
            "Batch 420 / 3125 - Loss:  0.9882 \n",
            "Batch 430 / 3125 - Loss:  0.9982 \n",
            "Batch 440 / 3125 - Loss:  1.0209 \n",
            "Batch 450 / 3125 - Loss:  0.9582 \n",
            "Batch 460 / 3125 - Loss:  0.9721 \n",
            "Batch 470 / 3125 - Loss:  1.0077 \n",
            "Batch 480 / 3125 - Loss:  1.1591 \n",
            "Batch 490 / 3125 - Loss:  1.0213 \n",
            "Batch 500 / 3125 - Loss:  1.0301 \n",
            "Batch 510 / 3125 - Loss:  0.9813 \n",
            "Batch 520 / 3125 - Loss:  0.9797 \n",
            "Batch 530 / 3125 - Loss:  0.9978 \n",
            "Batch 540 / 3125 - Loss:  0.9785 \n",
            "Batch 550 / 3125 - Loss:  0.9898 \n",
            "Batch 560 / 3125 - Loss:  1.0731 \n",
            "Batch 570 / 3125 - Loss:  1.0264 \n",
            "Batch 580 / 3125 - Loss:  0.9547 \n",
            "Batch 590 / 3125 - Loss:  1.1373 \n",
            "Batch 600 / 3125 - Loss:  0.9878 \n",
            "Batch 610 / 3125 - Loss:  0.9334 \n",
            "Batch 620 / 3125 - Loss:  1.0591 \n",
            "Batch 630 / 3125 - Loss:  1.0619 \n",
            "Batch 640 / 3125 - Loss:  1.0023 \n",
            "Batch 650 / 3125 - Loss:  1.0281 \n",
            "Batch 660 / 3125 - Loss:  1.0574 \n",
            "Batch 670 / 3125 - Loss:  1.1124 \n",
            "Batch 680 / 3125 - Loss:  1.0166 \n",
            "Batch 690 / 3125 - Loss:  0.9295 \n",
            "Batch 700 / 3125 - Loss:  0.9582 \n",
            "Batch 710 / 3125 - Loss:  1.0971 \n",
            "Batch 720 / 3125 - Loss:  1.0212 \n",
            "Batch 730 / 3125 - Loss:  1.0892 \n",
            "Batch 740 / 3125 - Loss:  0.9330 \n",
            "Batch 750 / 3125 - Loss:  1.0032 \n",
            "Batch 760 / 3125 - Loss:  0.9825 \n",
            "Batch 770 / 3125 - Loss:  1.0219 \n",
            "Batch 780 / 3125 - Loss:  0.9488 \n",
            "Batch 790 / 3125 - Loss:  1.0734 \n",
            "Batch 800 / 3125 - Loss:  1.0047 \n",
            "Batch 810 / 3125 - Loss:  1.0434 \n",
            "Batch 820 / 3125 - Loss:  1.0070 \n",
            "Batch 830 / 3125 - Loss:  1.0272 \n",
            "Batch 840 / 3125 - Loss:  0.9896 \n",
            "Batch 850 / 3125 - Loss:  1.0109 \n",
            "Batch 860 / 3125 - Loss:  1.0448 \n",
            "Batch 870 / 3125 - Loss:  1.0226 \n",
            "Batch 880 / 3125 - Loss:  1.0404 \n",
            "Batch 890 / 3125 - Loss:  1.1289 \n",
            "Batch 900 / 3125 - Loss:  0.9676 \n",
            "Batch 910 / 3125 - Loss:  0.9795 \n",
            "Batch 920 / 3125 - Loss:  1.0619 \n",
            "Batch 930 / 3125 - Loss:  1.0184 \n",
            "Batch 940 / 3125 - Loss:  1.0421 \n",
            "Batch 950 / 3125 - Loss:  0.9978 \n",
            "Batch 960 / 3125 - Loss:  1.0697 \n",
            "Batch 970 / 3125 - Loss:  1.0214 \n",
            "Batch 980 / 3125 - Loss:  0.9302 \n",
            "Batch 990 / 3125 - Loss:  1.0557 \n",
            "Batch 1000 / 3125 - Loss:  1.0606 \n",
            "Batch 1010 / 3125 - Loss:  1.0394 \n",
            "Batch 1020 / 3125 - Loss:  1.0003 \n",
            "Batch 1030 / 3125 - Loss:  1.1151 \n",
            "Batch 1040 / 3125 - Loss:  1.0013 \n",
            "Batch 1050 / 3125 - Loss:  0.9847 \n",
            "Batch 1060 / 3125 - Loss:  1.0264 \n",
            "Batch 1070 / 3125 - Loss:  1.0651 \n",
            "Batch 1080 / 3125 - Loss:  0.9833 \n",
            "Batch 1090 / 3125 - Loss:  1.0015 \n",
            "Batch 1100 / 3125 - Loss:  1.0362 \n",
            "Batch 1110 / 3125 - Loss:  0.9949 \n",
            "Batch 1120 / 3125 - Loss:  1.0261 \n",
            "Batch 1130 / 3125 - Loss:  0.9748 \n",
            "Batch 1140 / 3125 - Loss:  0.9854 \n",
            "Batch 1150 / 3125 - Loss:  1.0789 \n",
            "Batch 1160 / 3125 - Loss:  0.9655 \n",
            "Batch 1170 / 3125 - Loss:  0.9375 \n",
            "Batch 1180 / 3125 - Loss:  0.9631 \n",
            "Batch 1190 / 3125 - Loss:  1.0815 \n",
            "Batch 1200 / 3125 - Loss:  1.0231 \n",
            "Batch 1210 / 3125 - Loss:  1.2088 \n",
            "Batch 1220 / 3125 - Loss:  0.9048 \n",
            "Batch 1230 / 3125 - Loss:  1.0503 \n",
            "Batch 1240 / 3125 - Loss:  1.0381 \n",
            "Batch 1250 / 3125 - Loss:  0.9279 \n",
            "Batch 1260 / 3125 - Loss:  0.9723 \n",
            "Batch 1270 / 3125 - Loss:  1.1190 \n",
            "Batch 1280 / 3125 - Loss:  0.9781 \n",
            "Batch 1290 / 3125 - Loss:  0.9265 \n",
            "Batch 1300 / 3125 - Loss:  1.0230 \n",
            "Batch 1310 / 3125 - Loss:  0.9204 \n",
            "Batch 1320 / 3125 - Loss:  1.0611 \n",
            "Batch 1330 / 3125 - Loss:  1.0717 \n",
            "Batch 1340 / 3125 - Loss:  1.0029 \n",
            "Batch 1350 / 3125 - Loss:  0.9862 \n",
            "Batch 1360 / 3125 - Loss:  1.0956 \n",
            "Batch 1370 / 3125 - Loss:  1.1574 \n",
            "Batch 1380 / 3125 - Loss:  1.0457 \n",
            "Batch 1390 / 3125 - Loss:  1.0631 \n",
            "Batch 1400 / 3125 - Loss:  1.0618 \n",
            "Batch 1410 / 3125 - Loss:  1.1433 \n",
            "Batch 1420 / 3125 - Loss:  1.0435 \n",
            "Batch 1430 / 3125 - Loss:  0.9868 \n",
            "Batch 1440 / 3125 - Loss:  0.9104 \n",
            "Batch 1450 / 3125 - Loss:  1.0069 \n",
            "Batch 1460 / 3125 - Loss:  0.9592 \n",
            "Batch 1470 / 3125 - Loss:  1.0121 \n",
            "Batch 1480 / 3125 - Loss:  1.0473 \n",
            "Batch 1490 / 3125 - Loss:  1.0385 \n",
            "Batch 1500 / 3125 - Loss:  0.9154 \n",
            "Batch 1510 / 3125 - Loss:  1.0154 \n",
            "Batch 1520 / 3125 - Loss:  0.9000 \n",
            "Batch 1530 / 3125 - Loss:  0.9867 \n",
            "Batch 1540 / 3125 - Loss:  0.9782 \n",
            "Batch 1550 / 3125 - Loss:  1.0069 \n",
            "Batch 1560 / 3125 - Loss:  1.0227 \n",
            "Batch 1570 / 3125 - Loss:  0.9144 \n",
            "Batch 1580 / 3125 - Loss:  1.0488 \n",
            "Batch 1590 / 3125 - Loss:  1.0832 \n",
            "Batch 1600 / 3125 - Loss:  1.1172 \n",
            "Batch 1610 / 3125 - Loss:  0.9788 \n",
            "Batch 1620 / 3125 - Loss:  1.1060 \n",
            "Batch 1630 / 3125 - Loss:  0.9497 \n",
            "Batch 1640 / 3125 - Loss:  1.0981 \n",
            "Batch 1650 / 3125 - Loss:  1.0329 \n",
            "Batch 1660 / 3125 - Loss:  1.0238 \n",
            "Batch 1670 / 3125 - Loss:  0.9644 \n",
            "Batch 1680 / 3125 - Loss:  0.9979 \n",
            "Batch 1690 / 3125 - Loss:  1.0512 \n",
            "Batch 1700 / 3125 - Loss:  1.0338 \n",
            "Batch 1710 / 3125 - Loss:  1.0111 \n",
            "Batch 1720 / 3125 - Loss:  0.9694 \n",
            "Batch 1730 / 3125 - Loss:  0.9960 \n",
            "Batch 1740 / 3125 - Loss:  0.9853 \n",
            "Batch 1750 / 3125 - Loss:  0.9787 \n",
            "Batch 1760 / 3125 - Loss:  0.9043 \n",
            "Batch 1770 / 3125 - Loss:  1.1224 \n",
            "Batch 1780 / 3125 - Loss:  0.9267 \n",
            "Batch 1790 / 3125 - Loss:  1.0818 \n",
            "Batch 1800 / 3125 - Loss:  1.0542 \n",
            "Batch 1810 / 3125 - Loss:  1.1116 \n",
            "Batch 1820 / 3125 - Loss:  1.1301 \n",
            "Batch 1830 / 3125 - Loss:  0.9892 \n",
            "Batch 1840 / 3125 - Loss:  0.8816 \n",
            "Batch 1850 / 3125 - Loss:  0.9775 \n",
            "Batch 1860 / 3125 - Loss:  0.9731 \n",
            "Batch 1870 / 3125 - Loss:  1.0487 \n",
            "Batch 1880 / 3125 - Loss:  1.0242 \n",
            "Batch 1890 / 3125 - Loss:  1.0470 \n",
            "Batch 1900 / 3125 - Loss:  1.0154 \n",
            "Batch 1910 / 3125 - Loss:  0.9898 \n",
            "Batch 1920 / 3125 - Loss:  1.0619 \n",
            "Batch 1930 / 3125 - Loss:  1.0032 \n",
            "Batch 1940 / 3125 - Loss:  0.9680 \n",
            "Batch 1950 / 3125 - Loss:  0.9811 \n",
            "Batch 1960 / 3125 - Loss:  1.0035 \n",
            "Batch 1970 / 3125 - Loss:  0.9496 \n",
            "Batch 1980 / 3125 - Loss:  1.0202 \n",
            "Batch 1990 / 3125 - Loss:  1.0342 \n",
            "Batch 2000 / 3125 - Loss:  0.9981 \n",
            "Batch 2010 / 3125 - Loss:  0.9954 \n",
            "Batch 2020 / 3125 - Loss:  1.1179 \n",
            "Batch 2030 / 3125 - Loss:  1.0332 \n",
            "Batch 2040 / 3125 - Loss:  0.9846 \n",
            "Batch 2050 / 3125 - Loss:  1.1192 \n",
            "Batch 2060 / 3125 - Loss:  0.9999 \n",
            "Batch 2070 / 3125 - Loss:  1.0082 \n",
            "Batch 2080 / 3125 - Loss:  0.9623 \n",
            "Batch 2090 / 3125 - Loss:  1.0718 \n",
            "Batch 2100 / 3125 - Loss:  1.0735 \n",
            "Batch 2110 / 3125 - Loss:  0.9501 \n",
            "Batch 2120 / 3125 - Loss:  0.9384 \n",
            "Batch 2130 / 3125 - Loss:  1.0182 \n",
            "Batch 2140 / 3125 - Loss:  1.0063 \n",
            "Batch 2150 / 3125 - Loss:  1.0209 \n",
            "Batch 2160 / 3125 - Loss:  1.1357 \n",
            "Batch 2170 / 3125 - Loss:  1.0378 \n",
            "Batch 2180 / 3125 - Loss:  1.0487 \n",
            "Batch 2190 / 3125 - Loss:  1.0747 \n",
            "Batch 2200 / 3125 - Loss:  0.9656 \n",
            "Batch 2210 / 3125 - Loss:  0.9979 \n",
            "Batch 2220 / 3125 - Loss:  1.0450 \n",
            "Batch 2230 / 3125 - Loss:  1.1419 \n",
            "Batch 2240 / 3125 - Loss:  1.0244 \n",
            "Batch 2250 / 3125 - Loss:  0.9885 \n",
            "Batch 2260 / 3125 - Loss:  0.9986 \n",
            "Batch 2270 / 3125 - Loss:  1.0813 \n",
            "Batch 2280 / 3125 - Loss:  1.0834 \n",
            "Batch 2290 / 3125 - Loss:  1.0853 \n",
            "Batch 2300 / 3125 - Loss:  1.1084 \n",
            "Batch 2310 / 3125 - Loss:  0.9871 \n",
            "Batch 2320 / 3125 - Loss:  0.9318 \n",
            "Batch 2330 / 3125 - Loss:  1.0000 \n",
            "Batch 2340 / 3125 - Loss:  0.9362 \n",
            "Batch 2350 / 3125 - Loss:  0.9748 \n",
            "Batch 2360 / 3125 - Loss:  0.9905 \n",
            "Batch 2370 / 3125 - Loss:  1.0256 \n",
            "Batch 2380 / 3125 - Loss:  1.1424 \n",
            "Batch 2390 / 3125 - Loss:  1.0883 \n",
            "Batch 2400 / 3125 - Loss:  1.0274 \n",
            "Batch 2410 / 3125 - Loss:  1.0192 \n",
            "Batch 2420 / 3125 - Loss:  1.0453 \n",
            "Batch 2430 / 3125 - Loss:  1.0448 \n",
            "Batch 2440 / 3125 - Loss:  0.9984 \n",
            "Batch 2450 / 3125 - Loss:  1.0647 \n",
            "Batch 2460 / 3125 - Loss:  1.0649 \n",
            "Batch 2470 / 3125 - Loss:  1.0312 \n",
            "Batch 2480 / 3125 - Loss:  0.9987 \n",
            "Batch 2490 / 3125 - Loss:  0.9697 \n",
            "Batch 2500 / 3125 - Loss:  1.0192 \n",
            "Batch 2510 / 3125 - Loss:  1.0899 \n",
            "Batch 2520 / 3125 - Loss:  1.0560 \n",
            "Batch 2530 / 3125 - Loss:  0.9424 \n",
            "Batch 2540 / 3125 - Loss:  1.0262 \n",
            "Batch 2550 / 3125 - Loss:  1.0467 \n",
            "Batch 2560 / 3125 - Loss:  0.8912 \n",
            "Batch 2570 / 3125 - Loss:  1.0305 \n",
            "Batch 2580 / 3125 - Loss:  1.0044 \n",
            "Batch 2590 / 3125 - Loss:  1.0689 \n",
            "Batch 2600 / 3125 - Loss:  0.9951 \n",
            "Batch 2610 / 3125 - Loss:  1.0301 \n",
            "Batch 2620 / 3125 - Loss:  1.0308 \n",
            "Batch 2630 / 3125 - Loss:  1.0451 \n",
            "Batch 2640 / 3125 - Loss:  1.0900 \n",
            "Batch 2650 / 3125 - Loss:  0.9348 \n",
            "Batch 2660 / 3125 - Loss:  1.0791 \n",
            "Batch 2670 / 3125 - Loss:  1.0837 \n",
            "Batch 2680 / 3125 - Loss:  0.9888 \n",
            "Batch 2690 / 3125 - Loss:  0.9783 \n",
            "Batch 2700 / 3125 - Loss:  1.0345 \n",
            "Batch 2710 / 3125 - Loss:  1.0372 \n",
            "Batch 2720 / 3125 - Loss:  1.0461 \n",
            "Batch 2730 / 3125 - Loss:  0.8629 \n",
            "Batch 2740 / 3125 - Loss:  1.0102 \n",
            "Batch 2750 / 3125 - Loss:  1.0718 \n",
            "Batch 2760 / 3125 - Loss:  1.0894 \n",
            "Batch 2770 / 3125 - Loss:  1.0366 \n",
            "Batch 2780 / 3125 - Loss:  1.0278 \n",
            "Batch 2790 / 3125 - Loss:  1.0530 \n",
            "Batch 2800 / 3125 - Loss:  0.9510 \n",
            "Batch 2810 / 3125 - Loss:  0.8734 \n",
            "Batch 2820 / 3125 - Loss:  1.0241 \n",
            "Batch 2830 / 3125 - Loss:  0.9736 \n",
            "Batch 2840 / 3125 - Loss:  1.0103 \n",
            "Batch 2850 / 3125 - Loss:  1.0034 \n",
            "Batch 2860 / 3125 - Loss:  1.1145 \n",
            "Batch 2870 / 3125 - Loss:  1.0606 \n",
            "Batch 2880 / 3125 - Loss:  0.9664 \n",
            "Batch 2890 / 3125 - Loss:  1.0028 \n",
            "Batch 2900 / 3125 - Loss:  0.9943 \n",
            "Batch 2910 / 3125 - Loss:  0.8903 \n",
            "Batch 2920 / 3125 - Loss:  0.9952 \n",
            "Batch 2930 / 3125 - Loss:  0.9979 \n",
            "Batch 2940 / 3125 - Loss:  1.0519 \n",
            "Batch 2950 / 3125 - Loss:  0.9861 \n",
            "Batch 2960 / 3125 - Loss:  0.9614 \n",
            "Batch 2970 / 3125 - Loss:  0.9427 \n",
            "Batch 2980 / 3125 - Loss:  1.0059 \n",
            "Batch 2990 / 3125 - Loss:  1.0708 \n",
            "Batch 3000 / 3125 - Loss:  0.9984 \n",
            "Batch 3010 / 3125 - Loss:  1.0484 \n",
            "Batch 3020 / 3125 - Loss:  0.9317 \n",
            "Batch 3030 / 3125 - Loss:  1.0606 \n",
            "Batch 3040 / 3125 - Loss:  1.0598 \n",
            "Batch 3050 / 3125 - Loss:  1.0198 \n",
            "Batch 3060 / 3125 - Loss:  1.0531 \n",
            "Batch 3070 / 3125 - Loss:  1.0146 \n",
            "Batch 3080 / 3125 - Loss:  1.1333 \n",
            "Batch 3090 / 3125 - Loss:  1.0098 \n",
            "Batch 3100 / 3125 - Loss:  1.0386 \n",
            "Batch 3110 / 3125 - Loss:  1.1295 \n",
            "Batch 3120 / 3125 - Loss:  0.9854 \n",
            "Epoch 46 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0020 \n",
            "Batch 10 / 3125 - Loss:  0.9303 \n",
            "Batch 20 / 3125 - Loss:  0.9062 \n",
            "Batch 30 / 3125 - Loss:  1.0966 \n",
            "Batch 40 / 3125 - Loss:  0.9640 \n",
            "Batch 50 / 3125 - Loss:  0.9958 \n",
            "Batch 60 / 3125 - Loss:  1.0523 \n",
            "Batch 70 / 3125 - Loss:  1.0194 \n",
            "Batch 80 / 3125 - Loss:  1.0092 \n",
            "Batch 90 / 3125 - Loss:  1.0135 \n",
            "Batch 100 / 3125 - Loss:  0.9356 \n",
            "Batch 110 / 3125 - Loss:  0.9678 \n",
            "Batch 120 / 3125 - Loss:  0.9607 \n",
            "Batch 130 / 3125 - Loss:  1.0386 \n",
            "Batch 140 / 3125 - Loss:  1.1045 \n",
            "Batch 150 / 3125 - Loss:  0.8831 \n",
            "Batch 160 / 3125 - Loss:  0.9514 \n",
            "Batch 170 / 3125 - Loss:  1.0090 \n",
            "Batch 180 / 3125 - Loss:  0.9503 \n",
            "Batch 190 / 3125 - Loss:  0.9691 \n",
            "Batch 200 / 3125 - Loss:  0.9601 \n",
            "Batch 210 / 3125 - Loss:  0.9212 \n",
            "Batch 220 / 3125 - Loss:  0.9325 \n",
            "Batch 230 / 3125 - Loss:  1.0390 \n",
            "Batch 240 / 3125 - Loss:  1.0611 \n",
            "Batch 250 / 3125 - Loss:  0.9909 \n",
            "Batch 260 / 3125 - Loss:  1.0778 \n",
            "Batch 270 / 3125 - Loss:  0.9519 \n",
            "Batch 280 / 3125 - Loss:  1.1746 \n",
            "Batch 290 / 3125 - Loss:  1.1005 \n",
            "Batch 300 / 3125 - Loss:  1.0035 \n",
            "Batch 310 / 3125 - Loss:  1.0376 \n",
            "Batch 320 / 3125 - Loss:  1.0391 \n",
            "Batch 330 / 3125 - Loss:  0.9233 \n",
            "Batch 340 / 3125 - Loss:  1.0484 \n",
            "Batch 350 / 3125 - Loss:  1.0638 \n",
            "Batch 360 / 3125 - Loss:  0.9092 \n",
            "Batch 370 / 3125 - Loss:  0.9866 \n",
            "Batch 380 / 3125 - Loss:  1.0202 \n",
            "Batch 390 / 3125 - Loss:  0.9650 \n",
            "Batch 400 / 3125 - Loss:  0.8802 \n",
            "Batch 410 / 3125 - Loss:  0.9523 \n",
            "Batch 420 / 3125 - Loss:  1.0778 \n",
            "Batch 430 / 3125 - Loss:  0.9492 \n",
            "Batch 440 / 3125 - Loss:  1.0140 \n",
            "Batch 450 / 3125 - Loss:  0.9935 \n",
            "Batch 460 / 3125 - Loss:  0.8109 \n",
            "Batch 470 / 3125 - Loss:  1.1195 \n",
            "Batch 480 / 3125 - Loss:  0.9946 \n",
            "Batch 490 / 3125 - Loss:  0.9929 \n",
            "Batch 500 / 3125 - Loss:  1.0532 \n",
            "Batch 510 / 3125 - Loss:  1.2118 \n",
            "Batch 520 / 3125 - Loss:  0.9239 \n",
            "Batch 530 / 3125 - Loss:  0.9732 \n",
            "Batch 540 / 3125 - Loss:  1.1246 \n",
            "Batch 550 / 3125 - Loss:  0.9454 \n",
            "Batch 560 / 3125 - Loss:  1.0150 \n",
            "Batch 570 / 3125 - Loss:  1.0531 \n",
            "Batch 580 / 3125 - Loss:  0.9989 \n",
            "Batch 590 / 3125 - Loss:  1.0702 \n",
            "Batch 600 / 3125 - Loss:  0.9763 \n",
            "Batch 610 / 3125 - Loss:  0.9305 \n",
            "Batch 620 / 3125 - Loss:  1.1159 \n",
            "Batch 630 / 3125 - Loss:  0.9918 \n",
            "Batch 640 / 3125 - Loss:  1.0350 \n",
            "Batch 650 / 3125 - Loss:  0.9305 \n",
            "Batch 660 / 3125 - Loss:  0.9736 \n",
            "Batch 670 / 3125 - Loss:  0.9843 \n",
            "Batch 680 / 3125 - Loss:  1.0343 \n",
            "Batch 690 / 3125 - Loss:  1.0355 \n",
            "Batch 700 / 3125 - Loss:  0.9872 \n",
            "Batch 710 / 3125 - Loss:  1.0671 \n",
            "Batch 720 / 3125 - Loss:  1.0359 \n",
            "Batch 730 / 3125 - Loss:  1.0752 \n",
            "Batch 740 / 3125 - Loss:  0.9977 \n",
            "Batch 750 / 3125 - Loss:  1.0535 \n",
            "Batch 760 / 3125 - Loss:  0.9523 \n",
            "Batch 770 / 3125 - Loss:  0.9504 \n",
            "Batch 780 / 3125 - Loss:  1.0575 \n",
            "Batch 790 / 3125 - Loss:  1.1639 \n",
            "Batch 800 / 3125 - Loss:  0.9892 \n",
            "Batch 810 / 3125 - Loss:  1.1080 \n",
            "Batch 820 / 3125 - Loss:  1.0461 \n",
            "Batch 830 / 3125 - Loss:  0.9071 \n",
            "Batch 840 / 3125 - Loss:  1.0106 \n",
            "Batch 850 / 3125 - Loss:  1.0388 \n",
            "Batch 860 / 3125 - Loss:  0.9688 \n",
            "Batch 870 / 3125 - Loss:  0.9368 \n",
            "Batch 880 / 3125 - Loss:  1.0694 \n",
            "Batch 890 / 3125 - Loss:  1.1170 \n",
            "Batch 900 / 3125 - Loss:  1.0293 \n",
            "Batch 910 / 3125 - Loss:  1.0502 \n",
            "Batch 920 / 3125 - Loss:  1.0624 \n",
            "Batch 930 / 3125 - Loss:  1.1019 \n",
            "Batch 940 / 3125 - Loss:  0.9850 \n",
            "Batch 950 / 3125 - Loss:  1.0795 \n",
            "Batch 960 / 3125 - Loss:  1.0176 \n",
            "Batch 970 / 3125 - Loss:  0.9938 \n",
            "Batch 980 / 3125 - Loss:  1.0421 \n",
            "Batch 990 / 3125 - Loss:  1.1020 \n",
            "Batch 1000 / 3125 - Loss:  1.0203 \n",
            "Batch 1010 / 3125 - Loss:  1.0727 \n",
            "Batch 1020 / 3125 - Loss:  0.9225 \n",
            "Batch 1030 / 3125 - Loss:  0.9482 \n",
            "Batch 1040 / 3125 - Loss:  1.0090 \n",
            "Batch 1050 / 3125 - Loss:  1.1274 \n",
            "Batch 1060 / 3125 - Loss:  1.0153 \n",
            "Batch 1070 / 3125 - Loss:  1.0039 \n",
            "Batch 1080 / 3125 - Loss:  1.0261 \n",
            "Batch 1090 / 3125 - Loss:  0.9830 \n",
            "Batch 1100 / 3125 - Loss:  0.8718 \n",
            "Batch 1110 / 3125 - Loss:  0.9713 \n",
            "Batch 1120 / 3125 - Loss:  1.1134 \n",
            "Batch 1130 / 3125 - Loss:  0.9777 \n",
            "Batch 1140 / 3125 - Loss:  1.0315 \n",
            "Batch 1150 / 3125 - Loss:  1.0468 \n",
            "Batch 1160 / 3125 - Loss:  1.0719 \n",
            "Batch 1170 / 3125 - Loss:  1.0437 \n",
            "Batch 1180 / 3125 - Loss:  0.9833 \n",
            "Batch 1190 / 3125 - Loss:  0.9637 \n",
            "Batch 1200 / 3125 - Loss:  1.0017 \n",
            "Batch 1210 / 3125 - Loss:  1.0823 \n",
            "Batch 1220 / 3125 - Loss:  1.1352 \n",
            "Batch 1230 / 3125 - Loss:  1.0085 \n",
            "Batch 1240 / 3125 - Loss:  0.8614 \n",
            "Batch 1250 / 3125 - Loss:  0.9552 \n",
            "Batch 1260 / 3125 - Loss:  1.0531 \n",
            "Batch 1270 / 3125 - Loss:  1.0390 \n",
            "Batch 1280 / 3125 - Loss:  0.9587 \n",
            "Batch 1290 / 3125 - Loss:  1.1144 \n",
            "Batch 1300 / 3125 - Loss:  0.9755 \n",
            "Batch 1310 / 3125 - Loss:  1.0973 \n",
            "Batch 1320 / 3125 - Loss:  1.0798 \n",
            "Batch 1330 / 3125 - Loss:  1.0351 \n",
            "Batch 1340 / 3125 - Loss:  0.9460 \n",
            "Batch 1350 / 3125 - Loss:  1.0096 \n",
            "Batch 1360 / 3125 - Loss:  1.1216 \n",
            "Batch 1370 / 3125 - Loss:  1.0297 \n",
            "Batch 1380 / 3125 - Loss:  0.8583 \n",
            "Batch 1390 / 3125 - Loss:  1.1229 \n",
            "Batch 1400 / 3125 - Loss:  0.9897 \n",
            "Batch 1410 / 3125 - Loss:  1.0312 \n",
            "Batch 1420 / 3125 - Loss:  0.9428 \n",
            "Batch 1430 / 3125 - Loss:  0.9331 \n",
            "Batch 1440 / 3125 - Loss:  1.0523 \n",
            "Batch 1450 / 3125 - Loss:  1.0024 \n",
            "Batch 1460 / 3125 - Loss:  1.0107 \n",
            "Batch 1470 / 3125 - Loss:  0.9723 \n",
            "Batch 1480 / 3125 - Loss:  1.0075 \n",
            "Batch 1490 / 3125 - Loss:  1.0015 \n",
            "Batch 1500 / 3125 - Loss:  1.0388 \n",
            "Batch 1510 / 3125 - Loss:  1.0831 \n",
            "Batch 1520 / 3125 - Loss:  1.0559 \n",
            "Batch 1530 / 3125 - Loss:  0.9730 \n",
            "Batch 1540 / 3125 - Loss:  0.9856 \n",
            "Batch 1550 / 3125 - Loss:  0.9682 \n",
            "Batch 1560 / 3125 - Loss:  1.0637 \n",
            "Batch 1570 / 3125 - Loss:  1.0670 \n",
            "Batch 1580 / 3125 - Loss:  1.0846 \n",
            "Batch 1590 / 3125 - Loss:  0.9324 \n",
            "Batch 1600 / 3125 - Loss:  1.0995 \n",
            "Batch 1610 / 3125 - Loss:  0.9847 \n",
            "Batch 1620 / 3125 - Loss:  1.0180 \n",
            "Batch 1630 / 3125 - Loss:  1.0206 \n",
            "Batch 1640 / 3125 - Loss:  1.0496 \n",
            "Batch 1650 / 3125 - Loss:  0.9896 \n",
            "Batch 1660 / 3125 - Loss:  0.9429 \n",
            "Batch 1670 / 3125 - Loss:  0.9264 \n",
            "Batch 1680 / 3125 - Loss:  1.0507 \n",
            "Batch 1690 / 3125 - Loss:  1.0956 \n",
            "Batch 1700 / 3125 - Loss:  0.9512 \n",
            "Batch 1710 / 3125 - Loss:  1.0379 \n",
            "Batch 1720 / 3125 - Loss:  1.0197 \n",
            "Batch 1730 / 3125 - Loss:  1.0477 \n",
            "Batch 1740 / 3125 - Loss:  1.0717 \n",
            "Batch 1750 / 3125 - Loss:  1.0243 \n",
            "Batch 1760 / 3125 - Loss:  1.1181 \n",
            "Batch 1770 / 3125 - Loss:  1.1695 \n",
            "Batch 1780 / 3125 - Loss:  1.0178 \n",
            "Batch 1790 / 3125 - Loss:  0.9948 \n",
            "Batch 1800 / 3125 - Loss:  0.9248 \n",
            "Batch 1810 / 3125 - Loss:  1.0241 \n",
            "Batch 1820 / 3125 - Loss:  1.0467 \n",
            "Batch 1830 / 3125 - Loss:  1.0189 \n",
            "Batch 1840 / 3125 - Loss:  1.1574 \n",
            "Batch 1850 / 3125 - Loss:  0.9606 \n",
            "Batch 1860 / 3125 - Loss:  1.0099 \n",
            "Batch 1870 / 3125 - Loss:  0.9856 \n",
            "Batch 1880 / 3125 - Loss:  0.9870 \n",
            "Batch 1890 / 3125 - Loss:  1.1048 \n",
            "Batch 1900 / 3125 - Loss:  0.9758 \n",
            "Batch 1910 / 3125 - Loss:  0.9674 \n",
            "Batch 1920 / 3125 - Loss:  0.9076 \n",
            "Batch 1930 / 3125 - Loss:  1.0274 \n",
            "Batch 1940 / 3125 - Loss:  1.0308 \n",
            "Batch 1950 / 3125 - Loss:  1.0818 \n",
            "Batch 1960 / 3125 - Loss:  0.9705 \n",
            "Batch 1970 / 3125 - Loss:  1.0298 \n",
            "Batch 1980 / 3125 - Loss:  1.0054 \n",
            "Batch 1990 / 3125 - Loss:  0.9885 \n",
            "Batch 2000 / 3125 - Loss:  1.1104 \n",
            "Batch 2010 / 3125 - Loss:  1.0694 \n",
            "Batch 2020 / 3125 - Loss:  0.9493 \n",
            "Batch 2030 / 3125 - Loss:  0.9341 \n",
            "Batch 2040 / 3125 - Loss:  1.0235 \n",
            "Batch 2050 / 3125 - Loss:  1.1202 \n",
            "Batch 2060 / 3125 - Loss:  1.0206 \n",
            "Batch 2070 / 3125 - Loss:  0.9462 \n",
            "Batch 2080 / 3125 - Loss:  1.0080 \n",
            "Batch 2090 / 3125 - Loss:  1.0368 \n",
            "Batch 2100 / 3125 - Loss:  0.9898 \n",
            "Batch 2110 / 3125 - Loss:  1.0745 \n",
            "Batch 2120 / 3125 - Loss:  1.0167 \n",
            "Batch 2130 / 3125 - Loss:  0.9001 \n",
            "Batch 2140 / 3125 - Loss:  0.9284 \n",
            "Batch 2150 / 3125 - Loss:  1.1286 \n",
            "Batch 2160 / 3125 - Loss:  1.0085 \n",
            "Batch 2170 / 3125 - Loss:  1.1356 \n",
            "Batch 2180 / 3125 - Loss:  1.0257 \n",
            "Batch 2190 / 3125 - Loss:  0.9857 \n",
            "Batch 2200 / 3125 - Loss:  0.9768 \n",
            "Batch 2210 / 3125 - Loss:  0.9775 \n",
            "Batch 2220 / 3125 - Loss:  0.9493 \n",
            "Batch 2230 / 3125 - Loss:  1.0692 \n",
            "Batch 2240 / 3125 - Loss:  1.0939 \n",
            "Batch 2250 / 3125 - Loss:  1.0898 \n",
            "Batch 2260 / 3125 - Loss:  1.0596 \n",
            "Batch 2270 / 3125 - Loss:  1.0318 \n",
            "Batch 2280 / 3125 - Loss:  0.9031 \n",
            "Batch 2290 / 3125 - Loss:  0.9810 \n",
            "Batch 2300 / 3125 - Loss:  0.9854 \n",
            "Batch 2310 / 3125 - Loss:  0.9817 \n",
            "Batch 2320 / 3125 - Loss:  1.0677 \n",
            "Batch 2330 / 3125 - Loss:  1.0610 \n",
            "Batch 2340 / 3125 - Loss:  1.0658 \n",
            "Batch 2350 / 3125 - Loss:  1.0237 \n",
            "Batch 2360 / 3125 - Loss:  1.0259 \n",
            "Batch 2370 / 3125 - Loss:  0.9683 \n",
            "Batch 2380 / 3125 - Loss:  1.0934 \n",
            "Batch 2390 / 3125 - Loss:  0.9974 \n",
            "Batch 2400 / 3125 - Loss:  1.0775 \n",
            "Batch 2410 / 3125 - Loss:  0.9676 \n",
            "Batch 2420 / 3125 - Loss:  1.0074 \n",
            "Batch 2430 / 3125 - Loss:  1.0649 \n",
            "Batch 2440 / 3125 - Loss:  0.9833 \n",
            "Batch 2450 / 3125 - Loss:  1.0701 \n",
            "Batch 2460 / 3125 - Loss:  0.9820 \n",
            "Batch 2470 / 3125 - Loss:  1.1499 \n",
            "Batch 2480 / 3125 - Loss:  1.0663 \n",
            "Batch 2490 / 3125 - Loss:  1.0937 \n",
            "Batch 2500 / 3125 - Loss:  0.9159 \n",
            "Batch 2510 / 3125 - Loss:  0.9170 \n",
            "Batch 2520 / 3125 - Loss:  0.9875 \n",
            "Batch 2530 / 3125 - Loss:  1.0465 \n",
            "Batch 2540 / 3125 - Loss:  1.0421 \n",
            "Batch 2550 / 3125 - Loss:  0.9849 \n",
            "Batch 2560 / 3125 - Loss:  0.9584 \n",
            "Batch 2570 / 3125 - Loss:  1.0217 \n",
            "Batch 2580 / 3125 - Loss:  1.0998 \n",
            "Batch 2590 / 3125 - Loss:  1.0416 \n",
            "Batch 2600 / 3125 - Loss:  0.9376 \n",
            "Batch 2610 / 3125 - Loss:  1.0094 \n",
            "Batch 2620 / 3125 - Loss:  0.9072 \n",
            "Batch 2630 / 3125 - Loss:  1.0612 \n",
            "Batch 2640 / 3125 - Loss:  0.9685 \n",
            "Batch 2650 / 3125 - Loss:  0.9690 \n",
            "Batch 2660 / 3125 - Loss:  0.9244 \n",
            "Batch 2670 / 3125 - Loss:  1.0316 \n",
            "Batch 2680 / 3125 - Loss:  1.0113 \n",
            "Batch 2690 / 3125 - Loss:  1.0292 \n",
            "Batch 2700 / 3125 - Loss:  0.9952 \n",
            "Batch 2710 / 3125 - Loss:  1.0362 \n",
            "Batch 2720 / 3125 - Loss:  1.0315 \n",
            "Batch 2730 / 3125 - Loss:  0.9649 \n",
            "Batch 2740 / 3125 - Loss:  1.0604 \n",
            "Batch 2750 / 3125 - Loss:  0.9499 \n",
            "Batch 2760 / 3125 - Loss:  0.9429 \n",
            "Batch 2770 / 3125 - Loss:  0.9116 \n",
            "Batch 2780 / 3125 - Loss:  1.0128 \n",
            "Batch 2790 / 3125 - Loss:  1.0922 \n",
            "Batch 2800 / 3125 - Loss:  1.0252 \n",
            "Batch 2810 / 3125 - Loss:  0.9734 \n",
            "Batch 2820 / 3125 - Loss:  1.1625 \n",
            "Batch 2830 / 3125 - Loss:  0.9722 \n",
            "Batch 2840 / 3125 - Loss:  1.0796 \n",
            "Batch 2850 / 3125 - Loss:  0.9099 \n",
            "Batch 2860 / 3125 - Loss:  0.9416 \n",
            "Batch 2870 / 3125 - Loss:  0.9802 \n",
            "Batch 2880 / 3125 - Loss:  1.0912 \n",
            "Batch 2890 / 3125 - Loss:  0.8390 \n",
            "Batch 2900 / 3125 - Loss:  1.1164 \n",
            "Batch 2910 / 3125 - Loss:  0.9327 \n",
            "Batch 2920 / 3125 - Loss:  0.9241 \n",
            "Batch 2930 / 3125 - Loss:  1.2293 \n",
            "Batch 2940 / 3125 - Loss:  1.1188 \n",
            "Batch 2950 / 3125 - Loss:  0.9854 \n",
            "Batch 2960 / 3125 - Loss:  0.9314 \n",
            "Batch 2970 / 3125 - Loss:  0.9982 \n",
            "Batch 2980 / 3125 - Loss:  1.0250 \n",
            "Batch 2990 / 3125 - Loss:  1.0311 \n",
            "Batch 3000 / 3125 - Loss:  1.0901 \n",
            "Batch 3010 / 3125 - Loss:  0.9927 \n",
            "Batch 3020 / 3125 - Loss:  1.0377 \n",
            "Batch 3030 / 3125 - Loss:  0.9467 \n",
            "Batch 3040 / 3125 - Loss:  1.0471 \n",
            "Batch 3050 / 3125 - Loss:  1.0968 \n",
            "Batch 3060 / 3125 - Loss:  1.0726 \n",
            "Batch 3070 / 3125 - Loss:  1.0828 \n",
            "Batch 3080 / 3125 - Loss:  1.0190 \n",
            "Batch 3090 / 3125 - Loss:  0.9782 \n",
            "Batch 3100 / 3125 - Loss:  1.1363 \n",
            "Batch 3110 / 3125 - Loss:  1.0143 \n",
            "Batch 3120 / 3125 - Loss:  1.0356 \n",
            "Epoch 47 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0194 \n",
            "Batch 10 / 3125 - Loss:  1.0479 \n",
            "Batch 20 / 3125 - Loss:  0.9897 \n",
            "Batch 30 / 3125 - Loss:  0.9903 \n",
            "Batch 40 / 3125 - Loss:  1.0489 \n",
            "Batch 50 / 3125 - Loss:  1.0154 \n",
            "Batch 60 / 3125 - Loss:  0.8640 \n",
            "Batch 70 / 3125 - Loss:  1.1221 \n",
            "Batch 80 / 3125 - Loss:  0.9961 \n",
            "Batch 90 / 3125 - Loss:  0.9959 \n",
            "Batch 100 / 3125 - Loss:  1.1490 \n",
            "Batch 110 / 3125 - Loss:  1.0167 \n",
            "Batch 120 / 3125 - Loss:  0.9479 \n",
            "Batch 130 / 3125 - Loss:  1.0372 \n",
            "Batch 140 / 3125 - Loss:  1.0213 \n",
            "Batch 150 / 3125 - Loss:  1.0909 \n",
            "Batch 160 / 3125 - Loss:  1.0103 \n",
            "Batch 170 / 3125 - Loss:  0.9417 \n",
            "Batch 180 / 3125 - Loss:  1.0258 \n",
            "Batch 190 / 3125 - Loss:  1.0644 \n",
            "Batch 200 / 3125 - Loss:  0.9764 \n",
            "Batch 210 / 3125 - Loss:  1.1676 \n",
            "Batch 220 / 3125 - Loss:  1.0169 \n",
            "Batch 230 / 3125 - Loss:  1.0145 \n",
            "Batch 240 / 3125 - Loss:  1.0360 \n",
            "Batch 250 / 3125 - Loss:  0.9936 \n",
            "Batch 260 / 3125 - Loss:  0.9388 \n",
            "Batch 270 / 3125 - Loss:  0.9121 \n",
            "Batch 280 / 3125 - Loss:  0.9984 \n",
            "Batch 290 / 3125 - Loss:  0.9515 \n",
            "Batch 300 / 3125 - Loss:  1.0262 \n",
            "Batch 310 / 3125 - Loss:  1.0386 \n",
            "Batch 320 / 3125 - Loss:  0.8627 \n",
            "Batch 330 / 3125 - Loss:  1.0351 \n",
            "Batch 340 / 3125 - Loss:  1.0485 \n",
            "Batch 350 / 3125 - Loss:  1.1131 \n",
            "Batch 360 / 3125 - Loss:  0.9783 \n",
            "Batch 370 / 3125 - Loss:  0.8926 \n",
            "Batch 380 / 3125 - Loss:  1.0246 \n",
            "Batch 390 / 3125 - Loss:  0.9770 \n",
            "Batch 400 / 3125 - Loss:  0.9665 \n",
            "Batch 410 / 3125 - Loss:  0.9783 \n",
            "Batch 420 / 3125 - Loss:  1.0726 \n",
            "Batch 430 / 3125 - Loss:  0.9488 \n",
            "Batch 440 / 3125 - Loss:  1.0455 \n",
            "Batch 450 / 3125 - Loss:  1.0732 \n",
            "Batch 460 / 3125 - Loss:  1.0122 \n",
            "Batch 470 / 3125 - Loss:  0.9746 \n",
            "Batch 480 / 3125 - Loss:  1.0299 \n",
            "Batch 490 / 3125 - Loss:  1.0431 \n",
            "Batch 500 / 3125 - Loss:  1.0870 \n",
            "Batch 510 / 3125 - Loss:  1.1085 \n",
            "Batch 520 / 3125 - Loss:  1.0032 \n",
            "Batch 530 / 3125 - Loss:  1.0153 \n",
            "Batch 540 / 3125 - Loss:  1.0424 \n",
            "Batch 550 / 3125 - Loss:  1.1392 \n",
            "Batch 560 / 3125 - Loss:  0.9703 \n",
            "Batch 570 / 3125 - Loss:  1.0744 \n",
            "Batch 580 / 3125 - Loss:  1.0867 \n",
            "Batch 590 / 3125 - Loss:  0.9441 \n",
            "Batch 600 / 3125 - Loss:  1.1482 \n",
            "Batch 610 / 3125 - Loss:  1.1072 \n",
            "Batch 620 / 3125 - Loss:  1.0877 \n",
            "Batch 630 / 3125 - Loss:  0.9767 \n",
            "Batch 640 / 3125 - Loss:  0.9891 \n",
            "Batch 650 / 3125 - Loss:  0.8977 \n",
            "Batch 660 / 3125 - Loss:  1.1403 \n",
            "Batch 670 / 3125 - Loss:  1.0359 \n",
            "Batch 680 / 3125 - Loss:  1.0009 \n",
            "Batch 690 / 3125 - Loss:  1.0155 \n",
            "Batch 700 / 3125 - Loss:  1.0060 \n",
            "Batch 710 / 3125 - Loss:  1.0420 \n",
            "Batch 720 / 3125 - Loss:  1.0440 \n",
            "Batch 730 / 3125 - Loss:  1.0350 \n",
            "Batch 740 / 3125 - Loss:  1.0434 \n",
            "Batch 750 / 3125 - Loss:  0.9297 \n",
            "Batch 760 / 3125 - Loss:  1.0613 \n",
            "Batch 770 / 3125 - Loss:  0.9389 \n",
            "Batch 780 / 3125 - Loss:  1.0079 \n",
            "Batch 790 / 3125 - Loss:  1.0351 \n",
            "Batch 800 / 3125 - Loss:  0.9288 \n",
            "Batch 810 / 3125 - Loss:  1.0777 \n",
            "Batch 820 / 3125 - Loss:  0.9736 \n",
            "Batch 830 / 3125 - Loss:  1.0610 \n",
            "Batch 840 / 3125 - Loss:  0.9389 \n",
            "Batch 850 / 3125 - Loss:  0.9963 \n",
            "Batch 860 / 3125 - Loss:  1.1368 \n",
            "Batch 870 / 3125 - Loss:  1.0479 \n",
            "Batch 880 / 3125 - Loss:  1.1245 \n",
            "Batch 890 / 3125 - Loss:  1.0190 \n",
            "Batch 900 / 3125 - Loss:  0.9676 \n",
            "Batch 910 / 3125 - Loss:  1.0704 \n",
            "Batch 920 / 3125 - Loss:  0.9957 \n",
            "Batch 930 / 3125 - Loss:  1.0726 \n",
            "Batch 940 / 3125 - Loss:  1.0807 \n",
            "Batch 950 / 3125 - Loss:  1.0713 \n",
            "Batch 960 / 3125 - Loss:  1.0665 \n",
            "Batch 970 / 3125 - Loss:  1.0396 \n",
            "Batch 980 / 3125 - Loss:  0.9534 \n",
            "Batch 990 / 3125 - Loss:  1.0502 \n",
            "Batch 1000 / 3125 - Loss:  1.0522 \n",
            "Batch 1010 / 3125 - Loss:  0.9822 \n",
            "Batch 1020 / 3125 - Loss:  1.0098 \n",
            "Batch 1030 / 3125 - Loss:  0.9405 \n",
            "Batch 1040 / 3125 - Loss:  0.9418 \n",
            "Batch 1050 / 3125 - Loss:  1.0146 \n",
            "Batch 1060 / 3125 - Loss:  1.0661 \n",
            "Batch 1070 / 3125 - Loss:  1.0330 \n",
            "Batch 1080 / 3125 - Loss:  1.0273 \n",
            "Batch 1090 / 3125 - Loss:  1.1174 \n",
            "Batch 1100 / 3125 - Loss:  1.0419 \n",
            "Batch 1110 / 3125 - Loss:  0.9029 \n",
            "Batch 1120 / 3125 - Loss:  1.0092 \n",
            "Batch 1130 / 3125 - Loss:  0.9018 \n",
            "Batch 1140 / 3125 - Loss:  0.9422 \n",
            "Batch 1150 / 3125 - Loss:  0.9932 \n",
            "Batch 1160 / 3125 - Loss:  1.0945 \n",
            "Batch 1170 / 3125 - Loss:  0.9821 \n",
            "Batch 1180 / 3125 - Loss:  0.9339 \n",
            "Batch 1190 / 3125 - Loss:  1.0198 \n",
            "Batch 1200 / 3125 - Loss:  1.0182 \n",
            "Batch 1210 / 3125 - Loss:  0.9893 \n",
            "Batch 1220 / 3125 - Loss:  1.0566 \n",
            "Batch 1230 / 3125 - Loss:  1.0602 \n",
            "Batch 1240 / 3125 - Loss:  1.0005 \n",
            "Batch 1250 / 3125 - Loss:  0.9656 \n",
            "Batch 1260 / 3125 - Loss:  1.0104 \n",
            "Batch 1270 / 3125 - Loss:  1.0423 \n",
            "Batch 1280 / 3125 - Loss:  0.9314 \n",
            "Batch 1290 / 3125 - Loss:  1.0856 \n",
            "Batch 1300 / 3125 - Loss:  1.0214 \n",
            "Batch 1310 / 3125 - Loss:  1.0544 \n",
            "Batch 1320 / 3125 - Loss:  1.0883 \n",
            "Batch 1330 / 3125 - Loss:  0.9284 \n",
            "Batch 1340 / 3125 - Loss:  0.9811 \n",
            "Batch 1350 / 3125 - Loss:  1.0138 \n",
            "Batch 1360 / 3125 - Loss:  0.9607 \n",
            "Batch 1370 / 3125 - Loss:  1.0379 \n",
            "Batch 1380 / 3125 - Loss:  1.0121 \n",
            "Batch 1390 / 3125 - Loss:  1.0388 \n",
            "Batch 1400 / 3125 - Loss:  1.0598 \n",
            "Batch 1410 / 3125 - Loss:  1.0675 \n",
            "Batch 1420 / 3125 - Loss:  1.0489 \n",
            "Batch 1430 / 3125 - Loss:  1.0078 \n",
            "Batch 1440 / 3125 - Loss:  0.9808 \n",
            "Batch 1450 / 3125 - Loss:  1.0161 \n",
            "Batch 1460 / 3125 - Loss:  0.9723 \n",
            "Batch 1470 / 3125 - Loss:  1.0921 \n",
            "Batch 1480 / 3125 - Loss:  0.9563 \n",
            "Batch 1490 / 3125 - Loss:  0.9800 \n",
            "Batch 1500 / 3125 - Loss:  1.0494 \n",
            "Batch 1510 / 3125 - Loss:  1.0538 \n",
            "Batch 1520 / 3125 - Loss:  1.0366 \n",
            "Batch 1530 / 3125 - Loss:  1.0089 \n",
            "Batch 1540 / 3125 - Loss:  0.9579 \n",
            "Batch 1550 / 3125 - Loss:  0.9929 \n",
            "Batch 1560 / 3125 - Loss:  1.0347 \n",
            "Batch 1570 / 3125 - Loss:  1.0476 \n",
            "Batch 1580 / 3125 - Loss:  0.9870 \n",
            "Batch 1590 / 3125 - Loss:  1.0311 \n",
            "Batch 1600 / 3125 - Loss:  0.9576 \n",
            "Batch 1610 / 3125 - Loss:  1.0354 \n",
            "Batch 1620 / 3125 - Loss:  0.9127 \n",
            "Batch 1630 / 3125 - Loss:  0.9652 \n",
            "Batch 1640 / 3125 - Loss:  0.9699 \n",
            "Batch 1650 / 3125 - Loss:  0.9942 \n",
            "Batch 1660 / 3125 - Loss:  0.9392 \n",
            "Batch 1670 / 3125 - Loss:  1.0794 \n",
            "Batch 1680 / 3125 - Loss:  0.9792 \n",
            "Batch 1690 / 3125 - Loss:  0.9482 \n",
            "Batch 1700 / 3125 - Loss:  0.9697 \n",
            "Batch 1710 / 3125 - Loss:  1.1145 \n",
            "Batch 1720 / 3125 - Loss:  1.0237 \n",
            "Batch 1730 / 3125 - Loss:  0.9932 \n",
            "Batch 1740 / 3125 - Loss:  0.9788 \n",
            "Batch 1750 / 3125 - Loss:  0.8323 \n",
            "Batch 1760 / 3125 - Loss:  1.0019 \n",
            "Batch 1770 / 3125 - Loss:  1.0693 \n",
            "Batch 1780 / 3125 - Loss:  1.0483 \n",
            "Batch 1790 / 3125 - Loss:  0.8836 \n",
            "Batch 1800 / 3125 - Loss:  0.9097 \n",
            "Batch 1810 / 3125 - Loss:  0.9652 \n",
            "Batch 1820 / 3125 - Loss:  0.9733 \n",
            "Batch 1830 / 3125 - Loss:  1.0508 \n",
            "Batch 1840 / 3125 - Loss:  0.9462 \n",
            "Batch 1850 / 3125 - Loss:  1.0027 \n",
            "Batch 1860 / 3125 - Loss:  1.0325 \n",
            "Batch 1870 / 3125 - Loss:  1.0001 \n",
            "Batch 1880 / 3125 - Loss:  0.9199 \n",
            "Batch 1890 / 3125 - Loss:  0.9937 \n",
            "Batch 1900 / 3125 - Loss:  1.1301 \n",
            "Batch 1910 / 3125 - Loss:  0.9672 \n",
            "Batch 1920 / 3125 - Loss:  1.0255 \n",
            "Batch 1930 / 3125 - Loss:  0.9756 \n",
            "Batch 1940 / 3125 - Loss:  0.9979 \n",
            "Batch 1950 / 3125 - Loss:  0.9431 \n",
            "Batch 1960 / 3125 - Loss:  1.0044 \n",
            "Batch 1970 / 3125 - Loss:  0.9212 \n",
            "Batch 1980 / 3125 - Loss:  1.1446 \n",
            "Batch 1990 / 3125 - Loss:  1.1927 \n",
            "Batch 2000 / 3125 - Loss:  1.1560 \n",
            "Batch 2010 / 3125 - Loss:  1.0963 \n",
            "Batch 2020 / 3125 - Loss:  1.0082 \n",
            "Batch 2030 / 3125 - Loss:  1.0597 \n",
            "Batch 2040 / 3125 - Loss:  0.9119 \n",
            "Batch 2050 / 3125 - Loss:  1.0997 \n",
            "Batch 2060 / 3125 - Loss:  1.0060 \n",
            "Batch 2070 / 3125 - Loss:  1.1597 \n",
            "Batch 2080 / 3125 - Loss:  0.9235 \n",
            "Batch 2090 / 3125 - Loss:  0.9814 \n",
            "Batch 2100 / 3125 - Loss:  1.1047 \n",
            "Batch 2110 / 3125 - Loss:  0.9140 \n",
            "Batch 2120 / 3125 - Loss:  1.0476 \n",
            "Batch 2130 / 3125 - Loss:  0.9951 \n",
            "Batch 2140 / 3125 - Loss:  0.9538 \n",
            "Batch 2150 / 3125 - Loss:  0.9758 \n",
            "Batch 2160 / 3125 - Loss:  0.9838 \n",
            "Batch 2170 / 3125 - Loss:  0.9811 \n",
            "Batch 2180 / 3125 - Loss:  1.0566 \n",
            "Batch 2190 / 3125 - Loss:  1.0056 \n",
            "Batch 2200 / 3125 - Loss:  1.0266 \n",
            "Batch 2210 / 3125 - Loss:  1.0719 \n",
            "Batch 2220 / 3125 - Loss:  1.0457 \n",
            "Batch 2230 / 3125 - Loss:  1.0089 \n",
            "Batch 2240 / 3125 - Loss:  1.1691 \n",
            "Batch 2250 / 3125 - Loss:  0.9855 \n",
            "Batch 2260 / 3125 - Loss:  1.0206 \n",
            "Batch 2270 / 3125 - Loss:  0.9953 \n",
            "Batch 2280 / 3125 - Loss:  0.9642 \n",
            "Batch 2290 / 3125 - Loss:  0.9769 \n",
            "Batch 2300 / 3125 - Loss:  1.0379 \n",
            "Batch 2310 / 3125 - Loss:  1.1163 \n",
            "Batch 2320 / 3125 - Loss:  1.0303 \n",
            "Batch 2330 / 3125 - Loss:  0.9630 \n",
            "Batch 2340 / 3125 - Loss:  0.9462 \n",
            "Batch 2350 / 3125 - Loss:  1.0704 \n",
            "Batch 2360 / 3125 - Loss:  0.9213 \n",
            "Batch 2370 / 3125 - Loss:  0.9641 \n",
            "Batch 2380 / 3125 - Loss:  1.1421 \n",
            "Batch 2390 / 3125 - Loss:  0.9070 \n",
            "Batch 2400 / 3125 - Loss:  0.9705 \n",
            "Batch 2410 / 3125 - Loss:  0.9332 \n",
            "Batch 2420 / 3125 - Loss:  1.0725 \n",
            "Batch 2430 / 3125 - Loss:  1.0481 \n",
            "Batch 2440 / 3125 - Loss:  1.2127 \n",
            "Batch 2450 / 3125 - Loss:  1.0344 \n",
            "Batch 2460 / 3125 - Loss:  0.9294 \n",
            "Batch 2470 / 3125 - Loss:  0.9941 \n",
            "Batch 2480 / 3125 - Loss:  0.8965 \n",
            "Batch 2490 / 3125 - Loss:  1.1220 \n",
            "Batch 2500 / 3125 - Loss:  1.0736 \n",
            "Batch 2510 / 3125 - Loss:  0.9556 \n",
            "Batch 2520 / 3125 - Loss:  0.9660 \n",
            "Batch 2530 / 3125 - Loss:  0.8965 \n",
            "Batch 2540 / 3125 - Loss:  1.1042 \n",
            "Batch 2550 / 3125 - Loss:  0.9985 \n",
            "Batch 2560 / 3125 - Loss:  0.9725 \n",
            "Batch 2570 / 3125 - Loss:  0.9634 \n",
            "Batch 2580 / 3125 - Loss:  1.0792 \n",
            "Batch 2590 / 3125 - Loss:  0.9391 \n",
            "Batch 2600 / 3125 - Loss:  0.9486 \n",
            "Batch 2610 / 3125 - Loss:  1.0752 \n",
            "Batch 2620 / 3125 - Loss:  1.0079 \n",
            "Batch 2630 / 3125 - Loss:  1.0152 \n",
            "Batch 2640 / 3125 - Loss:  0.9203 \n",
            "Batch 2650 / 3125 - Loss:  0.9636 \n",
            "Batch 2660 / 3125 - Loss:  1.0580 \n",
            "Batch 2670 / 3125 - Loss:  1.0778 \n",
            "Batch 2680 / 3125 - Loss:  1.0584 \n",
            "Batch 2690 / 3125 - Loss:  1.0387 \n",
            "Batch 2700 / 3125 - Loss:  0.9538 \n",
            "Batch 2710 / 3125 - Loss:  1.0169 \n",
            "Batch 2720 / 3125 - Loss:  1.0905 \n",
            "Batch 2730 / 3125 - Loss:  0.9445 \n",
            "Batch 2740 / 3125 - Loss:  1.0504 \n",
            "Batch 2750 / 3125 - Loss:  1.0382 \n",
            "Batch 2760 / 3125 - Loss:  1.0430 \n",
            "Batch 2770 / 3125 - Loss:  0.9933 \n",
            "Batch 2780 / 3125 - Loss:  0.9776 \n",
            "Batch 2790 / 3125 - Loss:  0.9093 \n",
            "Batch 2800 / 3125 - Loss:  1.1003 \n",
            "Batch 2810 / 3125 - Loss:  0.9446 \n",
            "Batch 2820 / 3125 - Loss:  1.0250 \n",
            "Batch 2830 / 3125 - Loss:  0.9431 \n",
            "Batch 2840 / 3125 - Loss:  1.0665 \n",
            "Batch 2850 / 3125 - Loss:  1.0598 \n",
            "Batch 2860 / 3125 - Loss:  1.1079 \n",
            "Batch 2870 / 3125 - Loss:  1.0207 \n",
            "Batch 2880 / 3125 - Loss:  1.0012 \n",
            "Batch 2890 / 3125 - Loss:  1.0124 \n",
            "Batch 2900 / 3125 - Loss:  1.1226 \n",
            "Batch 2910 / 3125 - Loss:  0.9370 \n",
            "Batch 2920 / 3125 - Loss:  1.0325 \n",
            "Batch 2930 / 3125 - Loss:  1.0773 \n",
            "Batch 2940 / 3125 - Loss:  1.0176 \n",
            "Batch 2950 / 3125 - Loss:  0.9144 \n",
            "Batch 2960 / 3125 - Loss:  1.0577 \n",
            "Batch 2970 / 3125 - Loss:  0.9440 \n",
            "Batch 2980 / 3125 - Loss:  1.0389 \n",
            "Batch 2990 / 3125 - Loss:  1.0395 \n",
            "Batch 3000 / 3125 - Loss:  0.9949 \n",
            "Batch 3010 / 3125 - Loss:  0.9663 \n",
            "Batch 3020 / 3125 - Loss:  0.9293 \n",
            "Batch 3030 / 3125 - Loss:  0.9908 \n",
            "Batch 3040 / 3125 - Loss:  1.1016 \n",
            "Batch 3050 / 3125 - Loss:  1.0423 \n",
            "Batch 3060 / 3125 - Loss:  1.0265 \n",
            "Batch 3070 / 3125 - Loss:  1.0107 \n",
            "Batch 3080 / 3125 - Loss:  1.0480 \n",
            "Batch 3090 / 3125 - Loss:  0.9846 \n",
            "Batch 3100 / 3125 - Loss:  0.9966 \n",
            "Batch 3110 / 3125 - Loss:  0.9517 \n",
            "Batch 3120 / 3125 - Loss:  1.0241 \n",
            "Epoch 48 / 50 \n",
            "Batch 0 / 3125 - Loss:  0.9718 \n",
            "Batch 10 / 3125 - Loss:  1.0708 \n",
            "Batch 20 / 3125 - Loss:  1.0843 \n",
            "Batch 30 / 3125 - Loss:  0.9429 \n",
            "Batch 40 / 3125 - Loss:  0.9434 \n",
            "Batch 50 / 3125 - Loss:  1.0410 \n",
            "Batch 60 / 3125 - Loss:  1.0058 \n",
            "Batch 70 / 3125 - Loss:  1.0162 \n",
            "Batch 80 / 3125 - Loss:  1.0050 \n",
            "Batch 90 / 3125 - Loss:  1.0368 \n",
            "Batch 100 / 3125 - Loss:  1.0083 \n",
            "Batch 110 / 3125 - Loss:  0.9578 \n",
            "Batch 120 / 3125 - Loss:  1.0630 \n",
            "Batch 130 / 3125 - Loss:  1.0187 \n",
            "Batch 140 / 3125 - Loss:  1.0311 \n",
            "Batch 150 / 3125 - Loss:  1.0026 \n",
            "Batch 160 / 3125 - Loss:  0.9347 \n",
            "Batch 170 / 3125 - Loss:  1.0478 \n",
            "Batch 180 / 3125 - Loss:  0.9997 \n",
            "Batch 190 / 3125 - Loss:  0.9972 \n",
            "Batch 200 / 3125 - Loss:  1.0750 \n",
            "Batch 210 / 3125 - Loss:  1.0587 \n",
            "Batch 220 / 3125 - Loss:  1.0306 \n",
            "Batch 230 / 3125 - Loss:  1.0501 \n",
            "Batch 240 / 3125 - Loss:  1.0134 \n",
            "Batch 250 / 3125 - Loss:  0.9494 \n",
            "Batch 260 / 3125 - Loss:  1.0182 \n",
            "Batch 270 / 3125 - Loss:  1.0225 \n",
            "Batch 280 / 3125 - Loss:  1.0720 \n",
            "Batch 290 / 3125 - Loss:  1.0406 \n",
            "Batch 300 / 3125 - Loss:  1.0540 \n",
            "Batch 310 / 3125 - Loss:  1.0990 \n",
            "Batch 320 / 3125 - Loss:  0.9687 \n",
            "Batch 330 / 3125 - Loss:  1.0155 \n",
            "Batch 340 / 3125 - Loss:  1.1164 \n",
            "Batch 350 / 3125 - Loss:  0.9284 \n",
            "Batch 360 / 3125 - Loss:  1.0311 \n",
            "Batch 370 / 3125 - Loss:  1.1053 \n",
            "Batch 380 / 3125 - Loss:  1.0520 \n",
            "Batch 390 / 3125 - Loss:  1.0104 \n",
            "Batch 400 / 3125 - Loss:  1.0221 \n",
            "Batch 410 / 3125 - Loss:  0.9291 \n",
            "Batch 420 / 3125 - Loss:  1.0166 \n",
            "Batch 430 / 3125 - Loss:  0.8549 \n",
            "Batch 440 / 3125 - Loss:  0.9616 \n",
            "Batch 450 / 3125 - Loss:  0.9479 \n",
            "Batch 460 / 3125 - Loss:  0.9955 \n",
            "Batch 470 / 3125 - Loss:  1.0033 \n",
            "Batch 480 / 3125 - Loss:  1.0347 \n",
            "Batch 490 / 3125 - Loss:  1.0472 \n",
            "Batch 500 / 3125 - Loss:  0.9601 \n",
            "Batch 510 / 3125 - Loss:  0.9935 \n",
            "Batch 520 / 3125 - Loss:  1.0587 \n",
            "Batch 530 / 3125 - Loss:  1.0426 \n",
            "Batch 540 / 3125 - Loss:  1.0251 \n",
            "Batch 550 / 3125 - Loss:  1.0391 \n",
            "Batch 560 / 3125 - Loss:  0.9912 \n",
            "Batch 570 / 3125 - Loss:  0.9057 \n",
            "Batch 580 / 3125 - Loss:  0.9110 \n",
            "Batch 590 / 3125 - Loss:  1.0654 \n",
            "Batch 600 / 3125 - Loss:  1.0301 \n",
            "Batch 610 / 3125 - Loss:  1.0098 \n",
            "Batch 620 / 3125 - Loss:  0.9668 \n",
            "Batch 630 / 3125 - Loss:  1.1027 \n",
            "Batch 640 / 3125 - Loss:  0.9901 \n",
            "Batch 650 / 3125 - Loss:  1.0443 \n",
            "Batch 660 / 3125 - Loss:  1.0188 \n",
            "Batch 670 / 3125 - Loss:  0.8463 \n",
            "Batch 680 / 3125 - Loss:  1.0793 \n",
            "Batch 690 / 3125 - Loss:  0.9808 \n",
            "Batch 700 / 3125 - Loss:  1.0385 \n",
            "Batch 710 / 3125 - Loss:  1.0625 \n",
            "Batch 720 / 3125 - Loss:  0.9925 \n",
            "Batch 730 / 3125 - Loss:  1.0769 \n",
            "Batch 740 / 3125 - Loss:  0.9908 \n",
            "Batch 750 / 3125 - Loss:  1.0678 \n",
            "Batch 760 / 3125 - Loss:  1.0460 \n",
            "Batch 770 / 3125 - Loss:  1.1054 \n",
            "Batch 780 / 3125 - Loss:  1.0792 \n",
            "Batch 790 / 3125 - Loss:  1.0094 \n",
            "Batch 800 / 3125 - Loss:  1.0441 \n",
            "Batch 810 / 3125 - Loss:  0.9461 \n",
            "Batch 820 / 3125 - Loss:  0.9898 \n",
            "Batch 830 / 3125 - Loss:  1.1291 \n",
            "Batch 840 / 3125 - Loss:  1.0116 \n",
            "Batch 850 / 3125 - Loss:  1.0478 \n",
            "Batch 860 / 3125 - Loss:  1.1072 \n",
            "Batch 870 / 3125 - Loss:  1.0143 \n",
            "Batch 880 / 3125 - Loss:  1.0672 \n",
            "Batch 890 / 3125 - Loss:  1.0702 \n",
            "Batch 900 / 3125 - Loss:  0.9915 \n",
            "Batch 910 / 3125 - Loss:  1.0214 \n",
            "Batch 920 / 3125 - Loss:  0.9853 \n",
            "Batch 930 / 3125 - Loss:  0.9785 \n",
            "Batch 940 / 3125 - Loss:  1.1161 \n",
            "Batch 950 / 3125 - Loss:  0.9741 \n",
            "Batch 960 / 3125 - Loss:  1.0660 \n",
            "Batch 970 / 3125 - Loss:  1.0272 \n",
            "Batch 980 / 3125 - Loss:  0.9547 \n",
            "Batch 990 / 3125 - Loss:  1.0730 \n",
            "Batch 1000 / 3125 - Loss:  1.0397 \n",
            "Batch 1010 / 3125 - Loss:  0.9978 \n",
            "Batch 1020 / 3125 - Loss:  1.0100 \n",
            "Batch 1030 / 3125 - Loss:  1.0656 \n",
            "Batch 1040 / 3125 - Loss:  1.0514 \n",
            "Batch 1050 / 3125 - Loss:  0.9414 \n",
            "Batch 1060 / 3125 - Loss:  1.0029 \n",
            "Batch 1070 / 3125 - Loss:  0.9243 \n",
            "Batch 1080 / 3125 - Loss:  1.1289 \n",
            "Batch 1090 / 3125 - Loss:  0.9576 \n",
            "Batch 1100 / 3125 - Loss:  1.0239 \n",
            "Batch 1110 / 3125 - Loss:  0.9683 \n",
            "Batch 1120 / 3125 - Loss:  0.9783 \n",
            "Batch 1130 / 3125 - Loss:  1.0662 \n",
            "Batch 1140 / 3125 - Loss:  0.9527 \n",
            "Batch 1150 / 3125 - Loss:  1.0776 \n",
            "Batch 1160 / 3125 - Loss:  1.1098 \n",
            "Batch 1170 / 3125 - Loss:  0.9788 \n",
            "Batch 1180 / 3125 - Loss:  1.1007 \n",
            "Batch 1190 / 3125 - Loss:  0.9114 \n",
            "Batch 1200 / 3125 - Loss:  1.0315 \n",
            "Batch 1210 / 3125 - Loss:  1.0497 \n",
            "Batch 1220 / 3125 - Loss:  0.9597 \n",
            "Batch 1230 / 3125 - Loss:  0.9126 \n",
            "Batch 1240 / 3125 - Loss:  0.9536 \n",
            "Batch 1250 / 3125 - Loss:  1.0093 \n",
            "Batch 1260 / 3125 - Loss:  0.8928 \n",
            "Batch 1270 / 3125 - Loss:  0.9824 \n",
            "Batch 1280 / 3125 - Loss:  0.9611 \n",
            "Batch 1290 / 3125 - Loss:  1.0966 \n",
            "Batch 1300 / 3125 - Loss:  1.0527 \n",
            "Batch 1310 / 3125 - Loss:  1.0128 \n",
            "Batch 1320 / 3125 - Loss:  0.9999 \n",
            "Batch 1330 / 3125 - Loss:  1.0533 \n",
            "Batch 1340 / 3125 - Loss:  0.9599 \n",
            "Batch 1350 / 3125 - Loss:  1.0292 \n",
            "Batch 1360 / 3125 - Loss:  1.1439 \n",
            "Batch 1370 / 3125 - Loss:  0.9567 \n",
            "Batch 1380 / 3125 - Loss:  1.0590 \n",
            "Batch 1390 / 3125 - Loss:  1.0968 \n",
            "Batch 1400 / 3125 - Loss:  1.0803 \n",
            "Batch 1410 / 3125 - Loss:  1.0124 \n",
            "Batch 1420 / 3125 - Loss:  1.0853 \n",
            "Batch 1430 / 3125 - Loss:  1.0328 \n",
            "Batch 1440 / 3125 - Loss:  0.9825 \n",
            "Batch 1450 / 3125 - Loss:  1.1224 \n",
            "Batch 1460 / 3125 - Loss:  0.9895 \n",
            "Batch 1470 / 3125 - Loss:  1.0775 \n",
            "Batch 1480 / 3125 - Loss:  1.1443 \n",
            "Batch 1490 / 3125 - Loss:  0.9780 \n",
            "Batch 1500 / 3125 - Loss:  0.9709 \n",
            "Batch 1510 / 3125 - Loss:  1.0217 \n",
            "Batch 1520 / 3125 - Loss:  1.0746 \n",
            "Batch 1530 / 3125 - Loss:  0.9738 \n",
            "Batch 1540 / 3125 - Loss:  1.0455 \n",
            "Batch 1550 / 3125 - Loss:  0.9804 \n",
            "Batch 1560 / 3125 - Loss:  1.0398 \n",
            "Batch 1570 / 3125 - Loss:  1.0138 \n",
            "Batch 1580 / 3125 - Loss:  1.1087 \n",
            "Batch 1590 / 3125 - Loss:  1.0700 \n",
            "Batch 1600 / 3125 - Loss:  0.9746 \n",
            "Batch 1610 / 3125 - Loss:  1.0721 \n",
            "Batch 1620 / 3125 - Loss:  0.9883 \n",
            "Batch 1630 / 3125 - Loss:  1.0188 \n",
            "Batch 1640 / 3125 - Loss:  1.0755 \n",
            "Batch 1650 / 3125 - Loss:  1.0521 \n",
            "Batch 1660 / 3125 - Loss:  1.1336 \n",
            "Batch 1670 / 3125 - Loss:  0.9313 \n",
            "Batch 1680 / 3125 - Loss:  1.0235 \n",
            "Batch 1690 / 3125 - Loss:  0.9658 \n",
            "Batch 1700 / 3125 - Loss:  0.9889 \n",
            "Batch 1710 / 3125 - Loss:  1.0000 \n",
            "Batch 1720 / 3125 - Loss:  1.0219 \n",
            "Batch 1730 / 3125 - Loss:  1.0087 \n",
            "Batch 1740 / 3125 - Loss:  1.0220 \n",
            "Batch 1750 / 3125 - Loss:  0.9937 \n",
            "Batch 1760 / 3125 - Loss:  1.0893 \n",
            "Batch 1770 / 3125 - Loss:  0.9591 \n",
            "Batch 1780 / 3125 - Loss:  1.2092 \n",
            "Batch 1790 / 3125 - Loss:  1.0278 \n",
            "Batch 1800 / 3125 - Loss:  1.0139 \n",
            "Batch 1810 / 3125 - Loss:  1.0001 \n",
            "Batch 1820 / 3125 - Loss:  1.0503 \n",
            "Batch 1830 / 3125 - Loss:  0.9855 \n",
            "Batch 1840 / 3125 - Loss:  0.9726 \n",
            "Batch 1850 / 3125 - Loss:  1.0305 \n",
            "Batch 1860 / 3125 - Loss:  1.0018 \n",
            "Batch 1870 / 3125 - Loss:  0.9664 \n",
            "Batch 1880 / 3125 - Loss:  1.0591 \n",
            "Batch 1890 / 3125 - Loss:  1.0131 \n",
            "Batch 1900 / 3125 - Loss:  1.0808 \n",
            "Batch 1910 / 3125 - Loss:  0.9790 \n",
            "Batch 1920 / 3125 - Loss:  0.9078 \n",
            "Batch 1930 / 3125 - Loss:  0.9857 \n",
            "Batch 1940 / 3125 - Loss:  0.9963 \n",
            "Batch 1950 / 3125 - Loss:  0.9691 \n",
            "Batch 1960 / 3125 - Loss:  0.9284 \n",
            "Batch 1970 / 3125 - Loss:  1.0292 \n",
            "Batch 1980 / 3125 - Loss:  1.0966 \n",
            "Batch 1990 / 3125 - Loss:  0.9920 \n",
            "Batch 2000 / 3125 - Loss:  0.9834 \n",
            "Batch 2010 / 3125 - Loss:  1.0815 \n",
            "Batch 2020 / 3125 - Loss:  0.9601 \n",
            "Batch 2030 / 3125 - Loss:  1.0362 \n",
            "Batch 2040 / 3125 - Loss:  1.0687 \n",
            "Batch 2050 / 3125 - Loss:  0.9497 \n",
            "Batch 2060 / 3125 - Loss:  0.9828 \n",
            "Batch 2070 / 3125 - Loss:  0.9639 \n",
            "Batch 2080 / 3125 - Loss:  1.1257 \n",
            "Batch 2090 / 3125 - Loss:  1.0244 \n",
            "Batch 2100 / 3125 - Loss:  0.9835 \n",
            "Batch 2110 / 3125 - Loss:  1.0295 \n",
            "Batch 2120 / 3125 - Loss:  1.0149 \n",
            "Batch 2130 / 3125 - Loss:  1.0140 \n",
            "Batch 2140 / 3125 - Loss:  0.9485 \n",
            "Batch 2150 / 3125 - Loss:  1.0286 \n",
            "Batch 2160 / 3125 - Loss:  1.0539 \n",
            "Batch 2170 / 3125 - Loss:  1.0405 \n",
            "Batch 2180 / 3125 - Loss:  0.9815 \n",
            "Batch 2190 / 3125 - Loss:  1.0353 \n",
            "Batch 2200 / 3125 - Loss:  1.0942 \n",
            "Batch 2210 / 3125 - Loss:  1.0012 \n",
            "Batch 2220 / 3125 - Loss:  1.0009 \n",
            "Batch 2230 / 3125 - Loss:  1.1199 \n",
            "Batch 2240 / 3125 - Loss:  1.0878 \n",
            "Batch 2250 / 3125 - Loss:  1.0456 \n",
            "Batch 2260 / 3125 - Loss:  0.9701 \n",
            "Batch 2270 / 3125 - Loss:  1.0714 \n",
            "Batch 2280 / 3125 - Loss:  1.0899 \n",
            "Batch 2290 / 3125 - Loss:  1.0503 \n",
            "Batch 2300 / 3125 - Loss:  1.0471 \n",
            "Batch 2310 / 3125 - Loss:  1.0756 \n",
            "Batch 2320 / 3125 - Loss:  0.9419 \n",
            "Batch 2330 / 3125 - Loss:  1.0943 \n",
            "Batch 2340 / 3125 - Loss:  0.9964 \n",
            "Batch 2350 / 3125 - Loss:  1.1990 \n",
            "Batch 2360 / 3125 - Loss:  1.1208 \n",
            "Batch 2370 / 3125 - Loss:  1.1271 \n",
            "Batch 2380 / 3125 - Loss:  1.0170 \n",
            "Batch 2390 / 3125 - Loss:  0.9342 \n",
            "Batch 2400 / 3125 - Loss:  1.0053 \n",
            "Batch 2410 / 3125 - Loss:  1.0721 \n",
            "Batch 2420 / 3125 - Loss:  0.9986 \n",
            "Batch 2430 / 3125 - Loss:  0.9633 \n",
            "Batch 2440 / 3125 - Loss:  1.0990 \n",
            "Batch 2450 / 3125 - Loss:  0.9802 \n",
            "Batch 2460 / 3125 - Loss:  0.9403 \n",
            "Batch 2470 / 3125 - Loss:  0.9415 \n",
            "Batch 2480 / 3125 - Loss:  1.0636 \n",
            "Batch 2490 / 3125 - Loss:  1.1248 \n",
            "Batch 2500 / 3125 - Loss:  1.1614 \n",
            "Batch 2510 / 3125 - Loss:  0.9341 \n",
            "Batch 2520 / 3125 - Loss:  0.8993 \n",
            "Batch 2530 / 3125 - Loss:  1.0746 \n",
            "Batch 2540 / 3125 - Loss:  1.0065 \n",
            "Batch 2550 / 3125 - Loss:  0.9707 \n",
            "Batch 2560 / 3125 - Loss:  1.1593 \n",
            "Batch 2570 / 3125 - Loss:  0.9221 \n",
            "Batch 2580 / 3125 - Loss:  1.0774 \n",
            "Batch 2590 / 3125 - Loss:  0.9687 \n",
            "Batch 2600 / 3125 - Loss:  1.0316 \n",
            "Batch 2610 / 3125 - Loss:  1.0735 \n",
            "Batch 2620 / 3125 - Loss:  1.0466 \n",
            "Batch 2630 / 3125 - Loss:  1.0899 \n",
            "Batch 2640 / 3125 - Loss:  1.0800 \n",
            "Batch 2650 / 3125 - Loss:  0.9478 \n",
            "Batch 2660 / 3125 - Loss:  0.9757 \n",
            "Batch 2670 / 3125 - Loss:  1.0176 \n",
            "Batch 2680 / 3125 - Loss:  1.0331 \n",
            "Batch 2690 / 3125 - Loss:  0.9714 \n",
            "Batch 2700 / 3125 - Loss:  0.9272 \n",
            "Batch 2710 / 3125 - Loss:  0.9705 \n",
            "Batch 2720 / 3125 - Loss:  1.0434 \n",
            "Batch 2730 / 3125 - Loss:  1.0576 \n",
            "Batch 2740 / 3125 - Loss:  1.0210 \n",
            "Batch 2750 / 3125 - Loss:  1.0478 \n",
            "Batch 2760 / 3125 - Loss:  1.0744 \n",
            "Batch 2770 / 3125 - Loss:  0.9624 \n",
            "Batch 2780 / 3125 - Loss:  0.9828 \n",
            "Batch 2790 / 3125 - Loss:  1.1689 \n",
            "Batch 2800 / 3125 - Loss:  1.0898 \n",
            "Batch 2810 / 3125 - Loss:  0.9501 \n",
            "Batch 2820 / 3125 - Loss:  1.0602 \n",
            "Batch 2830 / 3125 - Loss:  0.9637 \n",
            "Batch 2840 / 3125 - Loss:  1.0601 \n",
            "Batch 2850 / 3125 - Loss:  1.0337 \n",
            "Batch 2860 / 3125 - Loss:  0.9960 \n",
            "Batch 2870 / 3125 - Loss:  1.0014 \n",
            "Batch 2880 / 3125 - Loss:  1.0548 \n",
            "Batch 2890 / 3125 - Loss:  1.0462 \n",
            "Batch 2900 / 3125 - Loss:  1.0031 \n",
            "Batch 2910 / 3125 - Loss:  1.0777 \n",
            "Batch 2920 / 3125 - Loss:  0.9821 \n",
            "Batch 2930 / 3125 - Loss:  0.9843 \n",
            "Batch 2940 / 3125 - Loss:  1.0284 \n",
            "Batch 2950 / 3125 - Loss:  1.0406 \n",
            "Batch 2960 / 3125 - Loss:  0.9928 \n",
            "Batch 2970 / 3125 - Loss:  1.0167 \n",
            "Batch 2980 / 3125 - Loss:  1.0392 \n",
            "Batch 2990 / 3125 - Loss:  0.9223 \n",
            "Batch 3000 / 3125 - Loss:  0.9577 \n",
            "Batch 3010 / 3125 - Loss:  1.0970 \n",
            "Batch 3020 / 3125 - Loss:  0.8826 \n",
            "Batch 3030 / 3125 - Loss:  1.0066 \n",
            "Batch 3040 / 3125 - Loss:  1.1081 \n",
            "Batch 3050 / 3125 - Loss:  1.1208 \n",
            "Batch 3060 / 3125 - Loss:  0.9875 \n",
            "Batch 3070 / 3125 - Loss:  1.0358 \n",
            "Batch 3080 / 3125 - Loss:  1.0768 \n",
            "Batch 3090 / 3125 - Loss:  1.0405 \n",
            "Batch 3100 / 3125 - Loss:  0.9830 \n",
            "Batch 3110 / 3125 - Loss:  0.9440 \n",
            "Batch 3120 / 3125 - Loss:  1.0121 \n",
            "Epoch 49 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0153 \n",
            "Batch 10 / 3125 - Loss:  1.1188 \n",
            "Batch 20 / 3125 - Loss:  0.9506 \n",
            "Batch 30 / 3125 - Loss:  0.9419 \n",
            "Batch 40 / 3125 - Loss:  0.8766 \n",
            "Batch 50 / 3125 - Loss:  0.9998 \n",
            "Batch 60 / 3125 - Loss:  0.9346 \n",
            "Batch 70 / 3125 - Loss:  1.1630 \n",
            "Batch 80 / 3125 - Loss:  1.0181 \n",
            "Batch 90 / 3125 - Loss:  1.0548 \n",
            "Batch 100 / 3125 - Loss:  1.0321 \n",
            "Batch 110 / 3125 - Loss:  1.0427 \n",
            "Batch 120 / 3125 - Loss:  1.0776 \n",
            "Batch 130 / 3125 - Loss:  1.0989 \n",
            "Batch 140 / 3125 - Loss:  1.0872 \n",
            "Batch 150 / 3125 - Loss:  1.0360 \n",
            "Batch 160 / 3125 - Loss:  1.0679 \n",
            "Batch 170 / 3125 - Loss:  1.0260 \n",
            "Batch 180 / 3125 - Loss:  1.0433 \n",
            "Batch 190 / 3125 - Loss:  1.0620 \n",
            "Batch 200 / 3125 - Loss:  1.1539 \n",
            "Batch 210 / 3125 - Loss:  0.9647 \n",
            "Batch 220 / 3125 - Loss:  0.9465 \n",
            "Batch 230 / 3125 - Loss:  0.9850 \n",
            "Batch 240 / 3125 - Loss:  0.9692 \n",
            "Batch 250 / 3125 - Loss:  0.9960 \n",
            "Batch 260 / 3125 - Loss:  1.1295 \n",
            "Batch 270 / 3125 - Loss:  0.9753 \n",
            "Batch 280 / 3125 - Loss:  1.0339 \n",
            "Batch 290 / 3125 - Loss:  1.0862 \n",
            "Batch 300 / 3125 - Loss:  0.9828 \n",
            "Batch 310 / 3125 - Loss:  0.9074 \n",
            "Batch 320 / 3125 - Loss:  0.9920 \n",
            "Batch 330 / 3125 - Loss:  0.8860 \n",
            "Batch 340 / 3125 - Loss:  1.1203 \n",
            "Batch 350 / 3125 - Loss:  1.0463 \n",
            "Batch 360 / 3125 - Loss:  1.0497 \n",
            "Batch 370 / 3125 - Loss:  1.0879 \n",
            "Batch 380 / 3125 - Loss:  1.0717 \n",
            "Batch 390 / 3125 - Loss:  1.1429 \n",
            "Batch 400 / 3125 - Loss:  1.0088 \n",
            "Batch 410 / 3125 - Loss:  1.0227 \n",
            "Batch 420 / 3125 - Loss:  0.9129 \n",
            "Batch 430 / 3125 - Loss:  0.9941 \n",
            "Batch 440 / 3125 - Loss:  1.0972 \n",
            "Batch 450 / 3125 - Loss:  0.9753 \n",
            "Batch 460 / 3125 - Loss:  1.0383 \n",
            "Batch 470 / 3125 - Loss:  1.0315 \n",
            "Batch 480 / 3125 - Loss:  1.0316 \n",
            "Batch 490 / 3125 - Loss:  1.0110 \n",
            "Batch 500 / 3125 - Loss:  1.0451 \n",
            "Batch 510 / 3125 - Loss:  0.9789 \n",
            "Batch 520 / 3125 - Loss:  0.9786 \n",
            "Batch 530 / 3125 - Loss:  1.0248 \n",
            "Batch 540 / 3125 - Loss:  1.1188 \n",
            "Batch 550 / 3125 - Loss:  0.9563 \n",
            "Batch 560 / 3125 - Loss:  1.0263 \n",
            "Batch 570 / 3125 - Loss:  1.0711 \n",
            "Batch 580 / 3125 - Loss:  1.0227 \n",
            "Batch 590 / 3125 - Loss:  1.0439 \n",
            "Batch 600 / 3125 - Loss:  1.0247 \n",
            "Batch 610 / 3125 - Loss:  1.0036 \n",
            "Batch 620 / 3125 - Loss:  1.1894 \n",
            "Batch 630 / 3125 - Loss:  1.0001 \n",
            "Batch 640 / 3125 - Loss:  0.9533 \n",
            "Batch 650 / 3125 - Loss:  0.9533 \n",
            "Batch 660 / 3125 - Loss:  1.0654 \n",
            "Batch 670 / 3125 - Loss:  0.9968 \n",
            "Batch 680 / 3125 - Loss:  1.1993 \n",
            "Batch 690 / 3125 - Loss:  0.9157 \n",
            "Batch 700 / 3125 - Loss:  1.0523 \n",
            "Batch 710 / 3125 - Loss:  1.0958 \n",
            "Batch 720 / 3125 - Loss:  0.9690 \n",
            "Batch 730 / 3125 - Loss:  0.9196 \n",
            "Batch 740 / 3125 - Loss:  1.1227 \n",
            "Batch 750 / 3125 - Loss:  1.0505 \n",
            "Batch 760 / 3125 - Loss:  1.0301 \n",
            "Batch 770 / 3125 - Loss:  1.0238 \n",
            "Batch 780 / 3125 - Loss:  1.0594 \n",
            "Batch 790 / 3125 - Loss:  1.0627 \n",
            "Batch 800 / 3125 - Loss:  0.9894 \n",
            "Batch 810 / 3125 - Loss:  1.0074 \n",
            "Batch 820 / 3125 - Loss:  1.0618 \n",
            "Batch 830 / 3125 - Loss:  1.0105 \n",
            "Batch 840 / 3125 - Loss:  0.9373 \n",
            "Batch 850 / 3125 - Loss:  1.0220 \n",
            "Batch 860 / 3125 - Loss:  0.9744 \n",
            "Batch 870 / 3125 - Loss:  1.0510 \n",
            "Batch 880 / 3125 - Loss:  1.1220 \n",
            "Batch 890 / 3125 - Loss:  0.9368 \n",
            "Batch 900 / 3125 - Loss:  0.9437 \n",
            "Batch 910 / 3125 - Loss:  1.0659 \n",
            "Batch 920 / 3125 - Loss:  0.9307 \n",
            "Batch 930 / 3125 - Loss:  0.9879 \n",
            "Batch 940 / 3125 - Loss:  0.9585 \n",
            "Batch 950 / 3125 - Loss:  1.0606 \n",
            "Batch 960 / 3125 - Loss:  1.0612 \n",
            "Batch 970 / 3125 - Loss:  1.0338 \n",
            "Batch 980 / 3125 - Loss:  0.9883 \n",
            "Batch 990 / 3125 - Loss:  1.0190 \n",
            "Batch 1000 / 3125 - Loss:  1.0189 \n",
            "Batch 1010 / 3125 - Loss:  0.8682 \n",
            "Batch 1020 / 3125 - Loss:  1.0698 \n",
            "Batch 1030 / 3125 - Loss:  0.9722 \n",
            "Batch 1040 / 3125 - Loss:  0.9109 \n",
            "Batch 1050 / 3125 - Loss:  1.0057 \n",
            "Batch 1060 / 3125 - Loss:  0.9455 \n",
            "Batch 1070 / 3125 - Loss:  1.1514 \n",
            "Batch 1080 / 3125 - Loss:  1.0771 \n",
            "Batch 1090 / 3125 - Loss:  1.0138 \n",
            "Batch 1100 / 3125 - Loss:  1.0473 \n",
            "Batch 1110 / 3125 - Loss:  0.9208 \n",
            "Batch 1120 / 3125 - Loss:  1.0357 \n",
            "Batch 1130 / 3125 - Loss:  1.0146 \n",
            "Batch 1140 / 3125 - Loss:  1.0105 \n",
            "Batch 1150 / 3125 - Loss:  0.9840 \n",
            "Batch 1160 / 3125 - Loss:  1.0318 \n",
            "Batch 1170 / 3125 - Loss:  0.9492 \n",
            "Batch 1180 / 3125 - Loss:  0.9866 \n",
            "Batch 1190 / 3125 - Loss:  0.9812 \n",
            "Batch 1200 / 3125 - Loss:  0.9238 \n",
            "Batch 1210 / 3125 - Loss:  1.0462 \n",
            "Batch 1220 / 3125 - Loss:  0.9901 \n",
            "Batch 1230 / 3125 - Loss:  1.0084 \n",
            "Batch 1240 / 3125 - Loss:  1.0131 \n",
            "Batch 1250 / 3125 - Loss:  1.2021 \n",
            "Batch 1260 / 3125 - Loss:  1.0197 \n",
            "Batch 1270 / 3125 - Loss:  1.0913 \n",
            "Batch 1280 / 3125 - Loss:  1.0436 \n",
            "Batch 1290 / 3125 - Loss:  1.1367 \n",
            "Batch 1300 / 3125 - Loss:  1.0163 \n",
            "Batch 1310 / 3125 - Loss:  1.1183 \n",
            "Batch 1320 / 3125 - Loss:  1.0151 \n",
            "Batch 1330 / 3125 - Loss:  0.9690 \n",
            "Batch 1340 / 3125 - Loss:  0.9898 \n",
            "Batch 1350 / 3125 - Loss:  0.9022 \n",
            "Batch 1360 / 3125 - Loss:  0.9872 \n",
            "Batch 1370 / 3125 - Loss:  1.1401 \n",
            "Batch 1380 / 3125 - Loss:  0.9590 \n",
            "Batch 1390 / 3125 - Loss:  1.0713 \n",
            "Batch 1400 / 3125 - Loss:  1.0577 \n",
            "Batch 1410 / 3125 - Loss:  0.9957 \n",
            "Batch 1420 / 3125 - Loss:  1.1280 \n",
            "Batch 1430 / 3125 - Loss:  1.0937 \n",
            "Batch 1440 / 3125 - Loss:  0.9203 \n",
            "Batch 1450 / 3125 - Loss:  1.0486 \n",
            "Batch 1460 / 3125 - Loss:  0.9674 \n",
            "Batch 1470 / 3125 - Loss:  0.8878 \n",
            "Batch 1480 / 3125 - Loss:  0.9773 \n",
            "Batch 1490 / 3125 - Loss:  0.9487 \n",
            "Batch 1500 / 3125 - Loss:  1.0652 \n",
            "Batch 1510 / 3125 - Loss:  1.1744 \n",
            "Batch 1520 / 3125 - Loss:  1.0319 \n",
            "Batch 1530 / 3125 - Loss:  1.1237 \n",
            "Batch 1540 / 3125 - Loss:  1.0251 \n",
            "Batch 1550 / 3125 - Loss:  0.9720 \n",
            "Batch 1560 / 3125 - Loss:  1.0827 \n",
            "Batch 1570 / 3125 - Loss:  1.0056 \n",
            "Batch 1580 / 3125 - Loss:  0.9949 \n",
            "Batch 1590 / 3125 - Loss:  0.9651 \n",
            "Batch 1600 / 3125 - Loss:  1.0707 \n",
            "Batch 1610 / 3125 - Loss:  1.0261 \n",
            "Batch 1620 / 3125 - Loss:  1.0297 \n",
            "Batch 1630 / 3125 - Loss:  0.9923 \n",
            "Batch 1640 / 3125 - Loss:  0.9718 \n",
            "Batch 1650 / 3125 - Loss:  0.9910 \n",
            "Batch 1660 / 3125 - Loss:  1.0577 \n",
            "Batch 1670 / 3125 - Loss:  1.0483 \n",
            "Batch 1680 / 3125 - Loss:  1.0034 \n",
            "Batch 1690 / 3125 - Loss:  1.0737 \n",
            "Batch 1700 / 3125 - Loss:  1.0505 \n",
            "Batch 1710 / 3125 - Loss:  0.9445 \n",
            "Batch 1720 / 3125 - Loss:  1.0315 \n",
            "Batch 1730 / 3125 - Loss:  0.9268 \n",
            "Batch 1740 / 3125 - Loss:  1.0125 \n",
            "Batch 1750 / 3125 - Loss:  1.0191 \n",
            "Batch 1760 / 3125 - Loss:  0.9857 \n",
            "Batch 1770 / 3125 - Loss:  0.9624 \n",
            "Batch 1780 / 3125 - Loss:  0.9326 \n",
            "Batch 1790 / 3125 - Loss:  1.0775 \n",
            "Batch 1800 / 3125 - Loss:  0.9761 \n",
            "Batch 1810 / 3125 - Loss:  1.0203 \n",
            "Batch 1820 / 3125 - Loss:  1.1003 \n",
            "Batch 1830 / 3125 - Loss:  1.0634 \n",
            "Batch 1840 / 3125 - Loss:  0.9398 \n",
            "Batch 1850 / 3125 - Loss:  0.9509 \n",
            "Batch 1860 / 3125 - Loss:  1.0013 \n",
            "Batch 1870 / 3125 - Loss:  1.0905 \n",
            "Batch 1880 / 3125 - Loss:  0.9396 \n",
            "Batch 1890 / 3125 - Loss:  1.0192 \n",
            "Batch 1900 / 3125 - Loss:  1.0260 \n",
            "Batch 1910 / 3125 - Loss:  1.0395 \n",
            "Batch 1920 / 3125 - Loss:  0.9637 \n",
            "Batch 1930 / 3125 - Loss:  1.0248 \n",
            "Batch 1940 / 3125 - Loss:  1.1128 \n",
            "Batch 1950 / 3125 - Loss:  0.9519 \n",
            "Batch 1960 / 3125 - Loss:  0.9171 \n",
            "Batch 1970 / 3125 - Loss:  1.1444 \n",
            "Batch 1980 / 3125 - Loss:  0.9257 \n",
            "Batch 1990 / 3125 - Loss:  0.9735 \n",
            "Batch 2000 / 3125 - Loss:  0.9783 \n",
            "Batch 2010 / 3125 - Loss:  1.1074 \n",
            "Batch 2020 / 3125 - Loss:  1.0851 \n",
            "Batch 2030 / 3125 - Loss:  0.9683 \n",
            "Batch 2040 / 3125 - Loss:  0.9427 \n",
            "Batch 2050 / 3125 - Loss:  0.9924 \n",
            "Batch 2060 / 3125 - Loss:  0.9335 \n",
            "Batch 2070 / 3125 - Loss:  0.9360 \n",
            "Batch 2080 / 3125 - Loss:  0.9824 \n",
            "Batch 2090 / 3125 - Loss:  0.9850 \n",
            "Batch 2100 / 3125 - Loss:  1.0283 \n",
            "Batch 2110 / 3125 - Loss:  1.0188 \n",
            "Batch 2120 / 3125 - Loss:  1.0884 \n",
            "Batch 2130 / 3125 - Loss:  0.9892 \n",
            "Batch 2140 / 3125 - Loss:  1.0830 \n",
            "Batch 2150 / 3125 - Loss:  0.9597 \n",
            "Batch 2160 / 3125 - Loss:  0.9498 \n",
            "Batch 2170 / 3125 - Loss:  0.9297 \n",
            "Batch 2180 / 3125 - Loss:  0.9006 \n",
            "Batch 2190 / 3125 - Loss:  1.0401 \n",
            "Batch 2200 / 3125 - Loss:  1.0638 \n",
            "Batch 2210 / 3125 - Loss:  1.0764 \n",
            "Batch 2220 / 3125 - Loss:  1.0585 \n",
            "Batch 2230 / 3125 - Loss:  1.0582 \n",
            "Batch 2240 / 3125 - Loss:  1.0129 \n",
            "Batch 2250 / 3125 - Loss:  1.0546 \n",
            "Batch 2260 / 3125 - Loss:  0.9862 \n",
            "Batch 2270 / 3125 - Loss:  0.9354 \n",
            "Batch 2280 / 3125 - Loss:  1.1320 \n",
            "Batch 2290 / 3125 - Loss:  0.9668 \n",
            "Batch 2300 / 3125 - Loss:  1.1012 \n",
            "Batch 2310 / 3125 - Loss:  0.8974 \n",
            "Batch 2320 / 3125 - Loss:  0.9843 \n",
            "Batch 2330 / 3125 - Loss:  0.9201 \n",
            "Batch 2340 / 3125 - Loss:  0.8764 \n",
            "Batch 2350 / 3125 - Loss:  1.0292 \n",
            "Batch 2360 / 3125 - Loss:  0.9431 \n",
            "Batch 2370 / 3125 - Loss:  1.1167 \n",
            "Batch 2380 / 3125 - Loss:  0.8747 \n",
            "Batch 2390 / 3125 - Loss:  1.0468 \n",
            "Batch 2400 / 3125 - Loss:  0.9679 \n",
            "Batch 2410 / 3125 - Loss:  1.0683 \n",
            "Batch 2420 / 3125 - Loss:  0.9324 \n",
            "Batch 2430 / 3125 - Loss:  0.9514 \n",
            "Batch 2440 / 3125 - Loss:  1.0344 \n",
            "Batch 2450 / 3125 - Loss:  1.0324 \n",
            "Batch 2460 / 3125 - Loss:  1.0973 \n",
            "Batch 2470 / 3125 - Loss:  0.9742 \n",
            "Batch 2480 / 3125 - Loss:  1.0260 \n",
            "Batch 2490 / 3125 - Loss:  0.9988 \n",
            "Batch 2500 / 3125 - Loss:  0.9168 \n",
            "Batch 2510 / 3125 - Loss:  1.0785 \n",
            "Batch 2520 / 3125 - Loss:  0.9531 \n",
            "Batch 2530 / 3125 - Loss:  1.0639 \n",
            "Batch 2540 / 3125 - Loss:  1.0333 \n",
            "Batch 2550 / 3125 - Loss:  0.9209 \n",
            "Batch 2560 / 3125 - Loss:  0.9834 \n",
            "Batch 2570 / 3125 - Loss:  1.1330 \n",
            "Batch 2580 / 3125 - Loss:  0.9367 \n",
            "Batch 2590 / 3125 - Loss:  0.9763 \n",
            "Batch 2600 / 3125 - Loss:  0.9917 \n",
            "Batch 2610 / 3125 - Loss:  1.0362 \n",
            "Batch 2620 / 3125 - Loss:  0.9538 \n",
            "Batch 2630 / 3125 - Loss:  1.0219 \n",
            "Batch 2640 / 3125 - Loss:  1.0058 \n",
            "Batch 2650 / 3125 - Loss:  1.1248 \n",
            "Batch 2660 / 3125 - Loss:  1.1488 \n",
            "Batch 2670 / 3125 - Loss:  1.0649 \n",
            "Batch 2680 / 3125 - Loss:  1.0897 \n",
            "Batch 2690 / 3125 - Loss:  0.9459 \n",
            "Batch 2700 / 3125 - Loss:  1.0012 \n",
            "Batch 2710 / 3125 - Loss:  0.9934 \n",
            "Batch 2720 / 3125 - Loss:  0.9735 \n",
            "Batch 2730 / 3125 - Loss:  1.0603 \n",
            "Batch 2740 / 3125 - Loss:  1.0360 \n",
            "Batch 2750 / 3125 - Loss:  0.9166 \n",
            "Batch 2760 / 3125 - Loss:  0.9910 \n",
            "Batch 2770 / 3125 - Loss:  1.0211 \n",
            "Batch 2780 / 3125 - Loss:  0.9176 \n",
            "Batch 2790 / 3125 - Loss:  1.0674 \n",
            "Batch 2800 / 3125 - Loss:  0.9773 \n",
            "Batch 2810 / 3125 - Loss:  1.0962 \n",
            "Batch 2820 / 3125 - Loss:  1.0200 \n",
            "Batch 2830 / 3125 - Loss:  0.9908 \n",
            "Batch 2840 / 3125 - Loss:  1.0676 \n",
            "Batch 2850 / 3125 - Loss:  0.9453 \n",
            "Batch 2860 / 3125 - Loss:  0.9780 \n",
            "Batch 2870 / 3125 - Loss:  1.0769 \n",
            "Batch 2880 / 3125 - Loss:  1.0294 \n",
            "Batch 2890 / 3125 - Loss:  1.0002 \n",
            "Batch 2900 / 3125 - Loss:  0.9748 \n",
            "Batch 2910 / 3125 - Loss:  1.1036 \n",
            "Batch 2920 / 3125 - Loss:  1.0638 \n",
            "Batch 2930 / 3125 - Loss:  1.0869 \n",
            "Batch 2940 / 3125 - Loss:  1.0479 \n",
            "Batch 2950 / 3125 - Loss:  1.0835 \n",
            "Batch 2960 / 3125 - Loss:  1.1013 \n",
            "Batch 2970 / 3125 - Loss:  0.9768 \n",
            "Batch 2980 / 3125 - Loss:  0.9560 \n",
            "Batch 2990 / 3125 - Loss:  0.8194 \n",
            "Batch 3000 / 3125 - Loss:  0.9628 \n",
            "Batch 3010 / 3125 - Loss:  0.9904 \n",
            "Batch 3020 / 3125 - Loss:  1.0824 \n",
            "Batch 3030 / 3125 - Loss:  1.0950 \n",
            "Batch 3040 / 3125 - Loss:  1.0921 \n",
            "Batch 3050 / 3125 - Loss:  1.0382 \n",
            "Batch 3060 / 3125 - Loss:  1.0776 \n",
            "Batch 3070 / 3125 - Loss:  1.0317 \n",
            "Batch 3080 / 3125 - Loss:  0.9212 \n",
            "Batch 3090 / 3125 - Loss:  0.9910 \n",
            "Batch 3100 / 3125 - Loss:  0.9813 \n",
            "Batch 3110 / 3125 - Loss:  0.9921 \n",
            "Batch 3120 / 3125 - Loss:  1.0004 \n",
            "Epoch 50 / 50 \n",
            "Batch 0 / 3125 - Loss:  1.0632 \n",
            "Batch 10 / 3125 - Loss:  1.0201 \n",
            "Batch 20 / 3125 - Loss:  1.0157 \n",
            "Batch 30 / 3125 - Loss:  0.9600 \n",
            "Batch 40 / 3125 - Loss:  1.0860 \n",
            "Batch 50 / 3125 - Loss:  0.9664 \n",
            "Batch 60 / 3125 - Loss:  0.9222 \n",
            "Batch 70 / 3125 - Loss:  0.9710 \n",
            "Batch 80 / 3125 - Loss:  1.1168 \n",
            "Batch 90 / 3125 - Loss:  0.9370 \n",
            "Batch 100 / 3125 - Loss:  0.9897 \n",
            "Batch 110 / 3125 - Loss:  1.0843 \n",
            "Batch 120 / 3125 - Loss:  0.8740 \n",
            "Batch 130 / 3125 - Loss:  1.0263 \n",
            "Batch 140 / 3125 - Loss:  1.0553 \n",
            "Batch 150 / 3125 - Loss:  1.0848 \n",
            "Batch 160 / 3125 - Loss:  1.1120 \n",
            "Batch 170 / 3125 - Loss:  0.9982 \n",
            "Batch 180 / 3125 - Loss:  0.9725 \n",
            "Batch 190 / 3125 - Loss:  0.9489 \n",
            "Batch 200 / 3125 - Loss:  1.0454 \n",
            "Batch 210 / 3125 - Loss:  0.9877 \n",
            "Batch 220 / 3125 - Loss:  0.9909 \n",
            "Batch 230 / 3125 - Loss:  1.0049 \n",
            "Batch 240 / 3125 - Loss:  1.0748 \n",
            "Batch 250 / 3125 - Loss:  1.0052 \n",
            "Batch 260 / 3125 - Loss:  0.8406 \n",
            "Batch 270 / 3125 - Loss:  0.9775 \n",
            "Batch 280 / 3125 - Loss:  0.9986 \n",
            "Batch 290 / 3125 - Loss:  0.8985 \n",
            "Batch 300 / 3125 - Loss:  1.0884 \n",
            "Batch 310 / 3125 - Loss:  0.9552 \n",
            "Batch 320 / 3125 - Loss:  0.9830 \n",
            "Batch 330 / 3125 - Loss:  0.9380 \n",
            "Batch 340 / 3125 - Loss:  1.0024 \n",
            "Batch 350 / 3125 - Loss:  1.1021 \n",
            "Batch 360 / 3125 - Loss:  1.0318 \n",
            "Batch 370 / 3125 - Loss:  0.9833 \n",
            "Batch 380 / 3125 - Loss:  0.9789 \n",
            "Batch 390 / 3125 - Loss:  1.0571 \n",
            "Batch 400 / 3125 - Loss:  1.0996 \n",
            "Batch 410 / 3125 - Loss:  1.0000 \n",
            "Batch 420 / 3125 - Loss:  1.0855 \n",
            "Batch 430 / 3125 - Loss:  0.9971 \n",
            "Batch 440 / 3125 - Loss:  1.0399 \n",
            "Batch 450 / 3125 - Loss:  0.9839 \n",
            "Batch 460 / 3125 - Loss:  1.0257 \n",
            "Batch 470 / 3125 - Loss:  0.9889 \n",
            "Batch 480 / 3125 - Loss:  1.1000 \n",
            "Batch 490 / 3125 - Loss:  1.0642 \n",
            "Batch 500 / 3125 - Loss:  1.1205 \n",
            "Batch 510 / 3125 - Loss:  1.0267 \n",
            "Batch 520 / 3125 - Loss:  1.0516 \n",
            "Batch 530 / 3125 - Loss:  0.9090 \n",
            "Batch 540 / 3125 - Loss:  0.9280 \n",
            "Batch 550 / 3125 - Loss:  1.0878 \n",
            "Batch 560 / 3125 - Loss:  1.0715 \n",
            "Batch 570 / 3125 - Loss:  0.9315 \n",
            "Batch 580 / 3125 - Loss:  0.9969 \n",
            "Batch 590 / 3125 - Loss:  0.9452 \n",
            "Batch 600 / 3125 - Loss:  0.9208 \n",
            "Batch 610 / 3125 - Loss:  0.9383 \n",
            "Batch 620 / 3125 - Loss:  0.9527 \n",
            "Batch 630 / 3125 - Loss:  1.0579 \n",
            "Batch 640 / 3125 - Loss:  1.0958 \n",
            "Batch 650 / 3125 - Loss:  1.0066 \n",
            "Batch 660 / 3125 - Loss:  1.0104 \n",
            "Batch 670 / 3125 - Loss:  0.9996 \n",
            "Batch 680 / 3125 - Loss:  0.9121 \n",
            "Batch 690 / 3125 - Loss:  0.9826 \n",
            "Batch 700 / 3125 - Loss:  1.1747 \n",
            "Batch 710 / 3125 - Loss:  1.0660 \n",
            "Batch 720 / 3125 - Loss:  1.0542 \n",
            "Batch 730 / 3125 - Loss:  1.0816 \n",
            "Batch 740 / 3125 - Loss:  1.1115 \n",
            "Batch 750 / 3125 - Loss:  0.9802 \n",
            "Batch 760 / 3125 - Loss:  0.9582 \n",
            "Batch 770 / 3125 - Loss:  1.0447 \n",
            "Batch 780 / 3125 - Loss:  1.0603 \n",
            "Batch 790 / 3125 - Loss:  1.0911 \n",
            "Batch 800 / 3125 - Loss:  1.0334 \n",
            "Batch 810 / 3125 - Loss:  1.0902 \n",
            "Batch 820 / 3125 - Loss:  0.9600 \n",
            "Batch 830 / 3125 - Loss:  1.1119 \n",
            "Batch 840 / 3125 - Loss:  1.0110 \n",
            "Batch 850 / 3125 - Loss:  1.0132 \n",
            "Batch 860 / 3125 - Loss:  1.0606 \n",
            "Batch 870 / 3125 - Loss:  1.0559 \n",
            "Batch 880 / 3125 - Loss:  0.9934 \n",
            "Batch 890 / 3125 - Loss:  0.9707 \n",
            "Batch 900 / 3125 - Loss:  0.9407 \n",
            "Batch 910 / 3125 - Loss:  0.9900 \n",
            "Batch 920 / 3125 - Loss:  1.0138 \n",
            "Batch 930 / 3125 - Loss:  0.9674 \n",
            "Batch 940 / 3125 - Loss:  1.0188 \n",
            "Batch 950 / 3125 - Loss:  1.0171 \n",
            "Batch 960 / 3125 - Loss:  1.0109 \n",
            "Batch 970 / 3125 - Loss:  1.0524 \n",
            "Batch 980 / 3125 - Loss:  0.9849 \n",
            "Batch 990 / 3125 - Loss:  0.9547 \n",
            "Batch 1000 / 3125 - Loss:  0.9291 \n",
            "Batch 1010 / 3125 - Loss:  1.0724 \n",
            "Batch 1020 / 3125 - Loss:  0.9443 \n",
            "Batch 1030 / 3125 - Loss:  1.1317 \n",
            "Batch 1040 / 3125 - Loss:  1.1137 \n",
            "Batch 1050 / 3125 - Loss:  0.9763 \n",
            "Batch 1060 / 3125 - Loss:  0.9857 \n",
            "Batch 1070 / 3125 - Loss:  1.0168 \n",
            "Batch 1080 / 3125 - Loss:  1.0934 \n",
            "Batch 1090 / 3125 - Loss:  0.9615 \n",
            "Batch 1100 / 3125 - Loss:  1.0354 \n",
            "Batch 1110 / 3125 - Loss:  0.9849 \n",
            "Batch 1120 / 3125 - Loss:  1.0522 \n",
            "Batch 1130 / 3125 - Loss:  1.0729 \n",
            "Batch 1140 / 3125 - Loss:  0.9561 \n",
            "Batch 1150 / 3125 - Loss:  0.9086 \n",
            "Batch 1160 / 3125 - Loss:  0.9765 \n",
            "Batch 1170 / 3125 - Loss:  1.0294 \n",
            "Batch 1180 / 3125 - Loss:  1.0213 \n",
            "Batch 1190 / 3125 - Loss:  1.0395 \n",
            "Batch 1200 / 3125 - Loss:  1.1025 \n",
            "Batch 1210 / 3125 - Loss:  1.0979 \n",
            "Batch 1220 / 3125 - Loss:  1.1148 \n",
            "Batch 1230 / 3125 - Loss:  1.0684 \n",
            "Batch 1240 / 3125 - Loss:  1.0034 \n",
            "Batch 1250 / 3125 - Loss:  0.9690 \n",
            "Batch 1260 / 3125 - Loss:  0.9176 \n",
            "Batch 1270 / 3125 - Loss:  0.9821 \n",
            "Batch 1280 / 3125 - Loss:  0.9981 \n",
            "Batch 1290 / 3125 - Loss:  1.0129 \n",
            "Batch 1300 / 3125 - Loss:  1.0996 \n",
            "Batch 1310 / 3125 - Loss:  1.0018 \n",
            "Batch 1320 / 3125 - Loss:  1.1081 \n",
            "Batch 1330 / 3125 - Loss:  0.9575 \n",
            "Batch 1340 / 3125 - Loss:  1.0061 \n",
            "Batch 1350 / 3125 - Loss:  1.1011 \n",
            "Batch 1360 / 3125 - Loss:  1.0367 \n",
            "Batch 1370 / 3125 - Loss:  0.9534 \n",
            "Batch 1380 / 3125 - Loss:  1.0349 \n",
            "Batch 1390 / 3125 - Loss:  1.0200 \n",
            "Batch 1400 / 3125 - Loss:  0.9102 \n",
            "Batch 1410 / 3125 - Loss:  0.9249 \n",
            "Batch 1420 / 3125 - Loss:  1.0103 \n",
            "Batch 1430 / 3125 - Loss:  0.9798 \n",
            "Batch 1440 / 3125 - Loss:  1.0102 \n",
            "Batch 1450 / 3125 - Loss:  1.0476 \n",
            "Batch 1460 / 3125 - Loss:  1.0054 \n",
            "Batch 1470 / 3125 - Loss:  1.1267 \n",
            "Batch 1480 / 3125 - Loss:  1.0763 \n",
            "Batch 1490 / 3125 - Loss:  0.9505 \n",
            "Batch 1500 / 3125 - Loss:  1.0681 \n",
            "Batch 1510 / 3125 - Loss:  0.9138 \n",
            "Batch 1520 / 3125 - Loss:  1.0724 \n",
            "Batch 1530 / 3125 - Loss:  0.9182 \n",
            "Batch 1540 / 3125 - Loss:  0.9616 \n",
            "Batch 1550 / 3125 - Loss:  1.0738 \n",
            "Batch 1560 / 3125 - Loss:  0.9506 \n",
            "Batch 1570 / 3125 - Loss:  0.9876 \n",
            "Batch 1580 / 3125 - Loss:  1.0222 \n",
            "Batch 1590 / 3125 - Loss:  1.0299 \n",
            "Batch 1600 / 3125 - Loss:  0.9830 \n",
            "Batch 1610 / 3125 - Loss:  0.9068 \n",
            "Batch 1620 / 3125 - Loss:  1.0532 \n",
            "Batch 1630 / 3125 - Loss:  1.1087 \n",
            "Batch 1640 / 3125 - Loss:  1.0485 \n",
            "Batch 1650 / 3125 - Loss:  1.0627 \n",
            "Batch 1660 / 3125 - Loss:  1.0808 \n",
            "Batch 1670 / 3125 - Loss:  0.9717 \n",
            "Batch 1680 / 3125 - Loss:  1.1586 \n",
            "Batch 1690 / 3125 - Loss:  0.9188 \n",
            "Batch 1700 / 3125 - Loss:  0.9633 \n",
            "Batch 1710 / 3125 - Loss:  1.0104 \n",
            "Batch 1720 / 3125 - Loss:  1.0989 \n",
            "Batch 1730 / 3125 - Loss:  1.0030 \n",
            "Batch 1740 / 3125 - Loss:  1.0545 \n",
            "Batch 1750 / 3125 - Loss:  1.0327 \n",
            "Batch 1760 / 3125 - Loss:  1.0561 \n",
            "Batch 1770 / 3125 - Loss:  1.1002 \n",
            "Batch 1780 / 3125 - Loss:  1.0435 \n",
            "Batch 1790 / 3125 - Loss:  1.0144 \n",
            "Batch 1800 / 3125 - Loss:  0.9362 \n",
            "Batch 1810 / 3125 - Loss:  0.9924 \n",
            "Batch 1820 / 3125 - Loss:  0.9609 \n",
            "Batch 1830 / 3125 - Loss:  1.0674 \n",
            "Batch 1840 / 3125 - Loss:  0.9405 \n",
            "Batch 1850 / 3125 - Loss:  1.0149 \n",
            "Batch 1860 / 3125 - Loss:  1.0262 \n",
            "Batch 1870 / 3125 - Loss:  1.0061 \n",
            "Batch 1880 / 3125 - Loss:  1.0006 \n",
            "Batch 1890 / 3125 - Loss:  0.9946 \n",
            "Batch 1900 / 3125 - Loss:  1.0267 \n",
            "Batch 1910 / 3125 - Loss:  0.9506 \n",
            "Batch 1920 / 3125 - Loss:  0.9408 \n",
            "Batch 1930 / 3125 - Loss:  1.1270 \n",
            "Batch 1940 / 3125 - Loss:  1.1086 \n",
            "Batch 1950 / 3125 - Loss:  1.1190 \n",
            "Batch 1960 / 3125 - Loss:  1.0641 \n",
            "Batch 1970 / 3125 - Loss:  1.0234 \n",
            "Batch 1980 / 3125 - Loss:  1.0723 \n",
            "Batch 1990 / 3125 - Loss:  1.0439 \n",
            "Batch 2000 / 3125 - Loss:  1.0122 \n",
            "Batch 2010 / 3125 - Loss:  1.0316 \n",
            "Batch 2020 / 3125 - Loss:  1.0284 \n",
            "Batch 2030 / 3125 - Loss:  1.0525 \n",
            "Batch 2040 / 3125 - Loss:  1.0088 \n",
            "Batch 2050 / 3125 - Loss:  0.9696 \n",
            "Batch 2060 / 3125 - Loss:  0.9611 \n",
            "Batch 2070 / 3125 - Loss:  1.0710 \n",
            "Batch 2080 / 3125 - Loss:  1.0378 \n",
            "Batch 2090 / 3125 - Loss:  1.0215 \n",
            "Batch 2100 / 3125 - Loss:  1.0001 \n",
            "Batch 2110 / 3125 - Loss:  0.9601 \n",
            "Batch 2120 / 3125 - Loss:  1.1067 \n",
            "Batch 2130 / 3125 - Loss:  1.0348 \n",
            "Batch 2140 / 3125 - Loss:  1.0338 \n",
            "Batch 2150 / 3125 - Loss:  0.8974 \n",
            "Batch 2160 / 3125 - Loss:  0.9742 \n",
            "Batch 2170 / 3125 - Loss:  0.9392 \n",
            "Batch 2180 / 3125 - Loss:  1.0144 \n",
            "Batch 2190 / 3125 - Loss:  1.0608 \n",
            "Batch 2200 / 3125 - Loss:  0.9890 \n",
            "Batch 2210 / 3125 - Loss:  0.9317 \n",
            "Batch 2220 / 3125 - Loss:  1.0413 \n",
            "Batch 2230 / 3125 - Loss:  1.0099 \n",
            "Batch 2240 / 3125 - Loss:  1.0658 \n",
            "Batch 2250 / 3125 - Loss:  0.9084 \n",
            "Batch 2260 / 3125 - Loss:  1.0266 \n",
            "Batch 2270 / 3125 - Loss:  0.9962 \n",
            "Batch 2280 / 3125 - Loss:  1.0056 \n",
            "Batch 2290 / 3125 - Loss:  1.1209 \n",
            "Batch 2300 / 3125 - Loss:  1.0698 \n",
            "Batch 2310 / 3125 - Loss:  0.9941 \n",
            "Batch 2320 / 3125 - Loss:  0.9383 \n",
            "Batch 2330 / 3125 - Loss:  1.0572 \n",
            "Batch 2340 / 3125 - Loss:  0.9833 \n",
            "Batch 2350 / 3125 - Loss:  1.1186 \n",
            "Batch 2360 / 3125 - Loss:  0.9786 \n",
            "Batch 2370 / 3125 - Loss:  1.0365 \n",
            "Batch 2380 / 3125 - Loss:  0.9798 \n",
            "Batch 2390 / 3125 - Loss:  1.0268 \n",
            "Batch 2400 / 3125 - Loss:  1.0033 \n",
            "Batch 2410 / 3125 - Loss:  1.0416 \n",
            "Batch 2420 / 3125 - Loss:  0.9931 \n",
            "Batch 2430 / 3125 - Loss:  0.9799 \n",
            "Batch 2440 / 3125 - Loss:  0.9990 \n",
            "Batch 2450 / 3125 - Loss:  0.9375 \n",
            "Batch 2460 / 3125 - Loss:  0.9492 \n",
            "Batch 2470 / 3125 - Loss:  1.0069 \n",
            "Batch 2480 / 3125 - Loss:  1.0422 \n",
            "Batch 2490 / 3125 - Loss:  1.0262 \n",
            "Batch 2500 / 3125 - Loss:  0.9455 \n",
            "Batch 2510 / 3125 - Loss:  0.9151 \n",
            "Batch 2520 / 3125 - Loss:  1.0423 \n",
            "Batch 2530 / 3125 - Loss:  1.0582 \n",
            "Batch 2540 / 3125 - Loss:  1.0022 \n",
            "Batch 2550 / 3125 - Loss:  1.0138 \n",
            "Batch 2560 / 3125 - Loss:  0.9886 \n",
            "Batch 2570 / 3125 - Loss:  0.9649 \n",
            "Batch 2580 / 3125 - Loss:  1.0320 \n",
            "Batch 2590 / 3125 - Loss:  1.0421 \n",
            "Batch 2600 / 3125 - Loss:  1.1519 \n",
            "Batch 2610 / 3125 - Loss:  0.9867 \n",
            "Batch 2620 / 3125 - Loss:  1.1340 \n",
            "Batch 2630 / 3125 - Loss:  1.0377 \n",
            "Batch 2640 / 3125 - Loss:  1.0544 \n",
            "Batch 2650 / 3125 - Loss:  0.9855 \n",
            "Batch 2660 / 3125 - Loss:  0.9786 \n",
            "Batch 2670 / 3125 - Loss:  1.1184 \n",
            "Batch 2680 / 3125 - Loss:  0.8976 \n",
            "Batch 2690 / 3125 - Loss:  0.9278 \n",
            "Batch 2700 / 3125 - Loss:  0.9807 \n",
            "Batch 2710 / 3125 - Loss:  0.8329 \n",
            "Batch 2720 / 3125 - Loss:  0.9589 \n",
            "Batch 2730 / 3125 - Loss:  1.1633 \n",
            "Batch 2740 / 3125 - Loss:  0.9318 \n",
            "Batch 2750 / 3125 - Loss:  1.0492 \n",
            "Batch 2760 / 3125 - Loss:  1.0759 \n",
            "Batch 2770 / 3125 - Loss:  1.0128 \n",
            "Batch 2780 / 3125 - Loss:  0.9931 \n",
            "Batch 2790 / 3125 - Loss:  0.9999 \n",
            "Batch 2800 / 3125 - Loss:  1.0451 \n",
            "Batch 2810 / 3125 - Loss:  1.0406 \n",
            "Batch 2820 / 3125 - Loss:  0.9865 \n",
            "Batch 2830 / 3125 - Loss:  0.9846 \n",
            "Batch 2840 / 3125 - Loss:  0.9657 \n",
            "Batch 2850 / 3125 - Loss:  1.0541 \n",
            "Batch 2860 / 3125 - Loss:  1.0543 \n",
            "Batch 2870 / 3125 - Loss:  0.9418 \n",
            "Batch 2880 / 3125 - Loss:  1.0826 \n",
            "Batch 2890 / 3125 - Loss:  0.9253 \n",
            "Batch 2900 / 3125 - Loss:  0.9790 \n",
            "Batch 2910 / 3125 - Loss:  1.0854 \n",
            "Batch 2920 / 3125 - Loss:  1.0794 \n",
            "Batch 2930 / 3125 - Loss:  0.9745 \n",
            "Batch 2940 / 3125 - Loss:  1.0564 \n",
            "Batch 2950 / 3125 - Loss:  1.1102 \n",
            "Batch 2960 / 3125 - Loss:  1.0568 \n",
            "Batch 2970 / 3125 - Loss:  1.0550 \n",
            "Batch 2980 / 3125 - Loss:  1.0785 \n",
            "Batch 2990 / 3125 - Loss:  0.9761 \n",
            "Batch 3000 / 3125 - Loss:  1.0097 \n",
            "Batch 3010 / 3125 - Loss:  0.9994 \n",
            "Batch 3020 / 3125 - Loss:  1.0443 \n",
            "Batch 3030 / 3125 - Loss:  1.0110 \n",
            "Batch 3040 / 3125 - Loss:  0.9327 \n",
            "Batch 3050 / 3125 - Loss:  1.0191 \n",
            "Batch 3060 / 3125 - Loss:  1.0299 \n",
            "Batch 3070 / 3125 - Loss:  1.0818 \n",
            "Batch 3080 / 3125 - Loss:  1.1670 \n",
            "Batch 3090 / 3125 - Loss:  1.0211 \n",
            "Batch 3100 / 3125 - Loss:  1.0959 \n",
            "Batch 3110 / 3125 - Loss:  1.0742 \n",
            "Batch 3120 / 3125 - Loss:  1.0342 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lmIutfkyO5Gw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "_WAma8CQCEDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "20af4633-a7e8-4eca-fc19-0512403d6230"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79b98e2abf50>]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_logs[200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD4gekxmlcd6",
        "outputId": "ad1f684f-2d66-42c4-9d10-cefa9d2d28d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([544., 335.,  10., 402., 128., 545., 547., 547., 547., 547., 547., 547.,\n",
              "        547., 547., 547., 547.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "74uAmi0ztCFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_session = [544, 335, 10, 335, 335, 16, 335, 10, 402, 545, 547., 547., 547., 547., 547., 547.]\n",
        "# Шумная сессия"
      ],
      "metadata": {
        "id": "ZEWzUaaNInqw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_session = torch.Tensor(noisy_session).view(1, -1).unsqueeze(1).cuda()"
      ],
      "metadata": {
        "id": "R7LvB67htwJd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = model(noisy_session)"
      ],
      "metadata": {
        "id": "_mxKs-1btEUq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise\n",
        "#шум"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF8sxsrTtM_z",
        "outputId": "cbaac8c8-181a-48f6-c4f7-c2bf8a427988"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2262,  0.2775, -0.0040, -0.0615,  0.0689,  0.2227,  0.1801,\n",
              "           0.2483,  0.2376,  0.0399,  0.0609, -0.0986, -0.1300, -0.1496,\n",
              "           0.0495, -0.0936]]], device='cuda:0', grad_fn=<ConvolutionBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_session = noisy_session - noise"
      ],
      "metadata": {
        "id": "hmhtrIm7tkGJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_session\n",
        "# очищенная сессия"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct_g_JvFtr8M",
        "outputId": "d09fa72c-b08f-48d8-dded-238a44f90897"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[543.7737, 334.7224,  10.0040, 335.0615, 334.9311,  15.7773, 334.8199,\n",
              "            9.7517, 401.7624, 544.9601, 546.9391, 547.0986, 547.1300, 547.1496,\n",
              "          546.9506, 547.0936]]], device='cuda:0', grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "titles = [\"Зашумленное\", \"Шум\", \"После диффузии\"]\n",
        "\n",
        "for i, img in enumerate([noisy_session.cpu(), noise.cpu().detach(), clean_session.cpu().detach()]):\n",
        "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(titles[i])\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Y8R1cIZht2gF",
        "outputId": "b1e95aff-895b-4f26-a664-f609d8ac0182"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_diffusion_process(x, t, noise_schedule):\n",
        "    beta_t = noise_schedule[t]\n",
        "    noise = torch.randn_like(x)\n",
        "    return torch.sqrt(1 - beta_t) * x + torch.sqrt(beta_t) * noise\n",
        "\n",
        "noise_slap = forward_diffusion_process(clean_logs[0],10, beta_schedule)\n",
        "print(noise_slap)"
      ],
      "metadata": {
        "id": "vp_NLoyGuR2p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "7e82abee-586c-4946-be7a-917c7ce8e2a5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-c1e4733fd4b2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnoise_slap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_diffusion_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_slap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c1e4733fd4b2>\u001b[0m in \u001b[0;36mforward_diffusion_process\u001b[0;34m(x, t, noise_schedule)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbeta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_schedule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnoise_slap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_diffusion_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_logs = load_dataset(file_path)\n",
        "clean_logs = torch.Tensor(clean_logs['screen_seq']).view(100000, 16)"
      ],
      "metadata": {
        "id": "R-N6m9RcpVVb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "titles = [\"Чистый лог\",\"Зашумленное\", \"Шум\", \"После диффузии\"]\n",
        "\n",
        "for i, img in enumerate([clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach()]):\n",
        "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(titles[i])\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "print(clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach())\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "moASjTGKVV9j",
        "outputId": "5647bbd3-0cc6-41f6-ba84-3984ea5e3b72"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[544., 335., 278., 309., 106., 545., 547., 547., 547., 547., 547.,\n",
            "          547., 547., 547., 547., 547.]]]) tensor([[[544.2263, 335.2776, 277.9961, 308.9385, 106.0689, 545.2227, 547.1801,\n",
            "          547.2484, 547.2376, 547.0399, 547.0609, 546.9014, 546.8700, 546.8504,\n",
            "          547.0494, 546.9064]]]) tensor([[[ 0.2262,  0.2775, -0.0040, -0.0615,  0.0689,  0.2227,  0.1801,\n",
            "           0.2483,  0.2376,  0.0399,  0.0609, -0.0986, -0.1300, -0.1496,\n",
            "           0.0495, -0.0936]]]) tensor([[[543.7737, 334.7224,  10.0040, 335.0615, 334.9311,  15.7773, 334.8199,\n",
            "            9.7517, 401.7624, 544.9601, 546.9391, 547.0986, 547.1300, 547.1496,\n",
            "          546.9506, 547.0936]]])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8aMLewo8hjsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_logs[0].view(1, -1).unsqueeze(1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULL5wuL3hnAZ",
        "outputId": "b63747d9-a0b1-499f-98cc-4cc6152272dc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_diffusion_step(x, t, noise_schedule):\n",
        "    beta_t = noise_schedule[t]\n",
        "    noise = torch.randn_like(x)\n",
        "    sqrt_term = torch.sqrt(torch.clamp(1 - beta_t, min=1e-5))\n",
        "    return (x - torch.sqrt(beta_t) * noise) / sqrt_term\n",
        "\n",
        "clean_session = forward_diffusion_process(noise_slap,10, beta_schedule)\n",
        "print(clean_session)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Jdi3ucCWWdKi",
        "outputId": "c5479e20-b36c-4e02-b214-41e2b25b9d56"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'noise_slap' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-53ce7cd7ab98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msqrt_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclean_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_diffusion_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_slap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'noise_slap' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "titles = [\"Чистый лог\",\"Зашумленное\", \"Шум\", \"После диффузии\"]\n",
        "\n",
        "for i, img in enumerate([clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().unsqueeze(0).unsqueeze(0).detach()]):\n",
        "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(titles[i])\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "print(clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach())\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "4VV-rd5ivXFA",
        "outputId": "6e5fef7a-e227-44af-bd41-7f7e5a3c6571"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-dd8eebc43725>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "titles = [\"Чистый лог\",\"Зашумленное\", \"Шум\", \"После диффузии\"]\n",
        "\n",
        "for i, img in enumerate([clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().unsqueeze(0).unsqueeze(0).detach()]):\n",
        "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(titles[i])\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "print(clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach())\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "titles = [\"Чистый лог\",\"Зашумленное\", \"Шум\", \"После диффузии\"]\n",
        "\n",
        "for i, img in enumerate([clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach()]):\n",
        "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(titles[i])\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "print(clean_logs[0].unsqueeze(0).unsqueeze(0).cpu(),clean_logs[0].unsqueeze(0).unsqueeze(0).cpu() + noise.cpu().detach(),noise.cpu().detach(), clean_session.cpu().detach())\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print((clean_session.cpu().detach())-(clean_session.cpu().detach))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "fJGSh5YcJDEU",
        "outputId": "ac6cc343-a186-44a6-ed67-d65aa3c3bdb0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a3c1be14b183>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oB2xS9R4P3a-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}